# Curso de Prompt Engineering

## Clase 1: Conceptos Clave para Entender los LLMs en Prompt Engineering

Â¿QuÃ© es exactamente un LLM?
ImagÃ­nate una biblioteca gigantesca que contiene absolutamente todo lo que se ha escrito en la humanidad. Ahora imagina esta biblioteca con una mente propia, capaz de entender significativamente el contenido. Esto es precisamente un LLM o modelo grande de lenguaje, una herramienta que aunque no tiene conciencia, posee la impresionante habilidad de predecir la siguiente palabra o frase con gran precisiÃ³n.


## Clase 2:  Vectores y Embeddings en Modelos de Lenguaje

- ComprensiÃ³n de LLMs
    - ComparaciÃ³n con buscar un libro en una biblioteca
    - TransformaciÃ³n de palabras en relaciones y caracterÃ­sticas
- Ejemplo de la palabra "motivar"
    - Contexto deportivo y su clasificaciÃ³n
    - AnÃ¡lisis de sentimientos y usos
- Concepto de vectores
    - DefiniciÃ³n y aplicaciÃ³n en diversas dimensiones
    - Ejemplos en la vida diaria
- Embeddings y dimensiones n-dimensionales
    - Dificultad de imaginar mÃ¡s de tres dimensiones
    - Ejemplos de evaluaciones en restaurantes y direcciones
- Similitud semÃ¡ntica y su rol en LLMs
    - Ejemplo con "aguacate" y "avocado"
    - Importancia de la elecciÃ³n de palabras

## Clase 4: AtenciÃ³n y Contexto en Modelos de Lenguaje Natural 

ğŸ§ª Ejercicio de Priming

ğŸ§  ActivaciÃ³n automÃ¡tica e inconsciente por estÃ­mulos previos
ğŸ¯ Ejemplo: pensar en un vegetal tras operaciones matemÃ¡ticas
ğŸ¤– AtenciÃ³n en modelos LLM

ğŸ” Asigna pesos a palabras
ğŸ“ Predice respuestas considerando relevancia contextual
ğŸ“± Diferencias con autocorrectores

ğŸ“² Autocorrector: solo considera la palabra anterior
ğŸ¤¯ LLM: considera todo el contexto (tus palabras y las del modelo)
ğŸªŸ Ventana de contexto

ğŸ“š Capacidad de procesar hasta 128,000 tokens (~160 pÃ¡ginas)
ğŸ”„ El modelo puede priorizar partes recientes de la conversaciÃ³n
ğŸ§­ Relevancia de las palabras

ğŸ± Ejemplo: â€œgatoâ€ tiene mÃ¡s contexto que â€œelâ€
ğŸ§© Ayuda a predecir de forma mÃ¡s precisa
ğŸ¯ TÃ©cnicas de prompting (en futuras clases)

ğŸ§² Redirigir la atenciÃ³n del modelo
ğŸ› ï¸ Personalizar y mejorar respuestas

## El Mecanismo de AtenciÃ³n 
Es la capacidad del modelo de enfocarse en los tokens/embeddings mÃ¡s relevantes del input, lograda mediante la comparaciÃ³n matemÃ¡tica de sus vectores (Embeddings) dentro del Espacio N-Dimensional. Esto le permite entender el contexto y es fundamental para su funcionamiento y aparente "razonamiento".

## Â¿CÃ³mo se relaciona con los conceptos anteriores (Tokens, Embeddings, Espacio N-Dimensional)?

- Opera sobre Tokens y sus Embeddings: El mecanismo de atenciÃ³n no trabaja directamente con las palabras o el texto crudo. Trabaja con las representaciones numÃ©ricas de los tokens: Â¡los Embeddings (Vectores)!

- Compara Embeddings en el Espacio N-Dimensional: El "prestar atenciÃ³n" del modelo se logra comparando los Embeddings de los diferentes tokens en el ``Espacio N-Dimensional. El mecanismo calcula una especie de "puntuaciÃ³n de relevancia" o "similitud" entre el embedding del token actual que estÃ¡ procesando y los embeddings de todos los otros tokens en la ventana de contexto. Estas puntuaciones se derivan de operaciones matemÃ¡ticas realizadas con los vectores en el espacio N-dimensional (como calcular cuÃ¡n "alineados" estÃ¡n).

El Resultado Ayuda a Entender el Contexto (Llevando a Embeddings Contextualizados): BasÃ¡ndose en estas puntuaciones de relevancia, el mecanismo de atenciÃ³n pesa la importancia de los embeddings de los otros tokens. El resultado final de este proceso de atenciÃ³n**** es una nueva representaciÃ³n (a menudo, un tipo de embedding contextualizado) para el token actual, que ahora incluye informaciÃ³n combinada de los tokens mÃ¡s relevantes de su contexto.

Pensemos en la AnalogÃ­a del Cocinero de nuevo:
Ahora el cocinero no solo corta bien los ingredientes (TokenizaciÃ³n) y sabe quÃ© "sabor" tiene cada uno individualmente (Embeddings base), sino que tambiÃ©n sabe que para hacer un buen plato, debe prestarle mÃ¡s atenciÃ³n a cÃ³mo interactÃºan ciertos sabores (el Mecanismo de AtenciÃ³n prestando atenciÃ³n a cÃ³mo interactÃºan ciertos embeddings/tokens). Sabe que la cebolla y el ajo son muy relevantes para el sabor de la mayorÃ­a de los sofritos, y les presta mÃ¡s "atenciÃ³n" al combinarlos.

