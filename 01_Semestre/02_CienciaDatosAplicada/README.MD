
# Tema 1. La ciencia del dato y los datos masivos

## 1.1. Generalidades
- La ciencia de datos y los datos masivos son disciplinas que se encuentran en constante crecimiento y evolución en el ámbito empresarial. 

- Su importancia radica en su capacidad para extraer conocimiento y generar valor a partir de grandes volúmenes de datos, 

- Permitiendo así tomar decisiones estratégicas fundamentadas en información precisa y relevante.

## 1.2. La Cadena de Valor

> La cadena de valor de los datos es el proceso de transformar datos crudos en acciones valiosas para una empresa. Comienza con la recolección de datos, continúa con su procesamiento y análisis, y culmina con la toma de decisiones informadas que generan valor. Cada etapa es crucial para aprovechar al máximo el potencial de los datos.

La cadena de valor de los datos consta de las siguientes etapas:

1. Recopilación: Se recolectan datos de diversas fuentes.
2. Procesamiento: Los datos se limpian, organizan y preparan para el análisis.
3. Análisis: Se aplican técnicas estadísticas y de aprendizaje automático para extraer insights.[perspectivas]
4. Información: Se generan conocimientos útiles a partir de los datos analizados.
5. Decisión: Se toman decisiones estratégicas basadas en la información obtenida.
6. Valor: Se crea valor para la empresa al implementar las decisiones.

## 1.3. ¿Qué son los datos masivos?

> Los datos masivos (big data) son conjuntos de información extremadamente grandes y complejos que se generan a una velocidad vertiginosa. Esta información proviene de diversas fuentes, como redes sociales, transacciones en línea y sensores.

## Características clave de los datos masivos:**

- Tienen Volumen: Grandes cantidades de datos que superan la capacidad de herramientas tradicionales.

- Tienen Velocidad: Datos generados a una velocidad muy alta y que necesitan ser procesados rápidamente.

- Tienen Variedad: Diferentes tipos de datos (texto, imágenes, videos, etc.) en diversos formatos.

## ¿Por qué son importantes los datos masivos?

- Contiene Información valiosa: Al analizar estos datos, se pueden obtener insights (perspectivas) clave para tomar mejores decisiones.

- Contienen Ventajas competitivas: Las empresas pueden identificar tendencias, predecir comportamientos y mejorar sus estrategias.

- Contienen Aprendizaje automático: Gracias a tecnologías como el aprendizaje automático, se pueden descubrir patrones complejos en los datos.

> Un dato puede definirse como un hecho concreto y discreto acerca de un evento. [ Seguiremos la proporcionada por Davenport y Prusak (1998).] La característica de ser discreto implica que, semánticamente, es la unidad mínima  que puede comunicarse o almacenarse. Por sí solos, los datos no brindan detalles significantes del entorno del que fueron obtenidos.

> La información puede definirse como un mensaje formado por la composición de varios datos

## 1.4. ¿Qué es la Ciencia de Datos?

- La ciencia de datos es una disciplina que busca extraer conocimiento a partir de grandes conjuntos de datos. 
- A diferencia de otras ciencias que se basan en teorías preexistentes, la ciencia de datos utiliza herramientas computacionales y estadísticas para descubrir patrones y tendencias ocultas en los datos.

**¿Qué hace única a la ciencia de datos?**

- Agnosticismo: Puede aplicarse a cualquier campo, desde química hasta marketing.
- Fundamental: Busca información básica en los datos, no solo confirmar teorías.
- Multidisciplinaria: Combina herramientas de matemáticas, estadística, informática, inteligencia artificial y más.
- Transformadora: Está cambiando la forma en que entendemos y abordamos la ciencia.

**Beneficios para las organizaciones:**

- Descubrimiento de patrones ocultos: Identificar tendencias y oportunidades que no son evidentes a simple vista.
- Toma de decisiones basada en datos: Utilizar la información para tomar decisiones más informadas y estratégicas.
- Optimización de procesos: Mejorar la eficiencia y reducir costos.
- Generación de estrategias competitivas: Obtener una ventaja competitiva en el mercado.

## 1.5. La toma de decisiones basadas en datos

La toma de decisiones basada en datos es un proceso que utiliza análisis estadístico profundo y herramientas avanzadas para transformar grandes cantidades de información en conocimiento accionable. Al emplear datos objetivos y rigurosos, las empresas pueden:

- Tomar decisiones más informadas: Identificando tendencias, patrones y riesgos.
- Optimizar procesos: Aumentando la eficiencia y reduciendo costos.
- Mejorar la toma de decisiones estratégicas: Alineado con los objetivos organizacionales.
- Minimizar la incertidumbre: Basando las decisiones en evidencia sólida.
- Incrementar la competitividad: Identificando nuevas oportunidades y mejorando la toma de decisiones.

## 1.6. Problemas empresariales y soluciones de ciencia de datos

- La ciencia de datos: un aliado estratégico para optimizar la cadena de suministro

- La ciencia de datos ofrece una herramienta poderosa para abordar de manera efectiva los desafíos que enfrentan las empresas en la actualidad. Al analizar grandes volúmenes de datos, esta disciplina permite:

- Optimizar la cadena de suministro: Identificando cuellos de botella, mejorando la planificación de la demanda y reduciendo costos.
- Detectar fraudes: Implementando sistemas de alerta temprana basados en patrones anómalos en los datos.
- Personalizar la experiencia del cliente: Analizando el comportamiento de los consumidores para ofrecer productos y servicios más relevantes.
- Segmentar el mercado: Identificando grupos de clientes con características similares para adaptar las estrategias de marketing.
- Predecir la demanda: Anticipando las necesidades futuras de los clientes para ajustar la producción y evitar desabastecimientos.

**¿Cómo funciona?**

A través de técnicas como el aprendizaje automático, la minería de datos y la inteligencia artificial, la ciencia de datos transforma los datos en información valiosa y accionable. Esto permite a las empresas tomar decisiones más informadas y basadas en evidencias, reduciendo así la incertidumbre y el riesgo.


## Cuestionario 

1. ¿Qué es la cadena de valor en la ciencia de datos?
    A. Un software específico para análisis de datos.
    B. Un proceso que transforma los datos en valor agregado para las empresas. -> Correcto 
    C. Una metodología de visualización de datos.
    D. Un tipo de algoritmo de aprendizaje automático.

2. ¿Qué caracteriza a los datos masivos?
    A. Pequeños volúmenes de información.
    B. Procesamiento manual de datos.
    C. Grandes volúmenes de datos que requieren herramientas especiales. -> Correcto 
    D. Datos siempre estructurados.

3. ¿Cuál es una aplicación de la ciencia de datos en las empresas?
    A. Reducir la cantidad de datos a analizar.
    B. Ignorar las tendencias del mercado.
    C. Tomar decisiones basadas en conjeturas. -> correcto 
    D. Optimizar procesos y mejorar la eficiencia operativa.

4. ¿Qué se requiere en la fase de recopilación de datos?
    A. Fuentes confiables y métodos eficientes. --> Correcto 
    B. Limitar la cantidad de datos recogidos.
    C. Usar exclusivamente datos estructurados.
    D. Evitar el uso de tecnología moderna.

5. ¿Qué permite el aprendizaje automático en el contexto de datos masivos?
    A. Reducir la cantidad de datos necesarios para análisis.
    B. Automatizar tareas complejas de análisis de datos. -> Datos
    C. Evitar el uso de cualquier tipo de datos.
    D. Utilizar exclusivamente datos antiguos.

6. ¿Cuál es el primer paso en la cadena de valor de la ciencia de datos?
    A. Visualización de datos. 
    B. Análisis de datos.
    C. Recopilación de datos. -> Correcto 
    D. Toma de decisiones.

7. ¿Qué tecnologías son cruciales para manejar datos masivos?
    A. Aprendizaje automático e inteligencia artificial. --> Correcto 
    B. Calculadoras básicas.
    C. Herramientas de escritura manual.
    D. Procesamiento de texto simple.

8. ¿Qué beneficios ofrece la ciencia de datos a las empresas?
    A. Limita las fuentes de datos utilizadas.
    B. Ignora las tendencias y patrones en los datos.
    C. Ayuda a tomar decisiones basadas en datos y resolver problemas --> Correcto 
    complejos.
    D. Fomenta decisiones basadas únicamente en la intuición.

9. ¿Qué implica la acción basada en decisiones dentro de la cadena de valor de los datos?
    A. No tomar ninguna medida basada en los análisis.
    B. Implementar decisiones que crean valor para la organización. --> Correcto 
    C. Desconocer los resultados del análisis.
    D. Revertir todas las decisiones previas.

10. ¿Qué se necesita para maximizar el potencial de los datos en beneficio de la empresa?
    A. Herramientas y técnicas específicas en cada etapa de la cadena de valor. --> Correcto 
    B. Evitar la adopción de nuevas tecnologías.
    C. Reducir la cantidad de datos analizados.
    D. Concentrarse en datos irrelevantes.


# Tema 2. Ciclo de vida de los datos masivos

## 2.1. Recolección
Es el proceso que nos permite iniciar con la redaudacción de datos aquí podremos indentifcar la fuente de información, tambien podemos implementar tareas de limpieza y tratamiento de datos como validaciones y comprobaciones, para luego ser almacenados bajo su estructura previamente analizada bien sea BD SQL o NOsql 

## Es importante conocer como se encuentran los datos para iniciar su recolección

**Datos no estructurados**
Los datos no estructurados son la forma «más cruda» de los datos sin una estructura
identificable y pueden ser cualquier tipo de dato como textos, imágenes, sonidos o
vídeos. Una forma de determinar si los datos son no estructurados es si no podemos
almacenarlos en filas y columnas en una base de datos relacional

## Algunos ejemplos de datos no estructurados serían:
- Textos: archivos de texto plano (.txt)
- Imágenes y animaciones: archivos JPEG, PNG, GIF, etc.
- Sonidos: archivos MP3, OGG, WAV, etc.
- Vídeos: archivos MPEG / MP4, AVI, MKV, OGG, etc

**Datos estructurados**
- Siguen un esquema (schema) que requiere cierto esfuerzo elaborar. 
- El caso más habitual de este tipo de esquema en los datos estructurados es el de tablas o esquema tabular. 
- En los datos tabulares podemos estructurar los datos en filas y columnas, como si de una tabla se
tratara.
- La cabecera (header) o primera fila nos indica el nombre (y tipo) de cada uno
de los atributos. 
- Una fila o un conjunto de atributos es conocida como un registro o una instancia de la especificación dada por el esquema de datos.

## Algunos ejemplos de datos estructurados serían:
- Los archivos CSV (comma-separated values) son un ejemplo claro de datos
estructurados, siendo ampliamente utilizados como datasets en ciencia de datos.
- Ficheros CSV.
- Hojas de cálculo (Excel o similares).
- Bases de datos relacionales (SQL u OLAP).


**Datos semiestructurados**
En los datos semiestructurados el esquema no está separado de los datos, sino que
estos son autodescriptivos. Esto no significa que el esquema como tal por separado
no exista, sino que, en realidad, este es opcional los datos semiestructurados son estructuras más flexibles que los datos
estructurados y no tienen por qué ser tabulares

## Algunos ejemplos de datos estructurados serían:

- Archivos XML (extended markup language)
- Archivos HTML (hypertext markup language)
- Archivos JSON (Javascript object notation)
- Archivos YAML (YAML ain’t markup language u, originalmente, yet another markup language), 
- Tripletas RDF (resource description framework)


## Mecanismos de muestreo

En este apartado vamos a describir, en primer lugar, los principales mecanismos de
muestreo a la hora de recoger los datos desde diferentes fuentes,

**Muestro de señales y el teorema de Shannon-Nyquist**
Un ejemplo evidente es la recogida de datos desde redes de sensores o
dispositivos IoT, ampliamente utilizados en la medición de datos acerca del
entorno. 

Según el teorema de Shannon-Nyquist, si una función x(t) no contiene frecuencias
superiores a B hertzios, podemos caracterizar completamente esta si tomamos
muestras cada 1/(2B) segundos o, lo que es lo mismo, con una frecuencia igual a 2B
(Vaidyanathan, 2001). 2B es llamada también la tasa de Nyquist, mientras que la
máxima B permitida por un equipo de muestro (un conversor analógico-digital) es la
frecuencia de Nyquist.


**Muestreo de poblaciones: métodos probabilísticos, métodos no probabilísticos y sesgo**

El muestreo puede basarse en la probabilidad, un enfoque que utiliza números
aleatorios que corresponden a puntos del conjunto de datos para garantizar que no
haya correlación entre los puntos elegidos para la muestra. Entre los métodos
probabilísticos podemos destacar:

- Muestreo aleatorio simple (simple random sampling): se utiliza un proceso
informático para seleccionar aleatoriamente a los sujetos de toda la población.
Muestreo estratificado (stratified sampling): se crean subconjuntos de los conjuntos
de datos o de la población en función de un factor común y se recogen muestras al
azar de cada subgrupo (por ejemplo, estratificar la población por sexo, edad,
provincia, ingresos, etc.). Así nos aseguramos de que no tenemos, por ejemplo, un
70 % de muestras de hombres, cuando estos no representan tal porcentaje en la
población total. En caso contrario, introduciríamos un sesgo en el muestreo.

- Muestreo por conglomerados (cluster sampling): el conjunto de datos más amplio
se divide en subconjuntos (clusters o clústeres) en función de un factor definido, y
luego se analiza un muestreo aleatorio de los clústeres. Por ejemplo, crear
subconjuntos por región o por provincia o crear subconjuntos por tipos de
consumidores que se comportan de forma similar. Es importante que el
comportamiento dentro de cada clúster sea relativamente homogéneo, mientras que
el comportamiento entre individuos de diferentes clústeres sea relativamente
heterogéneo.

- Muestreo multietapa (multistep sampling): se trata de una forma más complicada
del muestreo por conglomerados. Este método también implica la división de la
población más grande en una serie de clústeres. A continuación, los clústeres de la
segunda etapa se dividen en función de un factor secundario y esos clústeres se
muestrean y analizan. Este escalonamiento podría continuar a medida que se
identifican, agrupan y analizan múltiples subconjuntos.

- Muestreo sistemático (systematic sampling): en este método, se crea una muestra
estableciendo un intervalo en el que se extraen los datos de la población mayor. Por
ejemplo, seleccionando cada 10 filas en una hoja de cálculo o un fichero CSV de
2000 elementos para crear un tamaño de muestra de 200 filas para analizar.


**El muestreo también puede basarse en la no probabilidad, un enfoque en el que
se determina y extrae una muestra de datos basada en el juicio del científico de Datos**

- Muestreo de conveniencia (convenience sampling): los datos se recogen de un
grupo fácilmente accesible y disponible.

- Muestreo consecutivo (consecutive sampling): los datos se recogen de cada sujeto
que cumple los criterios hasta alcanzar el tamaño de muestra predeterminado.

- Muestreo intencional o de juicio (purposive o intentional sampling): el investigador
selecciona los datos a muestrear basándose en criterios predefinidos.

- Muestreo por cuotas (quota sampling): el investigador garantiza una representación
equitativa dentro de la muestra para todos los subgrupos del conjunto de datos o
población.


**¿Qué es ETL?**

ETL es un proceso de integración de datos que combina, limpia y organiza datos de múltiples fuentes en un único conjunto de datos coherente para su almacenamiento en un almacén de datos, un lago de datos u otro sistema de destino.

**Las tres fases del proceso ETL son:**

**Extracción (Extract):**

- En esta fase, se extraen los datos relevantes de los sistemas de origen.
- Los sistemas de origen pueden ser bases de datos, archivos planos, aplicaciones SaaS, etc.
- Los datos se extraen utilizando diferentes técnicas, como consultas SQL, API, o scripts personalizados.

**Transformación (Transform):**

- En esta fase, los datos extraídos se limpian, validan y transforman para que sean adecuados para el análisis.
- Las transformaciones comunes incluyen:
    - Limpieza de datos: Eliminar valores nulos, duplicados o inconsistentes.
    - Validación de datos: Asegurarse de que los datos cumplen con los estándares de calidad.
    - Normalización de datos: Convertir los datos a un formato estándar.
    - Agregación de datos: Combinar múltiples fuentes de datos en una sola.
    - Enriquecimiento de datos: Agregar contexto o información adicional a los datos.

**Carga (Load):**

- En esta fase, los datos transformados se cargan en el sistema de destino.
- El sistema de destino puede ser un almacén de datos, un lago de datos, una base de datos analítica, etc.
- Los datos se cargan utilizando diferentes técnicas, como carga por lotes, carga incremental o carga directa.

## Técnicas avanzadas en la recolección de datos masivos
- OCR (Reconocimiento óptico de caracteres). Convierte documentos impresos o imágenes en texto editable y buscable, facilitando el  acceso y la manipulación de información.

- ICR (Reconocimiento Inteligente de Caracteres). Capaz de reconocer y aprender de texto manuscrito, mejorando continuamente su precisión y eficiencia en el procesamiento de datos.

- RFID (Identificación por Radiofrecuencia). Utiliza etiquetas electrónicas para el seguimiento y gestión remota de inventarios y activos mediante señales de radio.

- NFC (Near-field communication-Comunicaciones de Campo Cercano). Permite la comunicación inalámbrica entre dispositivos a corta distancia. Ampliamente usado en pagos móviles y sistemas de seguridad.

- Aplicaciones de Estas Tecnologías. Esenciales en logística, retail, seguridad y gestión de datos, mejorando significativamente la eficiencia operativa y la precisión de la información.

## Desafíos y métodos de captura en la recolección de datos masivos

- Métodos de captura.
- Optimización mediante tecnología.
- Automatización de la captura. 

## 2.2. Análisis
Es el proceso que nos permite de ciertamanera iniciar con el estudio de los datos, preparar todo para realizar calculos, comparaciones y encontrar patrones que nos apoyen a generar insights. Podemos usar algunos modelos de regresión, clasificación, segmentación y recomendación

Hay tres tipos de análisis 

- Análisis descriptivo: describe lo que ha pasado con estadísticas, gráficos, tablas e informes.

- Análisis predictivo: realiza predicciones que van a ser útiles en el futuro siendo de especial importancia la precisión de la predicción.

- Análisis prescriptivo: ayuda a entender qué debe hacerse para obtener los resultados esperados en el futuro.



## Introducción Al Análisis de Datos 
- Definición de análisis de datos: Proceso de inspeccionar, limpiar, transformar y modelar datos con el objetivo de descubrir información útil, informar conclusiones y apoyar la toma de decisiones.
- Importancia en la era digital: El análisis de datos permite a las organizaciones y empresas aprovechar el potencial de los datos masivos para optimizar operaciones, prever tendencias y mejorar la experiencia del cliente.
- Desafíos actuales: Manejar el volumen creciente de datos, asegurar la calidad y la privacidad de los datos y extraer conocimientos significativos en tiempo real.

## Preparación de datos en análisis de datos
- Evaluación de la calidad: Incluye el análisis de formatos, completitud, integridad y disponibilidad de los datos. Fundamental para garantizar la fiabilidad de los análisis posteriores.
- Tareas de preprocesamiento: Realización de transformaciones y filtrados, análisis de datos faltantes, eliminación de datos ruidosos o atípicos.
- También aparecen tareas como la discretización o categorización, encodings y la reducción de los datos.


## El papel del machine learning en el análisis de datos
- Fundamentos del Machine Learning:  Machine learning utiliza algoritmos para analizar datos, aprender de ellos y hacer predicciones o tomar decisiones.
> Se considera una disciplina de la inteligencia artificial. 
- Se especializa en el reconocimiento de patrones; sobre muestras de datos, aprende y permite extraer inferencias de nuevos conjuntos de datos para los que no ha sido entrenado previamente.
- Impacto. Permite automatizar el análisis complejo de grandes volúmenes de datos y descubrir patrones y tendencias que no son evidentes para los métodos tradicionales de análisis.

## Técnicas de machine learning datos

- Técnicas comunes. Incluye métodos de análisis supervisado:
    - Regresión 
    - Clasificación
    - No supervisado. 
- Clustering, reducción de dimensiones y sistemas de recomendación, aplicables a diferentes tipos de datos y necesidades. También encontramos técnicas de aprendizaje semi-supervizado aprendizaje por refuerzo.
- Procesamiento del Lenguaje Natural. El ML es crucial en el procesamiento del lenguaje natural, ayudando a interpretar y generar lenguaje humano de manera efectiva.

## Visualización en el análisis de grandes volúmenes
- Propósito de la visualización: Transformar grandes volúmenes de datos en representaciones gráficas que facilitan la comprensión y la toma de decisiones rápidas.
- Herramientas esenciales: Herramientas de BI como Tableau, Qlik Sense, Looker, Domo, Power BI y Google Data Studio son plataformas líderes que permiten a los usuarios crear visualizaciones dinámicas y dashboards interactivos.
- Impacto en el análisis de datos: Mejora significativamente la accesibilidad y la interpretación de datos complejos, permitiendo a los analistas y stakeholders obtener insights valiosos de manera eficiente.

## 2.3. Visualización

Es el proceso mediante el cual la información analizada se presenta de manera gráfica. Esta fase es esencial porque transforma grandes cantidades de datos complejos en representaciones visuales más accesibles y comprensibles, como gráficos, diagramas y mapas de calor

## 2.4. Interpretación

Finalmente, la interpretación de los datos es el último paso, donde los resultados visualizados se examinan para tomar decisiones informadas


## Visualización efectiva de datos 

- Fundamentos de la Visualización: Consiste en transforma un conjunto complejo de datos en representaciones graficas intuitivas. 
- Estrategias para Visualización efectiva: Incluyen el uso de colores adecuados, diseño limpio y la seleccion de tipos de graficos que mejor represente los datos. 
- Herramienta Clave: Uso de herramienta Tableu, Power BI, Goggle Data Studio. 



# Técnicas para la interpretación de datos
- Análisis de sensibilidad: Evalúa cómo diferentes inputs afectan los resultados de un modelo, ayudando a identificar variables críticas y a entender la robustez de los modelos.
- Técnicas de visualización avanzadas: Incluye el uso de mapas de calor, gráficos de red y visualizaciones tridimensionales para una representación más rica y detallada de los datos.
- Discusión crítica y revisión por pares: Fomenta la evaluación y validación de los resultados de análisis mediante la discusión con expertos y colegas, asegurando la precisión y objetividad.

## Cuestionario 

1. ¿Cómo clasificarías un fichero JSON en función de su organización?
    A. Datos estructurados.
    B. Datos semiestructurados.        -> Correcto 
    C. Datos completamente estructurados.
    D. Ninguna de ellas.

2. ¿Cómo clasificarías una base de datos relacional en función de su organización?
    A. Datos estructurados.    --> Correcto 
    B. Datos semiestructurados.
    C. Datos completamente estructurados.
    D. Ninguna de ellas.

3. ¿Cuáles de los siguientes serían ejemplos de datos no estructurados?
    A. Imágenes, vídeos y sonidos. --> Correctos 
    B. Bases de datos SQL y OLAP.
    C. Archivos JSON y XML.
    D. Archivos CSV.

4. Si estoy creando un instrumento para capturar secuencias de voz humana para
su posterior procesamiento, ¿a qué frecuencia mínima debería muestrear el audio
para no tener aliasing?
    A. 10 Hz.
    B. 4 KHz.
    C. 8000 Hz.
    D. 16 KHz. --> Correcto 

5. ¿En qué método de muestreo probabilístico se crean subconjuntos de los
conjuntos de datos o de la población en función de un factor común y se recogen
muestras al azar de cada subgrupo?
    A. Muestro multietapa.
    B. Muestro sistemático.
    C. Muestreo por conglomerados.
    D. Muestro estratificado. ---> Correcto 


- Muestreo estratificado: En este método, la población se divide en subgrupos homogéneos (estratos) basados en una característica relevante para la investigación (edad, género, nivel de ingresos, etc.). Luego, se selecciona una muestra aleatoria de cada estrato, asegurando así que cada subgrupo esté representado en la muestra final.

- Muestreo multietapa: Implica seleccionar muestras en varias etapas, como seleccionar primero estados, luego ciudades, y finalmente hogares. No se enfoca en crear subgrupos homogéneos.

- Muestreo sistemático: Se selecciona un punto de partida aleatorio y luego se seleccionan elementos a intervalos regulares de la lista de población. No implica la creación de subgrupos.

- Muestreo por conglomerados: Se divide la población en grupos heterogéneos (conglomerados) y se selecciona una muestra aleatoria de conglomerados. Luego, se incluyen todos los elementos de los conglomerados seleccionados. A diferencia del estratificado, los conglomerados son heterogéneos.


6. En el método de muestreo no probabilístico intencional:
    A. Los datos se recogen de un grupo fácilmente accesible y disponible.
    B. Los datos se recogen de cada sujeto que cumple los criterios hasta
    alcanzar el tamaño de muestra predeterminado.
    C. El investigador selecciona los datos a muestrear basándose en criterios predefinidos. -->Correcto 
    D. El investigador garantiza una representación equitativa dentro de la
    muestra para todos los subgrupos del conjunto de datos o población.


Opción A: Corresponde más al muestreo por conveniencia, otro tipo de muestreo no probabilístico, en el cual se seleccionan los participantes más accesibles.
Opción B: Esta opción describe más un tipo de muestreo por cuotas, en el que se establecen cuotas para cada subgrupo y se seleccionan participantes hasta cumplirlas.
Opción C: Esta es la definición precisa del muestreo intencional. El investigador utiliza su juicio para seleccionar a los participantes que considera más adecuados para el estudio, basándose en criterios específicos relacionados con los objetivos de la investigación.
Opción D: La representación equitativa es una característica de los métodos de muestreo probabilístico, no de los no probabilísticos.


7. Los datos recogidos por el propio científico de datos mediante encuestas se
considerarían:
    A. Fuentes de datos primarias.  --> Correcto 
    B. Fuente de datos secundarias.
    C. Fuentes de datos terciarias.
    D. Ninguna de las demás respuestas es correcta.

- Fuentes de datos primarias: Son aquellos datos que son recolectados directamente por el investigador para un propósito específico. En este caso, el científico de datos está recolectando los datos a través de encuestas, lo que significa que él es la fuente original de esa información.
- Fuentes de datos secundarias: Son datos que ya han sido recopilados por alguien más y que el investigador utiliza para su análisis. Ejemplos incluyen datos de censos, informes de empresas o investigaciones previas.
- Fuentes de datos terciarias: Se refieren a materiales que indexan o resumen fuentes primarias y secundarias. Un ejemplo sería un directorio de revistas científicas.


8. El método de captura de datos de forma automatizada mediante el procesamiento de páginas HTML de un sitio web se conoce como:
    A. Web semántica.
    B. Web service.
    C. Web scraping. --> Correcto 
    D. Ninguna de las demás respuestas es correcta.

9. ¿En qué categoría de captura o fuente de datos encaja la lectura de información
del pulso cardíaco por una pulsera de actividad?
    A. Captura manual.
    B. Sensores.  
    C. Captura automatizada.
    D. B y C son correctas. --> Correcto 

10. La infografía y la visualización de datos tienen como objetivo principal:
    A. Presentar la información de una manera muy atractiva. --> Correcto 
    B. Informar y ampliar el conocimiento.
    C. Mostrar una información diferente al lector.
    D. Buscar y organizar datos.


# Tema 3. Arquitecturas Típicas en Proyectos de Datos Masivos

Fuentes Heterogenias de Datos 
- Tipos de Datos: 
    - Estructruturados: bases de datos SQL hojas de calculo 
    - Semiestructurados: documentos XML, json 
    - No estructurados: texto, imagenes, audio, video
    - Datos tiempo real:sensores datos GPS


- Desafios de Integración: La diversidad de formatos y tipos de datos, junto con la necesidad de mantener la calidad y consistencia  de los datos integrados. 
- Tecnologías y estrategias: uso de sistemas de gestion de datos flexibles, herramientas ETL, plataforma de big Data como Hadoop y analisis en tiempo real 
- Ejemplo de usos: Los bancos utilizan datos estructurados para el analisis de credito, Las redes sociales analizan datos no estructurados para insights sobre el comportamiento de usuarios.  


## Proceso ETL
- Extracción: recopilación de datos de diversas fuentes heterogéneas como bases de datos relacionales, NoSQL, archivos y los API-
- Transformación: limpieza, integración y transformación de datos. Aplicación de reglas y operaciones para convertir datos al formato adecuado.
- Carga: almacenamiento de datos transformados en sistemas de almacenamiento como data warehouses o data lakes.
- Herramientas: ejemplos de herramientas ETL incluyen Apache Nifi, Talend, Informatica, AWS Glue.


## Metodos de Extracción 


- Extracción en línea (online): los datos se recuperan directamente del sistema fuente. Puede acceder a tablas de origen o sistemas centrales que almacenan datos predefinidos. No difiere físicamente del sistema fuente.
- Extracción fuera de línea (offline): los datos se organizan fuera del sistema fuente original. Pueden ser registros de rehacer, archivos o creados por rutinas de extracción. Importancia de la recurrencia en extracciones incrementales o completas.
- Ejemplos de uso:
    - Online: recuperación directa de bases de datos operacionales. 
    - Offline: uso de archivos de registro o archivos de backup para extracción de datos.

- Consideraciones: los volúmenes de datos pueden ser grandes. La elección del método depende de la necesidad de frescura («edad») de datos y la infraestructura disponible.


## Transformación y tratamiento de datos
- Limpieza de datos: eliminación de errores, duplicados y datos inconsistentes para asegurar la integridad y calidad.
- Transformación: aplicación de reglas y operaciones para convertir datos a un formato adecuado para análisis.
- Integración: unificación de datos de diferentes fuentes para crear una vista coherente y completa.
- Validación: verificación de que los datos transformados cumplen con los estándares y requisitos definidos.


## Proceso de carga
- Importancia de la carga: incluye tanto la carga de tablas de dimensiones como de tablas de hechos. Asegurar que la carga se realice correctamente y con la menor cantidad de recursos posibles.
- Mejores prácticas: deshabilitar restricciones e índices antes de la carga y habilitarlos después. Mantener la integridad referencial durante el proceso de carga.

- Tipos de carga:
    - Carga inicial: poblar todas las tablas del data warehouse.
    - Carga incremental: aplicar cambios periódicos según sea necesario. 
    - Refresco completo: borrar y recargar una o más tablas con datos frescos.

- Destino principal: la base de datos del proceso de carga. Garantizar que el destino sea eficiente y mantenga la coherencia e integridad de los datos.


## Proceso ETL en la era actual
- Extracción moderna: uso de los API, servicios web y conectores avanzados para recopilar datos en tiempo real de diversas fuentes, incluyendo loT y redes sociales.
- Transformación avanzada: aplicación de machine learning para limpieza y enriquecimiento de datos. Uso de herramientas como Apache Spark para procesamiento distribuido.
- Carga eficiente: implementación en la nube con servicios como AWS Glue y Azure Data Factory, que permiten escalabilidad y flexibilidad en la gestión de grandes volúmenes de datos.
- Automatización: automatización de flujos de trabajo ETL con orquestadores como Apache Airflow, mejorando la eficiencia y reduciendo errores humanos.

## Resumen 
Los ETL ayudan a las empresas a integrar datos de multiples fuentes de datos, transformarlos de manera efectiva para luego almacenarlos en data warehouse para mejorar la toma de decisiones. 

Un Data Warehouse está optimizado para consultas rápidas y complejas, lo que mejora su rendimiento para análisis específicos.

La etapa de transformación implica limpiar, integrar y convertir los datos para que sean adecuados para el análisis.

La automatización de flujos de trabajo ETL mejora la eficiencia y reduce errores humanos, utilizando herramientas avanzadas.



## Videoclase 2. Almacenamiento de Datos

## Bases de datos Relacionales 
- Organización: datos almacenados en tablas con filas y columnas. Ideales para almacenar datos estructurados.
- Ventaja: integridad. Aseguran la consistencia y precisión mediante claves primarias y relaciones facilidad de uso, soporte amplio.
- SQL: utilizan el lenguaje de consulta estructurado (SQL) para gestionar y manipular datos. Permiten realizar consultas complejas.
- Normalización: estructuración para reducir redundancia y mejorar integridad de los datos.
- Desventajas: No son adecuadas para datos no estructurados o semiestructurados.


## Data Lake
- Almacenamiento: capacidad de almacenar grandes volúmenes de datos en diversos formatos.
- Flexibilidad: permiten almacenar datos sin procesar y definir el esquema durante el acceso. Almacena datos en su formato nativo, sin procesar. Puede contener datos estructurados, semiestructurados y no estructurados (schema-on-read)
- Costo: generalmente más económicos debido a su arquitectura escalable.
- Accesibilidad: datos accesibles para múltiples aplicaciones y análisis.
- Desventajas: riesgo de convertirse en un Data Swamp (Data Lake mal gestionado, donde los datos se almacenan sin un orden o metadatos adecuados).
- Un Data Lake puede convertirse en un Data Swamp si los datos se almacenan sin orden ni metadatos adecuados.

## Data Warehouses
- Almacenamiento: datos organizados en estructuras optimizadas para consultas y análisis (Schema-on-write), donde los datos deben cumplir con un esquema definido antes de ser almacenados.
- ETL: procesos de extracción, transformación y carga (ETL) para asegurar la calidad y consistencia de los datos.
- Ventajas: diseño optimizado para realizar consultas rápidas y complejas.. Alta calidad de datos, optimización para consultas analíticas.
- Rendimiento: alta velocidad y eficiencia en la ejecución de consultas. Desventajas: mayor rigidez comparado con Data Lakes, mayor costo de implementación.

## Data Mart
- Un Data Mart es una versión más pequeña y especializada de un Data Warehouse, enfocada en un área específica de la organización. Contiene una parte pequeña y específica de los datos que la empresa almacena en un sistema de almacenamiento más grande.
- Ventajas: menor tiempo de implementación, específico para necesidades de departamentos.
- Desventajas: puede llevar a la creación de silos de datos.

## Data Mesh (una alternativa de almacenamiento)
- Descentralización del almacenamiento de datos: permite a los equipos independientes gestionar y almacenar sus propios datos. Data Lake tiene un enfoque centralizado en un único repositorio que almacena todos los datos, mientras que el Data Mesh distribuye los datos entre varios dominios.
- Escalabilidad y flexibilidad: mejora la escalabilidad y flexibilidad al distribuir la gestión de datos.
- Responsabilidad de los datos: cada equipo es responsable de la calidad y consistencia de sus datos.
- Interoperabilidad: facilita la interoperabilidad entre diferentes sistemas y fuentes de datos.

## Tecnologías de almacenamiento en la nube
- Escalabilidad: capacidad de escalar recursos de almacenamiento según la demanda.
- Costo: modelos de pago por uso, permitiendo costos más bajos y predecibles.
- Accesibilidad: acceso a datos desde cualquier lugar y en cualquier momento.
Elasticidad: ajuste dinámico de recursos según las necesidades del negocio.
- Herramientas:
o AWS S3: almacenamiento de objetos escalable y de alta durabilidad. o Microsoft Azure Blob Storage: almacenamiento de objetos optimizado para datos no estructurados.
o Google Cloud Storage: solución unificada de almacenamiento de objetos para desarrolladores y empresas.


## Impacto en el análisis de datos
- Data Lakes:
    - Exploración de datos: facilitando el análisis exploratorio y la investigación.
    - Machine Learning: soporte para grandes volúmenes de datos no estructurados, ideal para entrenar modelos de ML.
    - Flexibilidad: permite adaptarse rápidamente a nuevas fuentes y tipos de datos.

- Data Warehouses:
    - Análisis de BI: optimización para consultas rápidas y generación de informes.
    - Integridad de datos: garantiza la calidad y consistencia de los datos.
    - Rendimiento: diseñado para manejar grandes volúmenes de consultas concurrentes.

## Videoclase 3. Visualización Avanzada de Datos

## Propósito de la visualización de datos
- Claridad:
    - Facilita la comprensión de grandes volúmenes de datos. Simplifica la complejidad de los datos crudos.
    - Ejemplos: diagramas de dispersión, gráficos de líneas. Comunicación efectiva:
    - Permite compartir insights de manera clara y concisa. Mejora la colaboración entre equipos.
    - Ejemplos: Dashboards interactivos.
    
- Detección de patrones:
    - Ayuda a identificar tendencias y patrones ocultos en los datos.
    - Facilita la identificación de anomalías.
    - Ejemplos: mapas de calor, gráficos de series temporales.

- Toma de decisiones:
    - Apoya la toma de decisiones basada en datos concretos y visuales.
    - Reduce la incertidumbre en la toma de decisiones.
    - Ejemplos: informes de Business Intelligence (BI).

## Herramientas de la visualización de datos
- Datawrapper:
    - Programa de código abierto para crear visualizaciones fácilmente. Ideal para periodistas y comunicadores.

- Timeline JS:
    - Construcción de líneas de tiempo interactivas.
    - Utilizado en presentaciones históricas y narrativas.

- RAWGraphs:
    - Visualizaciones con D3.js sin necesidad de programación. Flexible y altamente personalizable.

- CartoDB:
    - Mapas interactivos en la web.
    - Análisis geoespacial avanzado.

- Visualización e inteligencia empresarial
    - AWS, Microsoft Azure, Google Cloud Platform:
    - Integración de fuentes de datos y procesamiento.
    - Infraestructura escalable y segura.

- Tableau:

    - Conexión con diversas fuentes de datos y creación de cuadros de mando.
    - Interfaz intuitiva y potente capacidad de análisis.
    - Capacidad de manejar grandes volúmenes de datos.

- Google Data Studio:
    - Dashboards gratuitos y conectores de terceros.
    - Fácil de usar y compartir.
    - Interactividad y personalización.

- Power BI:
    - Visualizaciones de inteligencia empresarial con reconocimiento por voz.
    - Integración con Microsoft Office y otras herramientas.
    - Análisis en tiempo real y predicciones.



## Otras herramientas de visualización
- Looker:Solución de inteligencia empresarial de Google. Análisis de datos en tiempo real y exploración.
- Qlik:Visualización y análisis de datos IoT. Potente motor de asociación de datos.
- Adverity:Plataforma para ingestión, procesamiento y presentación de datos. Conexión a múltiples fuentes de datos y automatización de ETL.
- Funnel:Conexión de datos de múltiples fuentes con destinos diversos. Simplificación del marketing digital y análisis de rendimiento.

## Lenguajes de Programación para Visualización
    - Python:
        - Librerías: Matplotlib, Seaborn, Plotly, Bokeh.
        - Usado en machine learning, data science, inteligencia artificial.
        - Amplia comunidad y numerosas librerías.
        - Integración con proyectos de machine learning.

    - R:
        - Librerías: ggplot2, Shiny, Plotly.
        - Popular en análisis estadístico y académico.
        - Potente en análisis estadístico.
        - Interactividad y extensibilidad.


## Python y visualization
Matplotlib:
Capacidad de personalización avanzada.
Seaborn:
Mejora la estética y la capacidad gráfica. Se integra con R y Python.
Plotly:
Gráficos interactivos en web.
Compatible con Dash para dashboards interactivos.
Bokeh:
Visualización interactiva de datos a gran escala. Utilizado en análisis de grandes conjuntos de datos.

## R y visualization
- ggplot2:
    - Gráficos en una única figura.
    - Extensibilidad y personalización.
- Shiny:
    - Aplicaciones web interactivas para visualización de datos.
    - Facilitación de la creación de dashboards.
- Plotly en R:
    - Gráficos interactivos y altamente personalizables.
    - Integración con aplicaciones Shiny.
- Paquetes adicionales:
    - rgl para gráficos 3D.
    - Leaflet para mapas interactivos.

## Desafíos en la visualización de datos
- Desafíos:
    - Gestión de grandes volúmenes de datos.
    - Garantizar la precisión y calidad de los datos.
    - Elegir herramientas según necesidades específicas.
    - Considerar factores como usabilidad, costo y capacidades.
    - Formación continua en nuevas herramientas y tecnologías. Importancia de la alfabetización de datos.
    - Integrar visualización de datos en la estrategia empresarial.
    - Fomentar una cultura de datos y toma de decisiones basada en datos.

- Algunas soluciones:
    - Uso de herramientas avanzadas de visualización.
    - Implementación de buenas prácticas de gobernanza de datos.

- Tecnologías de apoyo:
    - Big Data, Machine Learning, inteligencia artificial. Integración con plataformas de análisis de datos.

- Beneficios:
    - Mejor toma de decisiones.
    - Eficiencia operativa y reducción de costos.

## Cuestionario 

1. ¿Qué nombre recibe un conjunto de datos persistente utilizado por un sistema de software?
    A. Archivo.
    B. Base de datos.
    C. Registro.
    D. Las respuestas A y B son correctas. -> Correcto  

2. ¿Qué tipo de datos puede almacenar un data warehouse?
    A. Datos estructurados.   --> Correcto 
    B. Datos no procesados.
    C. Las respuestas A y B son correctas.
    D. Ficheros planos.

3. Al proceso de utilizar métodos de minería de datos para extraer lo que se
considera conocimiento según la especificación de medidas y umbrales, utilizando
una base de datos junto con procesos de transformación de los datos se lo conoce
como:
    A. CRISP-DM.
    B. ETL.
    C. KDD.
    D. Machine learning.  --> Correcto 


4. Entre las ventajas de la preparación de los datos nos encontramos con las
siguientes (marca todas las correctas):
    A. Preparar los datos para el análisis de forma rentable y eficiente.
    B. Garantizar que los datos utilizados para el BI tengan niveles de calidad
    suficientes.
    C. Crear duplicados de los datos para que puedan utilizarse en múltiples
    aplicaciones de forma segura.
    D. Todas son correctas. -> Correcto 


5. La limpieza de datos corrige problemas como:
    A. Datos duplicados.
    B. Datos redundantes. -> Correcto 
    C. Datos no estructurados.
    D. Datos incoherentes.

6. ¿Cuáles de los siguientes repositorios de datos almacenan datos no
estructurados, semiestructurados y estructurados?
    A. Data warehouses.
    B. A y C son correctas.
    C. Data lakes. -> Correcto 
    D. Data marts.

7. ¿Cuáles de los siguientes repositorios de datos siguen una estructura de
procesamiento schema on write? Marca todas las correctas:
    A. Data swamps.
    B. Data warehouses.
    C. Data lakes. -> Correcto
    D. Data marts.

8. ¿Cuál de las siguientes afirmaciones describe mejor el propósito del proceso
ETL?
    A. Procesar eventos en tiempo real para análisis instantáneo.
    B. Integrar datos de múltiples fuentes en un formato homogéneo. -> Correcto 
    C. Enviar y recibir mensajes entre diferentes aplicaciones.
    D. Visualizar datos para reportes y paneles de control.

9. ¿Cuál es una fase crítica del proceso ETL donde se aplican reglas para corregir o
eliminar datos incorrectos o incompletos?
    A. Extracción.
    B. Transformación. -> correcto 
    C. Carga.
    D. Almacenamiento.

10. ¿Qué aspecto del proceso ETL se enfoca principalmente en mejorar el
rendimiento de las consultas y la escalabilidad del sistema de almacenamiento de
datos?
    A. Optimización de la extracción.
    B. Paralelización de la transformación.
    C. Incremento en la frecuencia de carga.
    D. Diseño del esquema de datos -> Duda





# Tema 4:  El Perfil del Científico de Datos

En el mundo actual, dominado por datos y tecnología, el papel del científico de datos
se ha vuelto indispensable en las organizaciones que buscan capitalizar la vasta
cantidad de información disponible. Este perfil profesional combina habilidades en
ciencias de la computación, matemáticas y estadística, comunicación y
conocimientos de negocios para extraer patrones significativos, predecir tendencias
futuras y proporcionar recomendaciones basadas en datos que impulsan las
decisiones estratégicas.

La importancia de los científicos de datos radica en su capacidad para no solo
manejar grandes volúmenes de datos sino también en transformar estos datos en
métricas accionables que pueden traducirse en ventajas competitivas para las
empresas

Los científicos de datos actúan como puentes entre los datos técnicos y las
decisiones de negocio, empleando su experiencia técnica para solucionar problemas
complejos y comunicando sus hallazgos de manera efectiva a los stakeholders de la
empresa. En una era donde los datos se generan a una velocidad y volumen sin
precedentes, estos profesionales son clave para navegar por el ruido informativo y
descubrir la información valiosa que subyace.

## Ciencias de la computación
En el campo de la ciencia de datos, las ciencias de la computación desempeñan un
papel esencial debido a que proporcionan la base técnica que permite el análisis y
manejo eficaz de grandes volúmenes de datos. La ciencia de la computación ofrece
las herramientas y técnicas necesarias para crear sistemas capaces de procesar,
almacenar y analizar datos a una escala sin precedentes.



## Importancia de la Ciencia de la Computación en la Ciencia de Datos

## Desarrollo de Algoritmos y Modelos de Aprendizaje Automático: 

- Los científicos de datos utilizan su conocimiento en ciencias de la computación para desarrollar algoritmos complejos. 
- Se desarrollan modelos que son fundamentales para transformar grandes
conjuntos de datos en insights accionables que pueden influir en decisiones críticas
de negocio.

## Gestión de Grandes Volúmenes de Datos: 

- Se necesitan habilidades en bases de datos, estructuras de datos y algoritmos, los científicos de datos están equipados para manejar y optimizar bases de datos y sistemas de almacenamiento de datos, asegurando que los datos se almacenen de manera eficiente y sean accesibles para el análisis.

## Optimización del Rendimiento de las Consultas: 

- El conocimiento en ciencias de la computación también permite a los científicos de datos optimizar las consultas a bases de datos para mejorar el rendimiento y la velocidad del análisis de datos, lo cual es crucial en entornos empresariales donde el tiempo de respuesta es crítico.

## Desarrollo de Software y Herramientas de Análisis: 

- La programación es una habilidad central en ciencias de la computación que los científicos de datos utilizan para escribir scripts y desarrollar software que automatiza la recopilación, el procesamiento y el análisis de datos.



## Videoclase 1. Competencias Fundamentales del Científico de Datos

## Notas: 
- La opción correcta es porque el científico de datos utiliza su conocimiento para convertir datos en información valiosa, apoyando así la toma de decisiones estratégicas en la empresa.   

- La opción correcta es porque las habilidades cuantitativas incluyen el uso de técnicas matemáticas, estadística e informática para analizar datos, identificar patrones y validar hipótesis. 

- La opción correcta es D porque el análisis de series de tiempo permite identificar patrones temporales y realizar predicciones, fundamental en sectores como finanzas y economía. 

- La opción correcta es porque la explicabilidad ayuda a entender las decisiones de los modelos de machine learning, asegurando su transparencia y confiabilidad. 

- La opción correcta es porque el científico de datos necesita habilidades en programación y manejo de datos a gran escala.

## Videoclase 2. La Comunicación en Ciencia de Datos

## Notas
- La opción correcta es porque la comunicación efectiva permite que los hallazgos sean entendidos por todos los interesados, facilitando la toma de decisiones. 

- La opción correcta es porque la comunicación visual utiliza representaciones gráficas para facilitar la comprensión de datos complejos

- La opción correcta es porque la comunicación escrita debe ser clara, detallada y estructurada para documentar y compartir hallazgos de manera efectiva

- La opción correcta es porque la integridad implica representar los datos de manera precisa, asegurando que no se distorsione la información

- La opción correcta es porque cada sector requiere un enfoque de comunicación distinto para garantizar que los datos sean relevantes y comprendidos por las partes interesadas

## Videoclase 3. Aplicaciones Prácticas en Negocios

## Notas
- La opción correcta es porque la ciencia de datos permite personalizar el contenido en función de las preferencias de los usuarios, mejorando la retención.
- La opción correcta es porque Amazon utiliza algoritmos para prever la demanda y optimizar la logística, mejorando su eficiencia operativa
- La opción correcta es porque Starbucks utiliza ciencia de datos para seleccionar ubicaciones estratégicas que maximicen la visibilidad y el tráfico.
- La opción correcta es porque Goldman Sachs utiliza modelos predictivos para manejar riesgos financieros y tomar decisiones informadas. 
- La opción correcta es porque Uber optimiza rutas y tiempos de espera usando algoritmos avanzados, mejorando la experiencia del usuario.



## Habilidades 

- Predicción de Demanda: utiliza modelos estadísticos y de series temporales para
prever la demanda futura de productos o servicios, permitiendo a las empresas
ajustar la producción, el inventario y la planificación de la logística.

- Sistemas de Recomendación: emplea técnicas de álgebra lineal y algoritmos de
aprendizaje automático para recomendar productos, películas o música a los
usuarios basándose en sus intereses y comportamientos pasados.

- Detección de Fraude: aplica algoritmos de clasificación y patrones estadísticos para
identificar transacciones o comportamientos anómalos que puedan indicar fraude en
sectores como banca y seguros.

- Optimización de Rutas: usa algoritmos de optimización para determinar la ruta más
eficiente en términos de costos y tiempo para la entrega de mercancías o la
planificación de rutas de transporte público.

- Análisis de Sentimiento: implementa modelos matemáticos para analizar y clasificar
opiniones de los usuarios en datos textuales, como reseñas o comentarios en redes
sociales, determinando si son positivas, negativas o neutrales.

- Segmentación de Mercado: utiliza técnicas de clustering y análisis de componentes
principales (PCA) para identificar segmentos de clientes basados en características
similares, lo que ayuda a las empresas a dirigir sus estrategias de marketing de
manera más efectiva.

- Evaluación de Riesgos: emplea modelos de regresión y simulaciones Monte Carlo
para evaluar y cuantificar los riesgos financieros, como el crédito o el mercado,
ayudando a las instituciones financieras en la toma de decisiones.

- Modelado de Propagación de Enfermedades: aplica modelos matemáticos de
epidemiología para predecir la propagación de enfermedades y evaluar la efectividad
de las intervenciones de salud pública.

- Análisis de Redes Sociales: usa teoría de grafos y algoritmos para analizar redes
sociales, identificando patrones de conexión, influenciadores clave y comunidades
dentro de las redes.

- Valoración de Activos: emplea modelos financieros y estadísticos para determinar el
valor justo de diversos activos, incluyendo acciones, bonos y derivados

## Comunicación

- La comunicación es una habilidad crítica en la ciencia de datos, vital para el éxito de cualquier proyecto de análisis de datos. 

- El científico de datos no solo necesita ser competente en técnicas estadísticas y de programación, sino también en la habilidad de comunicar hallazgos complejos de manera clara y persuasiva a un público diverso. 

- Comunicación de Resultados

    - Esto implica presentar los datos de manera que resalten las conclusiones clave sin perderse en detalles técnicos innecesarios.

    - Herramientas como visualizaciones de datos, dashboards interactivos y presentaciones claras son fundamentales. 
    
    - Por ejemplo, un científico de datos puede usar una visualización de gráfico de calor para demostrar áreas de alta actividad en un estudio de mercado, facilitando la comprensión rápida de datos complejos.


- Presentación de Avances de los Proyectos de Datos

    - Durante la gestión de proyectos de datos, comunicar los avances de manera efectiva asegura que todas las partes interesadas estén informadas sobre el progreso, los desafíos y los cambios en los objetivos del proyecto. 
    
    - Esto es crucial para mantener alineados a todos los miembros del equipo y para gestionar las expectativas de los
    stakeholders. 
    
    - Ejemplo de esto sería la actualización periódica a través de reuniones
    regulares donde se presentan métricas de progreso y se discuten las necesidades de
    ajustes en la estrategia o recursos del proyecto.

- Impacto de los Proyectos en la Sociedad

    - Los científicos de datos también tienen la responsabilidad de comunicar cómo los proyectos de datos impactan en la sociedad. 

    - Un científico de datos eficaz debe ser capaz de narrar una historia con los datos. 

    - Por ejemplo, en proyectos que involucran datos de salud pública, es vital comunicar cómo se manejan y protegen los datos para evitar preocupaciones sobre privacidad.


## Aspectos relevantes de la comunicación según el ámbito


- Sector Sanitario
    - En el ámbito sanitario, la comunicación debe manejar con cuidado la privacidad y la sensibilidad de la información personal de salud. 

- Sector Gubernamental
    - La comunicación en el sector gubernamental debe ser transparente y diseñada para fomentar la confianza pública.

- Ética en Ciencia de Datos
    - La comunicación en temas de ética implica discutir cómo se manejan los datos y las implicaciones de los proyectos de ciencia de datos. 
    - Esto incluye temas como el sesgo en los algoritmos y la equidad en el análisis de datos.

- Sostenibilidad
    - En el campo de la sostenibilidad, la comunicación se centra en cómo los proyectos de datos pueden ayudar a resolver problemas ambientales o mejorar la eficiencia de recursos.

## Notas: 

- Análisis de sentimiento:
    - Concepto: Es una técnica que utiliza la inteligencia artificial para determinar la opinión o emoción expresada en un texto. Puede ser positiva, negativa o neutral. Imagina que tienes miles de reseñas de un producto en Amazon, el análisis de sentimiento te permitiría identificar rápidamente si la mayoría de los clientes están satisfechos o no.
    
    - Ejemplo: Una empresa de aerolíneas utiliza el análisis de sentimiento para monitorear las opiniones de sus clientes en redes sociales. De esta manera, pueden identificar rápidamente problemas en sus servicios y tomar medidas correctivas.

- Algoritmos predictivos:

    - Concepto: Son modelos matemáticos que utilizan datos históricos para predecir futuros resultados. Son como adivinos, pero basados en datos. Por ejemplo, pueden predecir la demanda de un producto en un mes determinado, o la probabilidad de que un cliente abandone una empresa.

    - Ejemplo: Un banco utiliza algoritmos predictivos para identificar a los clientes con mayor probabilidad de solicitar un préstamo. De esta manera, pueden ofrecerles productos financieros personalizados.

- Minería de texto:

    - Concepto: Es el proceso de extraer información útil y significativa de grandes volúmenes de texto no estructurado. Imagina que tienes una enorme base de datos de correos electrónicos, la minería de texto te permitiría encontrar patrones, temas y tendencias en esos correos.
    - Ejemplo: Una empresa de investigación de mercado utiliza la minería de texto para analizar las transcripciones de entrevistas con clientes. De esta manera, pueden identificar las principales preocupaciones de sus clientes y mejorar sus productos y servicios.

- Análisis de redes sociales:

    - Concepto: Es el estudio de las interacciones sociales y el contenido generado en las redes sociales. Permite entender cómo se comporta una comunidad en línea, identificar influyentes y analizar la opinión pública sobre un tema determinado.
    - Ejemplo: Una marca de ropa utiliza el análisis de redes sociales para monitorear las conversaciones sobre sus productos en Twitter e Instagram. De esta manera, pueden identificar a los usuarios que influyen en la opinión de otros y colaborar con ellos para promocionar sus productos.


- Clustering:
    - Concepto: Es una técnica de aprendizaje no supervisado que busca agrupar un conjunto de datos en subconjuntos (clusters) de manera que los elementos dentro de cada grupo sean más similares entre sí que con los elementos de otros grupos.
    - Objetivo: Identificar patrones ocultos en los datos y crear categorías naturales.
    - Ejemplo: Segmentar a los clientes de una empresa en grupos con características similares (por ejemplo, edad, ingresos, hábitos de compra) para diseñar estrategias de marketing más efectivas.


- Regresión lineal:
    - Concepto: Es una técnica estadística que busca modelar la relación lineal entre una variable dependiente (a la que queremos predecir) y una o más variables independientes.
    - Objetivo: Predecir valores futuros de la variable dependiente basándose en los valores de las variables independientes.
    - Ejemplo: Predecir el precio de una vivienda en función de su tamaño, ubicación, número de habitaciones, etc.

- Aprendizaje automático:
    - Concepto: Es un subcampo de la inteligencia artificial que se enfoca en desarrollar algoritmos y modelos estadísticos que permiten a las computadoras aprender de los datos y realizar tareas sin ser programadas explícitamente para cada tarea
    - Objetivo: Crear sistemas inteligentes capaces de tomar decisiones, hacer predicciones y aprender de nuevas experiencias.
    - Ejemplo: Reconocimiento de voz, detección de fraudes, sistemas de recomendación.

- Análisis factorial:
    - Concepto: Es una técnica estadística que busca reducir la dimensionalidad de un conjunto de datos, identificando un número menor de variables latentes (factores) que explican la mayor parte de la variabilidad en los datos originales.
    - Objetivo: Simplificar la interpretación de los datos y descubrir las estructuras subyacentes que relacionan a las variables observadas.
    - Ejemplo: Identificar los factores que influyen en la satisfacción del cliente en una encuesta, reduciendo un gran número de preguntas a un conjunto más pequeño de factores subyacentes.


- Clustering: sirve para agrupar datos.
- Regresión lineal: se utiliza para predecir valores.
- Aprendizaje automático es un campo más amplio que abarca diversas técnicas para crear sistemas inteligentes.
- Análisis factorial: reduce la complejidad de los datos y revela estructuras latentes.


- la paralelización de procesos: En el ámbito de la ciencia de la computación, especialmente cuando se trabaja con grandes volúmenes de datos, la paralelización de procesos es una técnica esencial.
    - Aumento de velocidad: Al dividir un problema en múltiples subproblemas y asignarlos a diferentes procesadores o núcleos de un procesador, se pueden resolver tareas complejas en mucho menos tiempo.
    - Escalabilidad: La paralelización permite aprovechar el poder de cálculo de sistemas multi-core y clusters, lo que es fundamental para manejar conjuntos de datos cada vez más grandes.
    - Eficiencia: Al aprovechar al máximo los recursos computacionales disponibles, se optimiza el uso de hardware y se reducen los costos.

- Programación funcional: Aunque es un paradigma de programación poderoso, no es específico para el procesamiento de grandes volúmenes de datos.
- Algoritmos de ordenamiento: Son importantes para organizar datos, pero no resuelven el problema de procesar grandes volúmenes de datos en sí mismos.
- Uso de variables estáticas: Las variables estáticas tienen un alcance específico y no están directamente relacionadas con la capacidad de procesar grandes conjuntos de datos.


- El análisis de cluster: En el contexto del marketing digital, esto significa que podemos agrupar a los usuarios en segmentos basados en sus comportamientos, intereses, demografía y otras variables relevantes.
- Cálculo integral: Se utiliza para calcular áreas bajo curvas y volúmenes, no para agrupar datos.
- Álgebra lineal: Se utiliza para resolver sistemas de ecuaciones lineales y realizar transformaciones lineales, pero no es la herramienta principal para la segmentación.
- Probabilidad y estadística: Si bien son fundamentales para el análisis de datos, no se refieren específicamente a la técnica de agrupar datos en segmentos.


- Simulaciones Monte Carlo

    - Concepto: Es una técnica estadística que utiliza números aleatorios para simular un proceso o sistema muchas veces. Cada simulación produce un resultado diferente, lo que permite obtener una distribución de posibles resultados y evaluar la probabilidad de diferentes eventos.
    - Ejemplo: Se utiliza para estimar el valor futuro de una inversión en la bolsa, considerando la volatilidad del mercado y otros factores aleatorios.
    - Aplicaciones:
        - Finanzas: Valoración de opciones, gestión de riesgos, simulación de carteras de inversión.
        - Ingeniería: Análisis de confiabilidad de sistemas, optimización de procesos.
        - Ciencias naturales: Modelado de fenómenos físicos y biológicos.
        - Gestión de proyectos: Estimación de costos y plazos.

- Análisis de ubicación

    - Concepto: Es un conjunto de técnicas que ayudan a determinar la mejor ubicación para una nueva instalación (fábrica, tienda, almacén, etc.) considerando factores como la demanda, los costos de transporte, la accesibilidad y la competencia.
    - Ejemplo: Una empresa de logística busca encontrar el lugar óptimo para construir un nuevo centro de distribución para minimizar los costos de transporte a sus clientes.
    - Aplicaciones:
        - Logística: Diseño de redes de distribución, selección de ubicaciones para almacenes.
        - Marketing: Selección de ubicaciones para tiendas, análisis de zonas de influencia.
        - Planificación urbana: Ubicación de servicios públicos, análisis de impacto de nuevas construcciones.

- Modelos de regresión

    - Concepto: Son herramientas estadísticas que permiten modelar la relación entre una variable dependiente (lo que queremos predecir) y una o más variables independientes (factores que influyen en la variable dependiente).
    - Ejemplo: Predecir el precio de una vivienda en función de su tamaño, ubicación, número de habitaciones, etc.
    - Aplicaciones:
        - Economía: Análisis de series de tiempo, predicción de ventas.
        - Marketing: Análisis de la relación entre las campañas publicitarias y las ventas.
        - Ciencias sociales: Estudio de la relación entre variables socioeconómicas.

- Análisis de la competencia

    - Concepto: Es el proceso de recopilar y analizar información sobre los competidores de una empresa para identificar sus fortalezas, debilidades, estrategias y oportunidades.
    - Ejemplo: Una nueva empresa de tecnología quiere entrar al mercado y analiza a sus principales competidores para identificar un nicho de mercado donde pueda diferenciarse.
    - Aplicaciones:
        - Desarrollo de estrategias de negocio: Identificación de oportunidades de crecimiento, desarrollo de nuevos productos o servicios.
        - Fijación de precios: Análisis de los precios de la competencia.
        - Marketing: Diseño de campañas publicitarias diferenciadas.

## Cuestionario 

1. ¿Qué técnica utilizan las empresas como Amazon para optimizar su cadena de suministro?
    A. Análisis de sentimiento.
    B. Algoritmos predictivos. -> Correcto
    C. Minería de texto.
    D. Análisis de redes sociales.

2. ¿Qué método utiliza Netflix para personalizar las recomendaciones a sus usuarios?
    A. Clustering.
    B. Regresión lineal.
    C. Aprendizaje automático. -> Correcto
    D. Análisis factorial.

3. ¿Qué herramienta matemática es crucial en la evaluación de riesgos financieros en empresas como Goldman Sachs?
    A. Cálculo diferencial.
    B. Teoría de grafos.
    C. Modelos predictivos. -> correcto 
    D. Geometría analítica.

4. En el contexto de la ciencia de la computación, ¿qué técnica es fundamental para el procesamiento de grandes volúmenes de datos?
    A. Programación funcional.
    B. Algoritmos de ordenamiento.
    C. Paralelización de procesos. -> Correcto 
    D. Uso de variables estáticas.

5. ¿Qué técnica matemática es ampliamente utilizada para segmentar audiencias en marketing digital?
    A. Cálculo integral.
    B. Análisis de cluster. -> Correcto 
    C. Álgebra lineal.
    D. Probabilidad y estadística.

6. ¿Cómo contribuye la teoría de grafos en la ciencia de datos?
    A. Optimización de algoritmos de búsqueda.
    B. Mejora de interfaces gráficas.
    C. Análisis de redes sociales. -> Correcto 
    D. Desarrollo de juegos.

7. ¿Qué metodología utiliza Starbucks para determinar las ubicaciones óptimas para sus nuevas tiendas?
    A. Simulaciones Monte Carlo.
    B. Análisis de ubicación. -> correcto 
    C. Modelos de regresión.
    D. Análisis de la competencia.

8. ¿Cuál es un ejemplo de aplicación de regresión lineal en ciencia de datos?
    A. Predecir el precio futuro de las acciones. -> correcto 
    B. Codificar datos para algoritmos de cifrado.
    C. Crear gráficos interactivos.
    D. Diseñar bases de datos.

9. ¿Qué representa la comunicación efectiva de los resultados en proyectos de ciencia de datos en negocios?
    A. Publicar papers académicos.
    B. Convencer a los stakeholders del valor de los hallazgos. -> correcto
    C. Implementar directamente los cambios en la producción.
    D. Ninguna de las anteriores.

10. ¿Qué rol juegan las visualizaciones de datos en la comunicación científica de datos?
    A. Solo para presentaciones académicas.
    B. Para simplificar el código.
    C. Para hacer los hallazgos comprensibles y accesibles. -> Correcto 
    D. Para aumentar la carga computacional.


# Tema 5. Áreas de aplicación de la Ciencia de Datos

## Videoclase 1. Ciencia de Datos en la Salud
## Notas: 

La ciencia de datos está revolucionando el ecosistema de la atención médica:
    - Desde los sistemas de salud digitalizados (HCE: Historia Clínica Electrónica). 
    - Gestión más eficiente y segura de los datos de los pacientes.
    - Diagnósticos más precisos (reducción de errores de diagnóstico, análisis avanzado de imágenes...).
    - Tratamientos personalizados (análisis de datos genómicos). El uso de big data ha revolucionado la secuenciación del genoma humano, permitiendo avances en la comprensión de enfermedades genéticas y el tratamiento individualizado.
    - Identificar patrones y relaciones en datos de investigaciones médicas que no serían visibles a simple vista.
    - Los modelos predictivos ayudan a identificar moléculas prometedoras para el desarrollo de nuevos medicamentos. Detección temprana de enfermedades.
    - Hasta la implicación de datos en la gestión de la salud pública.

    - ¿Cómo está revolucionando la ciencia de datos la atención médica?
        - La ciencia de datos está mejorando la precisión de los diagnósticos, personalizando los tratamientos y gestionando los datos de manera eficiente, lo que permite una atención médica más precisa y eficaz. Las otras opciones no reflejan este impacto integral.
    - ¿Cuál es una ventaja clave del diagnóstico asistido por IA?
        - El diagnóstico asistido por IA mejora la precisión y velocidad al identificar enfermedades, reduciendo significativamente los errores humanos. Las demás opciones no abarcan el beneficio completo de la IA en el diagnóstico.
    -  El diagnóstico asistido por IA mejora la precisión y velocidad al identificar enfermedades, reduciendo significativamente los errores humanos. Las demás opciones no abarcan el beneficio completo de la IA en el diagnóstico.
        - Los modelos predictivos permiten identificar factores de riesgo a partir de datos históricos, anticipando enfermedades y mejorando los resultados para los pacientes. Las otras opciones no reflejan esta capacidad predictiva.
    - ¿Por qué es crucial la monitorización en tiempo real en la atención médica?
        - La monitorización en tiempo real permite detectar cambios críticos y actuar rápidamente, mejorando la gestión de enfermedades crónicas. Las demás opciones no capturan esta utilidad crucial.
    - ¿Cómo ayuda la ciencia de datos a optimizar las operaciones hospitalarias?
        - La ciencia de datos optimiza la gestión hospitalaria mediante una asignación eficiente de recursos y planificación, reduciendo tiempos de espera y mejorando la calidad del servicio. Las otras opciones no reflejan estos beneficios. 

## Videoclase 2.  Ciencia de Datos en la IndustriaPágina
## Notas: 

- ¿Qué papel juega la ciencia de datos en la optimización de la cadena de suministro?
 - La ciencia de datos optimiza la cadena de suministro mediante la previsión de la demanda y la optimización de recursos, lo que reduce costos y tiempos de entrega. Las demás opciones no abarcan este papel integral.

- ¿Cómo beneficia el mantenimiento predictivo a la industria?
    - El mantenimiento predictivo utiliza técnicas avanzadas para prever fallos en equipos, reduciendo tiempos de inactividad y costos operativos.

- ¿Por qué es importante la automatización en el control de calidad?
    - La automatización en el control de calidad garantiza la precisión en la inspección de productos, reduciendo errores humanos y asegurando estándares de calidad

- ¿Cómo contribuye la logística inteligente a la cadena de suministro?
    - La logística inteligente utiliza tecnologías avanzadas para optimizar rutas y gestionar flotas en tiempo real, mejorando la eficiencia y reduciendo costos

- ¿Qué desafíos éticos enfrenta la ciencia de datos en la industria?    
    - Los desafíos éticos incluyen proteger la privacidad de los datos, garantizar la transparencia de los algoritmos y asegurar que las decisiones automatizadas sean justas

## Videoclase 3.  Ciencia de Datos en las Finanzas
## Notas: 
    - ¿Cómo impacta la ciencia de datos en la gestión de riesgos financieros?
        - Utiliza modelos predictivos y técnicas avanzadas para identificar riesgos y anticipar pérdidas potenciales.
    - ¿Cuál es la importancia del trading algorítmico en los mercados financieros?
        - Automatiza las operaciones de mercado, aprovechando patrones y tendencias para mejorar la rentabilidad.
    - ¿Qué técnicas se utilizan para el pronóstico de precios en finanzas?
        - Modelos de machine learning, series de tiempo, análisis técnico y fundamental para prever movimientos de precios.
    - ¿Cómo apoya la ciencia de datos la inclusión financiera?
        - Utiliza datos alternativos y machine learning para evaluar la solvencia de individuos sin acceso al crédito tradicional.
    - ¿Qué desafíos éticos enfrenta la ciencia de datos en finanzas?
        - Asegurar la privacidad, la transparencia de los modelos de IA y la equidad en las decisiones automatizadas.

## 5.2. Comercio

La ciencia de datos ha revolucionado el sector comercial al permitir una toma de
decisiones más informada y estratégica. Mediante el análisis de grandes volúmenes
de datos, las empresas comerciales pueden optimizar operaciones, personalizar
experiencias de clientes y mejorar la eficiencia en la cadena de suministro. La
capacidad de extraer insights valiosos de los datos es ahora un diferenciador
competitivo clave en el comercio.

Las técnicas avanzadas de ciencia de datos, como el machine learning, la
inteligencia artificial, y la analítica predictiva, están al frente de esta transformación.
Empresas líderes en el sector comercial utilizan estas tecnologías para abordar
desafíos específicos y mejorar su rendimiento. A continuación, se presentan
ejemplos específicos y concretos de la aplicación de la ciencia de datos en el
comercio, resaltando los beneficios obtenidos.

## Ventjas en el comercio 
- Optimización de Precios Dinámicos
- Gestión de Inventario y Cadena de Suministro
- Personalización de la Experiencia del Cliente
- Detección de Fraudes y Anomalías

La aplicación de la ciencia de datos en el comercio no solo facilita operaciones más
eficientes, sino que también mejora la interacción con el cliente, personalizando
experiencias y anticipando sus necesidades.

## 5.3. Industria

Uno de los pilares de la cuarta revolución industrial (Industria 4.0) ha sido la
implementación de sistemas big data que han permitido hacer un uso más eficiente
de los datos empresariales a todos los niveles, gestionar la ingente sensorización de
entornos industriales, vehículos o robots, o mejorar la toma de decisiones a través de
la implementación de soluciones business intelligence.

El sector industrial ha experimentado una transformación significativa con la
incorporación de la ciencia de datos, que permite optimizar procesos, reducir costos
operativos y mejorar la calidad del producto. Las técnicas avanzadas de analítica y
machine learning son esenciales para aumentar la eficiencia y la sostenibilidad en la
manufactura y otras actividades industriales

## Ventajas en la Industria 
- Mantenimiento Predictivo
- Optimización de la Producción
- Control de Calidad Automatizado
- Gestión de la Cadena de Suministro

## 5.4. Salud

La aplicación de múltiples técnicas relacionadas con el big data permite que los
servicios relacionados con los sistemas de salud mejoren sustancialmente. Por
ejemplo, con la inteligencia artificial (IA) se pueden realizar simulaciones de
diagnósticos o análisis de imágenes que ayuden a la toma de decisiones de los
médicos.

En un contexto de consulta, los médicos pueden analizar pruebas pasadas,
tendencias y datos actuales. Con esta información y el acceso a los datos relativos al
estilo de vida, el historial y la genética de un paciente, se facilita una imagen holística
que puede ayudar a los médicos a proporcionar la atención más adecuada. 

Se espera que la cantidad de datos sobre la salud a nivel global aumente
drásticamente en los próximos años. Las primeras estimaciones hechas hacia 2013
sugieren que hubo alrededor de 153 exabytes de datos de salud generados en ese
año, proyectando una generación de hasta 2314 exabytes de nuevos datos
generados en 2020

## Ventajas en la Salud 
- Mejoras en los dianosticos al usar patrones de reconocimiento en los historicos del paciente
- Una atención mas personalizada al aplicar marching learning en la secuenciación del genoma
- Mejoras administrativa en los recursos al predecir fallas y excases  en los suministros. 

## 5.5. Seguridad y Ciberseguridad

En el ámbito de la seguridad y ciberseguridad, la ciencia de datos ha emergido como
una herramienta crítica para detectar, prevenir y responder a amenazas. Utilizando
algoritmos de machine learning y análisis de grandes volúmenes de datos, las
organizaciones pueden identificar patrones anómalos y comportamientos
sospechosos que podrían indicar intentos de intrusión o brechas de seguridad.

## Ventajas en la Salud 
- Detección de Amenazas en Tiempo Real
- Análisis de Comportamiento del Usuario
- Phishing y Prevención de Fraudes
- Respuesta Automatizada a Incidentes
- 

## 5.6. Finanzas

Los sectores económico, financiero y bancario han experimentado cambios
importantes y profundos gracias a la aplicación de los sistemas big data, los cuales
eran difíciles de imaginar no hace mucho tiempo. Como ejemplo sencillo, el big data
permite a los sistemas financieros disponer de información en tiempo real de
cualquier mercado en cualquier parte del mundo

Bajo el paraguas de los sistemas big data y tecnologías como blockchain han surgido
en este ámbito las fintech. Maestre (2020) define a estas como «empresas
financieras tecnológicas que tratan de aportar nuevas ideas y que reformulan gracias
a las nuevas tecnologías de la información, las aplicaciones móviles o el big data, la
forma de entender y prestar los servicios financieros».

## Ventajas en las finanzas

- Gestión de datos en tiempo real y análisis predictivo. L
- Detección de fraude
- Gestión de datos de clientes. 
- Atención al cliente.
- Sistemas de recomendación.
- Segmentación de clientes. 
- Protección y mantenimiento de clientes.



## 5.7. Conducción Autónoma

Las técnicas de visión por computadora, procesamiento de señales, y redes
neuronales profundas son fundamentales en el desarrollo de sistemas autónomos de
conducción. Estos sistemas aprenden de vastas cantidades de datos recopilados
durante miles de horas de conducción para tomar decisiones en tiempo real. A
continuación, se presentan ejemplos específicos de cómo la ciencia de datos está
siendo aplicada en la conducción automática, destacando los beneficios tangibles.

## Ventajas en las Conducción Autónoma
- Percepción y Procesamiento Sensorial
- Predicción de Comportamiento de Otros Conductores y Peatones
- Optimización de Rutas y Gestión del Tráfico
- Entrenamiento y Validación de Sistemas Autónomos


## Sectores 

- Compras: las tarjetas de fidelización permiten identificar más fácilmente nuestros
hábitos de compra o preferencias, de forma que podamos recibir ofertas o
descuentos.

- Transporte: el uso de la tarjeta de transporte permite registrar cuándo y dónde
viajamos, de forma que las empresas de gestión pueden regular mejor los servicios.

- Deporte: los relojes inteligentes, las pulseras de actividad o el propio teléfono móvil
recogen información de nuestra actividad diaria o cuando practicamos algún tipo de
deporte, de forma que podemos estar monitorizados, seguir un plan de
entrenamiento o recibir incentivos para llevar un estilo de vida más saludable.

- Publicidad: Las redes sociales (Facebook, Instagram, Twitter, TikTok), buscadores
(Google, Bing) o marketplaces (Amazon, Alibaba) hacen uso de nuestra huella digital
para personalizar la publicidad que recibimos al hacer uso de los servicios que nos
ofrecen de forma gratuita.

- Salud: la información y la investigación en el ámbito de la salud se han visto
beneficiadas gracias a los sistemas big data, tal y como pudo comprobarse con la
información, casi en tiempo real, que recibimos sobre la pandemia provocada por la
COVID-19.

- Educación: la implantación de la educación online o la mejora de los métodos
tradicionales mediante el uso de herramientas innovadoras permiten aumentar el
rendimiento de los estudiantes y proporcionar realimentación a los profesores que
anteriormente era difícil de captar y digerir.

- Seguros: las aseguradoras utilizan estas tecnologías para ajustar las primas e
incluso incentivar a sus usuarios a reducir el riesgo, como sucede en aquellas que
recopilan la información proveniente de la actividad física de sus clientes gracias a la
conexión con las aplicaciones específicas (y la autorización de estos).

- Agricultura y ganadería: el uso de sistemas big data en la agricultura y la
ganadería ha permitido el desarrollo de sistemas predictivos y preventivos de gran
ayuda para la toma de decisiones, facilitando, por ejemplo, la detección de
infecciones en cultivos o la reducción de la consanguinidad en la cría de ganado en
pureza.

- Turismo: el sector turístico es otro de los mayores usuarios de sistemas big data,
tanto en la optimización de los sistemas de gestión de reservas como a nivel más
práctico, incluyendo la implementación de guías en dispositivos móviles o realidad
aumentada en museos, entre otros muchos.

- Telecomunicaciones: el sector de las telecomunicaciones es también un gran
consumidor de big data debido a la cada vez mayor cantidad de información que
deben gestionar sus redes.

## 5.8. Otros

Los Gobiernos y la Administración pública se han convertido en los últimos años en
grandes usuarios de los sistemas big data. La diversidad de funciones que cubren las
Administraciones públicas hace que el uso que se hace de los grandes conjuntos de
datos sea igualmente diverso.

La primera utilidad que podemos encontrar es más política que enfocada al
ciudadano: el análisis de la intención de voto. Existen un gran número de
empresas dedicadas a estimar cómo votaríamos los ciudadanos en un momento
determinado, de forma que la información que se recoge permita identificar y analizar
patrones que nos dejen descubrir el resultado de unas elecciones.

Por otro lado, y muy relacionada con el ejemplo anterior, la seguridad es otro de
los ámbitos en los que los Gobiernos se benefician de la aplicación de
sistemas big data. En múltiples países, como, por ejemplo, Estados Unidos, se
hace uso de modelos de riesgos que predicen la delincuencia identificando edificios o
zonas potencialmente peligrosas. Esto ayuda a que los cuerpos y fuerzas de
seguridad del estado dispongan de herramientas útiles que les permitan una reacción
más rápida ante situaciones que comprometan la seguridad ciudadana, utilizando
planos, información en tiempo real o índices de riesgo, entre otros muchos recursos.
De este modo, se puede reducir el número de robos, el uso indebido de armas de
fuego y, en definitiva, la delincuencia en genera


## Cuestionario 

1. ¿Qué técnica utilizan empresas como Amazon para ajustar los precios en tiempo
real?
A. Regresión lineal. 
B. Clustering.
C. Algoritmos predictivos. -> Correcto 
D. Análisis factorial.

2. ¿Cuál es una aplicación común de la ciencia de datos en la industria
manufacturera?
A. Publicidad en redes sociales.
B. Optimización de la cadena de suministro. -> Correcto 
C. Análisis de sentimientos.
D. Gestión de recursos humanos.

3. ¿Qué tecnología es fundamental en los vehículos autónomos para 'ver' el
entorno?
A. Análisis de texto.
B. Visión por computadora. -> Correcto 
C. Bases de datos NoSQL.
D. Blockchain.

4. ¿Qué método utilizan las instituciones financieras para detectar fraudes?
A. Análisis predictivo.
B. Minería de datos.
C. Modelos de regresión logística.
D. Todas las anteriores. -> Correcto 

5. ¿Cómo ayuda la ciencia de datos en la atención médica?
A. Optimización de precios.
B. Predicción de enfermedades.  -> Correcto 
C. Mejora de algoritmos de redes sociales.
D. Desarrollo de juegos.

6. ¿Cuál es un beneficio clave del uso de la ciencia de datos en la seguridad
cibernética?
A. Aumento de ventas.
B. Detección de amenazas en tiempo real. -> Correcto 
C. Mejora en la gestión de inventarios.
D. Optimización de rutas de transporte.

7. ¿Qué rol juega la ciencia de datos en la personalización de experiencias de
usuario en plataformas como Netflix?
A. Aprendizaje automático. -> Correcto 
B. Análisis de redes.
C. Simulaciones Monte Carlo.
D. Regresión lineal.

8. ¿En qué sector es crítico el uso de simulaciones para entrenar sistemas antes de
su implementación real?
A. Educación.
B. Conducción autónoma. -> Correcto 
C. E-commerce.
D. Hostelería.

9. ¿Qué aplicación de la ciencia de datos ayuda a Starbucks a decidir dónde abrir
nuevas tiendas?
A. Optimización de menús.
B. Análisis de ubicación.  -> Correcto
C. Gestión de recursos humanos.
D. Campañas de marketing digital. 

10. ¿Qué técnica es usada en el sector industrial para predecir el mantenimiento de
maquinaria?
A. Mantenimiento predictivo.  -> Correcto 
B. Gestión de la calidad del producto.
C. Optimización de la experiencia del cliente.
D. Análisis de sentimientos.


# Tema 6. Estrategias en Almacenamiento Masivo

6.2. Data Mart
> Un Data Mart es una estructura de almacenamiento de datos optimizada para un departamento o una función específica dentro de una organización, como ventas, marketing o finanzas. Se trata de una versión más pequeña y especializada de un Data Warehouse, diseñada para satisfacer las necesidades de un grupo particular de usuarios con datos específicos y relevantes (Inmon, W. H., Strauss, D, 2008).

**Principales características**
- Enfoque específico
- Menor tamaño
- Rápida implementación:
- Facilidad de uso
- Estructura simplificada

**Desventajas**
- Visión limitada: Al estar enfocados en un área específica, los Data Marts no proporcionan una visión global y consolidada de los datos de toda la organización.
- Duplicación de datos: En organizaciones con múltiples Data Marts, puede ocurrir duplicación de datos, lo que puede llevar a inconsistencias y problemas de integridad.
- Mantenimiento: Gestionar múltiples Data Marts puede ser más complicado y costoso en términos de mantenimiento y actualización, especialmente si no están integrados adecuadamente.
- Escalabilidad: Los Data Marts pueden no ser adecuados para empresas que necesitan escalar rápidamente el almacenamiento y el procesamiento de datos a nivel globa. 

6.3. Data Warehouse
Un Data Warehouse (almacén de datos) es una base de datos centralizada que recopila, integra y almacena grandes volúmenes de datos procedentes de diversas fuentes heterogéneas. Está diseñado para el análisis y la generación de informes,
proporcionando una visión coherente y consolidada de la información a lo largo del tiempo. Los datos en un Data Warehouse suelen ser históricos y están organizados para facilitar consultas complejas y análisis avanzados.

**Principales características**
- Integración de datos: Un Data Warehouse unifica datos de múltiples fuentes, como bases de datos operacionales, sistemas transaccionales, archivos y otras fuentes externas, en un formato coherente y homogéneo.
- Orientado al análisis: A diferencia de los sistemas transaccionales que manejan operaciones diarias, los Data Warehouses están optimizados para consultas complejas, análisis y generación de informes.
- Datos históricos: Almacena datos históricos que permiten realizar análisis de tendencias y comparativas a lo largo del tiempo.
- Alta disponibilidad y rendimiento: Diseñado para soportar un gran volumen de consultas y análisis sin afectar el rendimiento de los sistemas operacionales.
- Modelado de datos: Utiliza esquemas de datos específicos como el esquema estrella o el esquema copo de nieve, que facilitan el acceso y la consulta de datos.

**Desventajas**
- Coste de implementación: La creación y el mantenimiento de un Data Warehouse pueden ser costosos debido a la necesidad de hardware especializado, software y personal capacitado.
- Complejidad: La integración de datos de múltiples fuentes y el mantenimiento de la coherencia pueden ser complejos y requerir una gestión cuidadosa.
- Tiempo de carga: La extracción, transformación y carga (ETL) de datos en un Data Warehouse puede ser un proceso largo y complicado.
- Rigidez: Los Data Warehouses son menos flexibles para cambios rápidos en la estructura de datos o en los requisitos de análisis en comparación con sistemas más modernos como los Data Lakes.

6.4. Data Lake

Un Data Lake es una arquitectura de almacenamiento de datos que permite guardar
grandes volúmenes de datos en su formato original y sin procesar. A diferencia de
los sistemas tradicionales de almacenamiento, que requieren una estructura
predefinida y esquemas rígidos, un Data Lake ofrece una flexibilidad sin precedentes
al permitir que los datos se almacenen tal como llegan, sin necesidad de
estructurarlos previamente

**Principales características**

- Almacenamiento de datos crudos: Los Data Lakes permiten almacenar datos en su forma original sin necesidad de procesarlos o estructurarlos previamente, lo que facilita la ingesta de datos desde múltiples fuentes.
- Escalabilidad: Los Data Lakes están diseñados para escalar horizontalmente, lo que significa que pueden manejar grandes volúmenes de datos en crecimiento continuo sin afectar el rendimiento.
- Flexibilidad: Ofrecen la capacidad de almacenar diferentes tipos de datos, desde estructurados hasta no estructurados, permitiendo un análisis más completo y diverso.
- Economía: El almacenamiento en Data Lakes es generalmente más económico en comparación con los sistemas de almacenamiento estructurados, ya que no requieren costosos esquemas de preprocesamiento. 
- Acceso múltiple: Permiten el acceso simultáneo a los datos por parte de diferentes herramientas y lenguajes de análisis, facilitando el trabajo de científicos de datos, analistas y desarrolladores.
- Metadatos y catalogación: Los Data Lakes suelen incorporar herramientas de gestión de metadatos y catalogación que ayudan a los usuarios a encontrar y utilizar los datos de manera eficiente. 

**Desventajas**
- Calidad de datos: Al permitir la ingesta de datos sin procesar, los Data Lakes pueden enfrentarse a problemas de calidad de datos, como duplicación, datos incompletos o inconsistentes.
- Complejidad de gestión: Sin una gestión adecuada, un Data Lake puede convertirse en un "data swamp" (pantano de datos), donde los datos son difíciles de encontrar y utilizar.
- Rendimiento: La consulta de datos crudos en un Data Lake puede ser menos eficiente en comparación con los datos estructurados en un Data Warehouse.
- Seguridad y gobernanza: La flexibilidad de los Data Lakes puede dificultar la implementación de políticas de seguridad y gobernanza de datos, lo que puede llevar a riesgos de privacidad y cumplimiento normativo.


## Videoclase 1. Data Mart versus Data Warehouse
## Notas: 
- ¿Cuál es la principal diferencia entre un Data Mart y un Data Warehouse? 
    - Un Data Mart se enfoca en un área específica del negocio, mientras que un Data Warehouse integra datos de toda la organización.
- ¿Cuál es una ventaja clave de usar un Data Mart?
    - Ofrece una implementación rápida y costos reducidos para análisis específicos.
- ¿En qué escenario es más adecuado utilizar un Data Warehouse?
    - Cuando se requiere una vista integral y consolidada de todas las operaciones de una multinacional.
- ¿Qué limitación tiene el uso de un Data Mart?
    - Puede crear silos de datos, dificultando la integración y el análisis a nivel empresarial.
- ¿Por qué podría ser valioso utilizar tanto un Data Mart como un Data Warehouse en la arquitectura de datos?
    - Para combinar análisis específicos y rápidos con una visión integral y consolidada de los datos empresariales.
 
## Videoclase 2. Data Lake y su Importancia
## Notas: 
- ¿Qué es un Data Lake y cómo se diferencia de un Data Warehouse?
    - Un Data Lake almacena datos en su formato nativo y permite gestionar datos estructurados, semiestructurados y no estructurados a cualquier escala.
- ¿Cuándo podría no ser aconsejable utilizar un Data Lake?
    - Cuando se requiere una estructura rígida de datos y consultas complejas en entornos regulados.
- ¿Por qué los Data Lakes son esenciales en entornos de Big Data y Machine Learning?
    - Proporcionan un repositorio centralizado para grandes cantidades de datos diversos, necesarios para entrenar modelos de IA y Machine Learning.
- ¿Qué desventaja principal puede tener el uso de un Data Lake?
    - Puede ser complejo gestionar y acceder a los datos sin las herramientas adecuadas.
- ¿Qué es esencial para mantener la calidad de los datos en un Data Lake?
    - Establecer una gobernanza de datos robusta para asegurar la calidad y seguridad de los datos almacenados.

## Videoclase 3. Nuevas Tendencias en Almacenamiento Masivo
## Notas: 
- ¿Qué es el almacenamiento definido por software (SDS) y cómo se diferencia del almacenamiento tradicional?
    - SDS permite gestionar el almacenamiento a través de software, proporcionando una mayor flexibilidad y control sobre los recursos de almacenamiento. 
- ¿Cómo beneficia el Edge Storage a las aplicaciones de tiempo real?
    - Procesando y almacenando datos cerca de donde se generan, lo que minimiza la latencia y mejora la respuesta en tiempo real.
- ¿Qué ventaja ofrece el enfoque de Multi-Cloud en el almacenamiento de datos?
    - Mejora la resiliencia y evita la dependencia de un solo proveedor, optimizando el rendimiento.
- ¿Cómo se aplica el almacenamiento en la periferia (Edge Storage) en la industria de la salud?
    - Facilita la monitorización en tiempo real de pacientes y mejora el diagnóstico mediante dispositivos IoT con capacidad de almacenamiento local.
- ¿Qué tendencia emergente en almacenamiento explora el uso de ADN?
    - Codificación de datos digitales en secuencias de ADN sintético para almacenar grandes cantidades en espacios reducidos.


## Cuestionario 

1. ¿Qué es un Data Mart?

A. Un sistema de almacenamiento en la nube.
B. Una versión simplificada y específica de un Data Warehouse. -> correcto 
C. Un tipo de base de datos relacional.
D. Un método de análisis de datos en tiempo real.

2. ¿Cuál es una característica principal de los Data Marts?

A. Integración de datos de múltiples fuentes.
B. Almacenamiento de datos en su formato original.
C. Escalabilidad horizontal.
D. Enfoque específico en un área de negocio. -> Correcto 

3. ¿Cuál de las siguientes es una desventaja de los Data Marts?

A. Alta escalabilidad.
B. Flexibilidad en la gestión de datos. -> Duda
C. Pueden llevar a duplicación de datos.
D. Integración de múltiples fuentes de datos.

4. ¿Qué tipo de esquema es comúnmente utilizado en los Data Marts?

A. Esquema en red.
B. Esquema estrella. -> Correcto 
C. Esquema jerárquico.
D. Esquema de malla.

5. ¿Para qué tipo de proyectos son especialmente útiles los Data Marts?
A. Proyectos a corto plazo. -> Duda
B. Proyectos a largo plazo.
C. Proyectos de infraestructura.
D. Proyectos de migración de datos.

6. ¿Qué es un Data Warehouse?

A. Un repositorio centralizado de datos integrados de múltiples fuentes. -> Correcto
B. Un sistema de análisis en tiempo real.
C. Una base de datos distribuida.
D. Un sistema de almacenamiento temporal.

7. ¿Cuál es una ventaja clave de un Data Warehouse?

A. Menor coste de implementación.
B. Flexibilidad en la estructura de datos.
C. Proporciona una visión global y consolidada de los datos de la organización. -> correcto 
D. Almacenamiento de datos en su formato original.

8. ¿Cuál es una desventaja de los Data Warehouses?

A. Facilidad de implementación.
B. La implementación y mantenimiento pueden ser costosos. -> correcto 
C. Alta flexibilidad para cambios rápidos. 
D. Baja escalabilidad.

9. ¿Qué es un Data Lake?

A. Un sistema de almacenamiento temporal.
B. Una base de datos relacional.
C. Un almacenamiento flexible y escalable de datos crudos. -> Correcto 
D. Un sistema de procesamiento en tiempo real.

10. ¿Cuál es una característica principal de los Data Lakes?

A. Almacenan datos altamente estructurados.
B. Requieren preprocesamiento de datos antes de almacenarlos.
C. Almacenan datos en su forma original sin procesar. -> Correcto 
D. Son exclusivamente para datos transaccionales.


# Tema 7. Estrategias de Aplicación de la Ciencia de Datos y Datos Masivos

## 7.2. Inteligencia de Negocio (BI)
> Es una disciplina que se encarga de procesar datos en bruto tranformandolo en información significativa y útil, con el proposito de ayudar a la empresas en optimizar los 

El propósito principal de la BI es ayudar a las empresas a optimizar su rendimiento mediante la recopilación, integración, análisis y presentación de datos empresariales.
Esto incluye la utilización de herramientas de visualización, almacenamiento de datos y técnicas de minería de datos para descubrir patrones y tendencias ocultas

## Componentes clave:
- Fuentes de datos: Estas incluyen bases de datos transaccionales, sistemas de gestión de relaciones con los clientes (CRM), sistemas de planificación de recursos empresariales (ERP) y datos externos de mercado.
- Almacén de datos: Un repositorio centralizado que almacena datos integrados de múltiples fuentes, diseñado para facilitar el análisis y la generación de informes.
- Herramientas ETL (Extract, Transform, Load): Procesos que extraen datos de diversas fuentes, los transforman en un formato adecuado y los cargan en el almacén de datos.
- Herramientas de análisis y minería de datos: Software que permite descubrir patrones, tendencias y relaciones en los datos.
- Dashboards y visualización de datos: Interfaces que permiten a los usuarios visualizar y explorar datos a través de gráficos interactivos y visualizaciones (Turban et al., 2011).
- La capacidad de BI para integrar y analizar datos de diversas fuentes hace posible la toma de decisiones más rápida y precisa. Esta capacidad es fundamental en un entorno empresarial donde las decisiones basadas en datos pueden significar la diferencia entre el éxito y el fracaso.

## Herramientas y tecnologías
- Tableau: Una de las herramientas de visualización más populares, conocida por su capacidad para crear visualizaciones interactivas y dashboards de forma intuitiva. Tableau permite a los usuarios conectar, visualizar y compartir datos rápidamente, y sus capacidades de análisis son robustas y fáciles de usar (Chen et al., 2012).

- Power BI: Una herramienta de Microsoft que se integra perfectamente con otros productos de la suite Office y Azure. Power BI es conocido por su capacidad para transformar datos en visualizaciones coherentes y compartir insights en toda la organización.

- QlikView: Ofrece análisis de datos en memoria, conocido por su rapidez y eficiencia. QlikView permite a los usuarios explorar grandes volúmenes de datos y descubrir patrones que podrían no ser evidentes con otras herramientas (Few, 2006)

## Sistemas de gestión de bases de datos:
- SQL Server: Proporciona capacidades robustas de gestión de datos y análisis. SQL Server es ideal para manejar grandes volúmenes de datos y realizar análisis complejos.
- Oracle: Ofrece soluciones avanzadas para la gestión de grandes volúmenes de datos y análisis complejos, siendo una de las plataformas más utilizadas en BI.
- Hadoop: Ideal para almacenar y procesar grandes volúmenes de datos no estructurados. Hadoop permite a las organizaciones manejar y analizar datos masivos de manera eficiente (Dean & Ghemawat, 2008).

## Herramientas de minería de datos:
- RapidMiner: Plataforma que permite realizar minería de datos y aprendizaje automático sin necesidad de conocimientos profundos de programación. RapidMiner facilita el proceso de descubrir patrones ocultos en los datos y generar modelos predictivos.

- KNIME: Software de código abierto que facilita el análisis de datos a través de flujos de trabajo visuales. KNIME es flexible y extensible, lo que permite a los usuarios integrar diferentes tipos de análisis y técnicas de minería de datos en sus proyectos

## Implementación de BI
- Definición de objetivos y alcance
- Recopilación de requisitos
- Diseño de la arquitectura
- Desarrollo e integración
- Pruebas y validación
- Despliegue y capacitación
- Mantenimiento y mejora continua

## Casos de uso
- Sector minorista: En el sector minorista, BI se utiliza para optimizar la gestión de inventarios y mejorar
la experiencia del cliente
- Sector financiero: En el sector financiero, BI juega un papel crucial en la detección de fraudes y la
gestión de riesgos
- Sector salud: En el sector salud, BI se utiliza para mejorar la calidad del cuidado del paciente y la
eficiencia operativa
- Sector manufacturero:En la manufactura, BI ayuda a mejorar la eficiencia de la producción y la gestión de
la cadena de suministro

![alt text](/01_Cuatrimestre/02_CienciaDatosAplicada/info/image_001.png)

## 7.3. Analítica de negocio (BA)

> se refiere al uso de datos, análisis estadísticos y técnicas de modelado predictivo para tomar decisiones empresariales informadas y optimizar los resultados. La analítica de negocio se enfoca en la interpretación de datos históricos y actuales para prever tendencias futuras y comportamientos.

## Componentes clave de la analítica de negocio

- Análisis descriptivo: Se centra en la interpretación de datos históricos para entender qué ha ocurrido en el pasado. Utiliza técnicas como informes, dashboards y
visualizaciones para proporcionar una visión clara de los eventos históricos.

- Análisis predictivo: Utiliza modelos estadísticos y algoritmos de aprendizaje automático para predecir futuros eventos basados en datos históricos. Esta técnica es esencial para prever tendencias y tomar decisiones proactivas.

- Análisis prescriptivo: Va un paso más allá del análisis predictivo al no solo prever lo que puede suceder, sino también recomendar acciones específicas que pueden influir en esos futuros eventos

## Herramientas populares de analítica de negocio:

- SAS (Statistical Analysis System): Un software líder en el campo de la analítica
avanzada que ofrece una amplia gama de capacidades analíticas, desde la gestión
de datos hasta la analítica predictiva y la minería de datos. SAS es conocido por su
robustez y flexibilidad en el manejo de grandes volúmenes de datos (SAS Institute,
2012).
- R: Un lenguaje de programación y entorno de software para la estadística y el
análisis gráfico. R es ampliamente utilizado en la analítica de negocio debido a su
capacidad para manejar datos complejos y realizar análisis estadísticos avanzados
(R Core Team, 2013).
- Python: Un lenguaje de programación versátil que se ha convertido en una
herramienta esencial para la analítica de negocio debido a sus bibliotecas
específicas para análisis de datos como Pandas, NumPy, y Scikit-learn. Python es
apreciado por su facilidad de uso y su capacidad para integrarse con otros sistemas
(Van Rossum & Drake, 2009).
- IBM SPSS (Statistical Package for the Social Sciences): Ofrece herramientas
avanzadas para la analítica estadística y predictiva. SPSS es popular en la
investigación de mercado, salud y educación debido a su facilidad de uso y potentes
capacidades analíticas (IBM, 2011).


## Métodos y técnicas clave en analítica de negocio:
- Análisis estadístico: Incluye técnicas como regresión, análisis de varianza (ANOVA),
y pruebas de hipótesis. Estas técnicas son fundamentales para identificar relaciones
y patrones en los datos.

- Modelos predictivos: Utilizan algoritmos como árboles de decisión, redes neuronales
y máquinas de soporte vectorial (SVM) para prever futuros eventos basados en
datos históricos. Los modelos predictivos son esenciales para anticipar
comportamientos del cliente, detectar fraudes y gestionar riesgos (Friedman, 1997).

- Análisis de series temporales: Involucra la evaluación de datos secuenciales a lo
largo del tiempo para identificar tendencias, patrones estacionales y ciclos. Esta
técnica es particularmente útil en la previsión de ventas y la planificación de la
demanda (Box & Jenkins, 1976).

- Análisis de cohortes: Permite segmentar datos en grupos basados en características
compartidas y analizar sus comportamientos a lo largo del tiempo. Esto es útil para
entender la retención de clientes y el impacto de las campañas de marketing.


## Aplicaciones prácticas de la analítica de negocio

- Optimización de procesos
    - Evitar cuellos de botella
    - En la manufactura
    - Análisis de datos de producción 

- Mejora de la toma de decisiones
    - Sectores como el retail y las finanzas

- Personalización del marketing    
    - segmentar a los clientes y personalizar las campañas de marketing
    - Diseñar campañas más efectivas y dirigidas

- Gestión del riesgo
    - Identificación de posibles fraudes y la evaluación de la solvencia crediticia de los clientes


## 7.4. Minería de Datos (Data Mining) (DM)
> es un proceso analítico diseñado para explorar grandes cantidades de datos en busca de patrones consistentes o relaciones sistemáticas entre variables

## caracteristicas 
- Usa tecnicas avanzadas como el análisis estadístico, aprendizaje automático y bases de datos para descubrir y validar
estos patrones. 
- Transformar datos en conocimiento útil y aplicable. 

## Los componentes clave de la minería de datos
- Recopilación de datos: La primera fase de la minería de datos implica la recopilación de grandes volúmenes de datos de diversas fuentes. 
- Preparación de datos: Esta fase implica limpiar y transformar los datos para que sean adecuados para el análisis. 
- Selección de datos: No todos los datos recopilados son relevantes para cada análisis
- Modelado: Implica la aplicación de algoritmos y técnicas de minería de datos para identificar patrones en los datos
    - Descriptivo: Identificando patrones existentes
    - Predictivo: Prediciendo futuros eventos basados en datos históricos. 
- Evaluación: Una vez que se ha construido un modelo, se evalúa su efectividad y precisión. 
- Despliegue: Los modelos validados se implementan en el entorno de producción. 

## Herramientas y software
- Weka: Un conjunto de herramientas de aprendizaje automático para minería de
datos que incluye una colección de algoritmos y visualización de datos. Weka es
conocido por su facilidad de uso y su capacidad para integrar múltiples técnicas de
minería de datos en un solo entorno.

- RapidMiner: Una plataforma de análisis de datos que permite realizar procesos de
minería de datos y aprendizaje automático. RapidMiner es popular por su interfaz
intuitiva y su capacidad para manejar grandes volúmenes de datos.

- KNIME: (Konstanz Information Miner) es una plataforma de integración de datos,
procesamiento y análisis de datos de código abierto. KNIME permite la creación de
flujos de trabajo visuales y la integración de diversas técnicas de minería de datos.

- Apache Mahout: Una biblioteca de aprendizaje automático escalable para sistemas
distribuidos

## Proceso de minería de datos
- Cross-Industry Standard Process for Data Mining, es un método probado para orientar sus trabajos de minería de datos (Proceso estándar intersectorial para la minería de datos)
- Comprensión del negocio
- Comprensión de los datos
- Preparación de los datos
- Modelado
- Evaluación 
- Despliegue 

## Aplicaciones y casos de estudio

- Marketing y ventas
- Detección de fraudes
- Salud
- Manufactura


## 7.5. Aprendizaje Automático (ML)
> es una rama de la inteligencia artificial que se centra en el desarrollo de algoritmos y modelos que permiten a las máquinas aprender de los datos y mejorar su desempeño con la experiencia

## Componentes clave del aprendizaje automático
- Datos
- Algoritmos
- Modelos
- Entrenamientos
- Validación y prueba


## Algoritmos y técnicas clave en aprendizaje automático
- Regresión lineal: Un método estadístico para modelar la relación entre una variable
dependiente y una o más variables independientes. Es comúnmente utilizado para
predicción y análisis de tendencias.

- Árboles de decisión: Algoritmos que dividen repetidamente los datos en
subconjuntos basados en valores de características, formando una estructura similar
a un árbol. Son útiles para tareas de clasificación y regresión.

- Máquinas de soporte vectorial (SVM): Algoritmos de clasificación que encuentran
el hiperplano óptimo que separa las diferentes clases en el espacio de
características. Son efectivos en problemas de alta dimensionalidad

- Redes neuronales: Modelos inspirados en la estructura del cerebro humano,
compuestos por capas de nodos (neuronas) que procesan información y aprenden
patrones complejos. Las redes neuronales profundas (Deep Learning) han
revolucionado campos como la visión por computadora y el procesamiento del
lenguaje natural (Goodfellow, Bengio & Courville, 2016).

- K-means: Un algoritmo de clustering que agrupa datos en k clusters basados en
características similares. Es útil para segmentación de clientes y análisis exploratorio
de datos. 

## Herramientas y entornos

- Scikit-learn: Una biblioteca de Python que proporciona herramientas simples y
eficientes para el análisis de datos y minería de datos

- TensorFlow: Una biblioteca de código abierto desarrollada por Google para la
implementación de redes neuronales y otros algoritmos de aprendizaje profundo. Es
conocida por su flexibilidad y escalabilidad.

- Keras: Una biblioteca de alto nivel que se ejecuta sobre TensorFlow y facilita la
construcción y el entrenamiento de modelos de redes neuronales. Keras es popular
por su simplicidad y modularidad.

- PyTorch: Una biblioteca de aprendizaje automático de código abierto desarrollada
por Facebook. PyTorch es conocida por su capacidad para trabajar con gráficos
computacionales dinámicos, lo que la hace muy adecuada para la investigación en
aprendizaje profundo

## Evaluación y validación de modelos
- Validación cruzada: Un método que divide los datos en múltiples subconjuntos y
entrena el modelo en algunos de estos subconjuntos mientras lo valida en los
restantes. Este proceso se repite varias veces para asegurar una evaluación
robusta.

- Métricas de rendimiento: Dependiendo del tipo de problema (clasificación o
regresión), se utilizan diferentes métricas como precisión

- Conjunto de prueba: Después del entrenamiento, el modelo se evalúa utilizando un
conjunto de datos que no se utilizó durante el entrenamiento ni la validación

- Regularización: se utilizan para prevenir el sobreajuste ajustando los parámetros del modelo.

##  Implementación en la industria 
- Sector financiero:
- Salud:
- Marketing
- Automoción


## 7.6. Inteligencia Artificial (IA)
> se refiere a la simulación de procesos de inteligencia humana por parte de máquinas, especialmente sistemas informáticos. Estos procesos incluyen el aprendizaje (la adquisición de información y reglas para el uso de la información), el razonamiento (usar reglas para llegar a conclusiones aproximadas o definitivas) y la autocorrección

## Componentes clave de la inteligencia artificia
- Algoritmos: Los algoritmos son las secuencias de instrucciones que las máquinas
siguen para resolver problemas y tomar decisiones. Estos pueden variar desde
algoritmos simples de búsqueda y clasificación hasta complejas redes neuronales.

- Datos: La IA requiere grandes cantidades de datos para entrenar y mejorar sus
modelos. Los datos pueden provenir de diversas fuentes como sensores, bases de
datos, y plataformas en línea.

- Modelos: Un modelo de IA es una representación matemática de un sistema o
proceso que se ha entrenado para realizar tareas específicas.

- Entrenamiento y aprendizaje: El proceso mediante el cual un modelo de IA mejora
su rendimiento ajustando sus parámetros con base en los datos de entrenamiento.

- Interfaz de usuario: La manera en que los usuarios interactúan con sistemas de IA,
que puede incluir interfaces de voz, visuales, y textuales.


## Algoritmos y técnicas de IA

- Aprendizaje supervisado: Este enfoque utiliza datos etiquetados para entrenar
modelos que pueden hacer predicciones o clasificaciones. Ejemplos incluyen la
regresión lineal, máquinas de soporte vectorial (SVM) y redes neuronales.

- Aprendizaje no supervisado: Se utiliza para encontrar patrones o agrupaciones en
datos no etiquetados. Ejemplos incluyen algoritmos de clustering como k-means y
análisis de componentes principales (PCA).

- Aprendizaje por refuerzo: Un enfoque en el que un agente aprende a comportarse
en un entorno realizando acciones y recibiendo recompensas o castigos. Este
método es fundamental en el desarrollo de sistemas de IA para juegos y robótica
(Sutton & Barto, 2018).

- Redes neuronales profundas: Estas son un tipo de red neuronal con múltiples
capas entre la entrada y la salida, que son capaces de aprender representaciones
complejas de datos. El aprendizaje profundo ha revolucionado áreas como la visión
por computadora y el procesamiento del lenguaje natural (Goodfellow, Bengio &
Courville, 2016).

## Herramientas y entornos

- TensorFlow: Una biblioteca de código abierto desarrollada por Google que se utiliza
ampliamente para el desarrollo de modelos de aprendizaje profundo.

-PyTorch: Desarrollada por Facebook, esta biblioteca es popular en la investigación
y desarrollo de IA debido a su flexibilidad y facilidad de uso.

- Keras: Una biblioteca de alto nivel que se ejecuta sobre TensorFlow, que facilita la
creación y el entrenamiento de redes neuronales.

- OpenAI Gym: Un toolkit para el desarrollo y la comparación de algoritmos de
aprendizaje por refuerzo.

## Aplicaciones actuales de la IA

- Salud: La IA se utiliza para el análisis de imágenes médicas, predicción de
enfermedades, y personalización de tratamientos. Modelos de IA pueden analizar
imágenes de resonancias magnéticas para detectar anomalías que indican
enfermedades.

- Finanzas: Utilizada para la detección de fraudes, evaluación de riesgos y análisis de
mercado. La IA puede analizar transacciones en tiempo real para detectar patrones
sospechosos.

- Automoción: Los vehículos autónomos utilizan IA para procesar datos de sensores
y tomar decisiones en tiempo real, incluyendo el reconocimiento de señales de
tráfico y la detección de obstáculos. 

- Atención al cliente: Chatbots y asistentes virtuales impulsados por IA están
mejorando la eficiencia y la experiencia del cliente en diversas industrias.

- Marketing: La IA se utiliza para la segmentación de clientes, personalización de
campañas publicitarias y análisis de sentimiento en redes sociales (Kaplan &
Haenlein, 2019).

## Cuestionario 

1. ¿Cuál es uno de los beneficios principales de los Data Marts en la inteligencia de
negocio?
A. Almacenan grandes volúmenes de datos no estructurados.
B. Facilitan la segmentación y análisis de datos específicos de diferentes
áreas de negocio. -> correcto 
C. Automatizan procesos financieros.
D. Mejoran la velocidad de las transacciones.

2. ¿Qué herramienta de Oracle BI permite a los usuarios crear visualizaciones
interactivas?
A. Oracle BI Publisher.
B. Oracle BI Interactive Dashboards. -> Correcto 
C. Oracle BI Answers.
D. Oracle Real-Time Decisions.

3. ¿Qué técnica analítica es esencial para anticipar cambios en el mercado según
IBM?
A. Análisis descriptivo.
B. Análisis predictivo.  -> correcto 
C. Análisis de componentes principales.
D. Análisis de cluster.

4. ¿Cuál es una limitación importante de la analítica de negocio mencionada por
SAS?
A. Mejora la eficiencia operativa.
B. Facilita la toma de decisiones informadas.
C. Requiere competencias especializadas en análisis de datos. -> Correcto 
D. Simplifica la implementación de soluciones.

5. ¿Qué describe mejor el proceso de preparación de datos en la minería de datos?
A. Almacenamiento de datos en un data warehouse
B. Limpieza y transformación de datos para análisis. -> Correcto 
C. Visualización de datos en dashboards interactivos.
D. Automatización de procesos transaccionales.

6. ¿Cuál es una aplicación común de la minería de datos en el sector financiero?
A. Optimización de inventarios.
B. Detección de fraudes. -> Correcto 
C. Personalización de campañas de marketing.
D. Análisis de la cadena de suministro.

7. ¿Qué herramienta de aprendizaje automático es conocida por su capacidad para
trabajar con gráficos computacionales dinámicos?
A. Scikit-learn.
B. TensorFlow.
C. PyTorch. -> correcto 
D. Keras.

8. ¿Qué método de evaluación asegura que un modelo de aprendizaje automático
no se ajuste demasiado a los datos de entrenamiento?
A. Regularización. 
B. Análisis de componentes principales.
C. Validación cruzada. -> DUDA
D. Clustering


9. ¿Cuál es una aplicación de la inteligencia artificial en la atención al cliente?
A. Optimización de procesos de manufactura.
B. Detección de enfermedades.
C. Chatbots y asistentes virtuales. -> Correcto 
D. Análisis de imágenes médicas.

10. ¿Qué avance en inteligencia artificial se espera que haga los modelos más
transparentes y comprensibles para los humanos?
A. IA general.
B. IA explicable (XAI). -> correcto 
C. Aprendizaje no supervisado.
D. Clustering.


## Videoclase 1. Inteligencia y Analítica de Negocio
## Notas: 
- ¿Cuál es la importancia de la inteligencia de negocio (BI) en la toma de decisiones empresariales?
    - Ofrece datos validados y actuales, reduciendo errores y mejorando la precisión y objetividad en las decisiones.
- ¿Cuál es un componente clave de BI que facilita la generación de informes y análisis?
    - Herramientas ETL (Extract, Transform, Load) que preparan los datos para el análisis.
- ¿Qué ventaja ofrece Power BI como herramienta de inteligencia de negocio?
    - Ofrece amplia conectividad con diversas fuentes de datos y facilita la creación de informes personalizados.
- ¿Cómo mejora el Machine Learning (ML) la eficiencia de BI?
    - Automatiza el análisis de datos y revela patrones complejos.
- ¿Qué caso de uso destaca la aplicación de BI en el sector minorista?
    - Optimizar inventarios y mejorar la experiencia del cliente mediante el análisis de patrones de compra.


## Videoclase 2. Aplicaciones de la Minería de Datos
## Notas: 
- Una de las diferencias resumidas entre la minería de datos y el aprendizaje automático (ML) es:
    - La minería de datos encuentra patrones, mientras el aprendizaje automático aprende de datos para predecir. -> La minería de datos se enfoca en descubrir patrones y relaciones en grandes conjuntos de datos, mientras que el aprendizaje automático utiliza esos datos para que las máquinas aprendan y hagan predicciones automáticamente
- ¿Cuál es una de las aplicaciones de la minería de datos en el sector financiero?
    - Detección de fraudes mediante la identificación de patrones inusuales.
- ¿Cómo beneficia la minería de datos al marketing?
    - Permite la segmentación de clientes y la personalización de campañas.
-  Un ejemplo de aplicación de la minería de datos en salud es: 
    - Analizar datos clínicos para el diagnóstico temprano de enfermedades utilizando técnicas de clasificación.
-  ¿Qué futuro se espera para la minería de datos? 
    - Integrarse con Big Data para analizar grandes volúmenes de datos en tiempo real y expandirse a nuevos campos.

## Videoclase 3. Aplicaciones del Aprendizaje Automático
## Notas: 
- ¿Qué componente es fundamental para el aprendizaje automático (ML)?
    - Una cantidad considerable de datos es esencial para entrenar algoritmos de ML de forma efectiva.
- ¿Cuál es una aplicación de ML en el sector automotriz moderno?
    - Procesar datos y tomar decisiones en tiempo real al utilizar vehículos autónomos.
- ¿Qué uso tiene ML en el marketing digital y tradicional?
    - Utilizar sistemas de recomendación y análisis de sentimientos para personalizar la publicidad y aumentar la eficiencia de las campañas con base en datos.
- ¿Cómo se utiliza ML para la detección de fraudes en las transacciones financieras?  
    - Identificando patrones anómalos en transacciones para detectar actividades fraudulentas en tiempo real y prevenir pérdidas.
- ¿Cuál es una tendencia futura del aprendizaje automático (ML) en diferentes industrias?
    - Avances en IA explicable, automatización y aplicaciones interdisciplinarias en nuevos sectores de la economía.


# Tema 8. Aplicaciones en Minería de Datos
## 8.2. Conceptos de Minería de datos 

>[!NOTE]
> El avance de la tecnología ha permitido el desarrollo de sistemas que funcionan con un consumo mínimo, por lo que son capaces de capturar enormes cantidades de datos. 
> Dentro de los tipos de datos que más se generan se encuentran: los archivos de la web, las interacciones financieras, la interacciones entre usuarios y el Internet de las cosas.

**La minería de datos y el descubrimiento de conocimiento en bases de datos
(KDD, por sus siglas en inglés Knowledge Discovery in Databases)**
- DM es solo una parte del proceso KDD, si bien es la porción central y más importante. 
- Es su desarrollo a través de etapas.

**Las etapas en seis pasos**
- Especificación del problema
- Comprensión del problema
- Pre procesamiento de datos
- Mineria de Datos
- Evaluación 
- Exploración de resultados 



## 8.3. Objetivos de la Minería de Datos

La minería de datos surge de la intersección de varias áreas, como la estadística, la
inteligencia artificial, el aprendizaje automático y las bases de datos. 

**Su objetivo principal es extraer patrones, conocimientos y tendencias útiles a partir de grandes
conjuntos de datos.** 

Estos descubrimientos pueden ser utilizados para mejorar la toma de decisiones, optimizar procesos, identificar oportunidades y detectar problemas antes de que se conviertan en críticos al negocio. 


# Métodos de Minería de Datos


## Predicción
* **Métodos estadísticos:**
    * **Modelos de regresión:** Utilizados para predecir un valor numérico continuo a partir de una o más variables independientes.
    * **Redes neuronales:** Modelos inspirados en el cerebro humano, capaces de aprender patrones complejos en grandes conjuntos de datos.
    * **Bayesiano:** Basados en el teorema de Bayes, utilizan probabilidades para hacer inferencias a partir de datos.
    * **Aprendizaje basado en instancias:** Almacena ejemplos de entrenamiento y clasifica nuevos datos comparándolos con los ejemplos más similares.
    * **Máquinas de soporte vectorial:** Buscan el hiperplano que mejor separa los datos en diferentes clases.
    * **Árbol de decisión:** Crea una estructura jerárquica de decisiones para clasificar o predecir.
* **Métodos simbólicos:**
    * **Aprendizaje de reglas:** Extraen reglas "si-entonces" a partir de los datos para realizar predicciones.
    * **Árbol de decisión:** Crea una estructura jerárquica de decisiones para clasificar o predecir.

## Descripción
* **Agrupamiento:** Divide los datos en grupos homogéneos basados en sus características.
* **Reglas de Asociación:** Descubren relaciones entre variables en grandes bases de datos.

## 8.4. Procesos de descubrimiento de conocimiento

## Aprendizaje Supervisado
**Definición:** El aprendizaje supervisado es un tipo de aprendizaje automático en el que se proporciona al modelo un conjunto de datos de entrenamiento etiquetados. Esto significa que cada ejemplo en el conjunto de datos tiene una etiqueta correspondiente a la salida correcta. El modelo aprende a mapear las entradas a las salidas correctas, y luego puede utilizarse para hacer predicciones sobre nuevos datos no vistos.

**Características:**
- Datos etiquetados: Los datos de entrada están asociados con una respuesta correcta o etiqueta.
- Feedback directo: El modelo recibe retroalimentación directa sobre la precisión de sus predicciones durante el entrenamiento.
- Predicción de resultados futuros: Se utiliza para predecir resultados futuros o clasificar nuevos datos.

## Aprendizaje No Supervisado
**Definición:** El aprendizaje no supervisado es un tipo de aprendizaje automático en el que se proporciona al modelo un conjunto de datos de entrenamiento sin etiquetas. El objetivo es descubrir patrones ocultos o estructuras en los datos sin ninguna guía previa.

**Características:**
- Sin etiquetas: Los datos de entrada no tienen etiquetas asociadas.
- Sin feedback: El modelo no recibe retroalimentación directa sobre la calidad de sus resultados.
- Encontrar estructuras de datos: Se utiliza para encontrar agrupaciones, asociaciones o anomalías en los datos.

## Aprendizaje por Refuerzo
**Definición:** El aprendizaje por refuerzo es un tipo de aprendizaje automático en el que un agente aprende a tomar acciones en un entorno para maximizar una recompensa 1 a largo plazo. El agente interactúa con el entorno, recibe retroalimentación en forma de recompensas o penalizaciones, y aprende a tomar decisiones que maximicen la recompensa acumulada. 

**Características:**
- Proceso de decisión: El agente debe tomar decisiones en cada paso de tiempo.
- Sistema de recompensa: El agente recibe recompensas o penalizaciones basadas en las acciones que toma.
- Aprender series de acciones: El objetivo es aprender una secuencia de acciones óptima para lograr un objetivo a largo plazo.

| Tipo de Aprendizaje | Características Clave | Ejemplo de Uso |
|---|---|---|
| Supervisado | Datos etiquetados, feedback directo, predicción | Clasificación de correos electrónicos como spam o no spam, predicción de precios de viviendas. |
| No Supervisado | Sin etiquetas, encontrar estructuras | Segmentación de clientes, detección de anomalías en datos de transacciones. |
| Por Refuerzo | Proceso de decisión, sistema de recompensa | Juegos como el ajedrez o el Go, control de robots. |

## 8.5. Ejemplos de aplicaciones de Minería de Datos

- Detección de fraudes.
- Previsión financiera y empresarial.
- Diagnóstico médico.


## CUESTIONARIO 

1. ¿Cuáles son los métodos usados en DM para resolución de problemas?
A. Métodos de clasificación, estadísticos, simbólicos y de grafos.
B. Métodos de predicción, estadísticos y simbólicos.
C. Aprendizaje supervisado y no supervisado. -> Correcto 
D. Aprendizaje de máquina y aprendizaje profundo.

2. Son aplicaciones de la minería de datos:
A. Detección de fraudes.
B. Previsión financiera y empresarial.
C. Diagnóstico médico.
D. Todas las respuestas anteriores son correctas. -> Correcto 

3. No es un método estadístico dentro de la minería de datos:
A. Modelos de regresión.
B. Redes neuronales.
C. Árbol de decisión.-> Correcto 
D. Máquina de soporte vectorial.

4. Es un algoritmo que utiliza el aprendizaje basado en instancias:
A. K vecinos más cercanos o KNN. -> correcto 
B. Árboles de decisión.
C. Máquina de vectores de soporte.
D. Random Forest.

> Aprendizaje basado en instancias: KNN es un algoritmo que almacena todos los datos de entrenamiento y clasifica nuevos puntos de datos basándose en la similitud con sus k vecinos más cercanos. Es decir, no construye un modelo general a partir de los datos, sino que compara directamente el nuevo dato con los ejemplos existentes.

5. Desde la obtención de los datos hasta generar conocimiento se deben seguir las
siguientes etapas:
A. Todas son válidas. -> Correcto 
B. Especificación y comprensión del problema.
C. Preprocesamiento de los datos.
D. Minería de datos, evaluación y exploración de resultados.

6. Es una afirmación falsa sobre las máquinas de soporte vectorial:
A. Pertenece a la familia de clasificadores lineales.
B. Transforma el conjunto de datos de una dimensión n a un espacio de
dimensión superior.
C. Es considerado una extensión del perceptrón.
D. Su objetivo es minimizar los errores. -> Correcto 

- Pertenece a la familia de clasificadores lineales: Esta afirmación es parcialmente correcta. Si bien las SVM pueden realizar clasificaciones lineales, su verdadera potencia radica en su capacidad de manejar problemas no lineales mediante el uso de funciones kernel. Estas funciones permiten mapear los datos a un espacio de mayor dimensión donde la separación entre clases pueda ser lineal.
- Transforma el conjunto de datos de una dimensión n a un espacio de dimensión superior: Esta afirmación es correcta. Como se mencionó anteriormente, las SVM utilizan funciones kernel para transformar los datos a un espacio de mayor dimensión. Esto permite encontrar hiperplanos de separación más complejos y abordar problemas no lineales.
- Es considerado una extensión del perceptrón: Esta afirmación es correcta. Las SVM pueden verse como una extensión del perceptrón, ya que buscan encontrar un hiperplano que separe óptimamente las clases. Sin embargo, las SVM optimizan el margen entre las clases, lo que las hace más robustas y generalizables.


7. Es un método de agrupamiento:
A. Agrupamiento jerárquico. -> Correcto 
B. Agrupamiento denso.
C. Agrupamiento central.
D. Agrupamiento no denso.

- Agrupamiento jerárquico: Este método crea una jerarquía de clusters, donde cada cluster se divide en subclusters más pequeños. Es como crear un árbol genealógico de los datos

8. Es una ventaja de las reglas de asociación:
A. Posibilidad de asociación.
B. Obtención de muchas conclusiones al poseer muchas reglas. -> 
C. Fiabilidad alta.
D. Todas las respuestas anteriores son correctas.

- Abundancia de información: Las reglas de asociación pueden generar una gran cantidad de patrones y relaciones entre los datos. Esto significa que podemos obtener muchas conclusiones diferentes a partir de un mismo conjunto de datos.
- Descubrimiento de conocimientos: Al generar tantas reglas, se facilita la identificación de relaciones inesperadas o poco evidentes, lo que puede llevar a nuevos descubrimientos y conocimientos.

9. Es un tipo de aprendizaje basado en recompensas, aprende de una serie de
acciones y lleva a cabo un proceso de decisión.
A. Aprendizaje supervisado.
B. Aprendizaje no supervisado.
C. Aprendizaje reforzado. -> Correcto 
D. Todas las respuestas anteriores son correctas.

10. Es característico de un problema de regresión:
A. Es típico en aplicaciones que incluyen análisis de precios, tendencias y
previsión. -> Correcto 
B. Discriminación de instancias.
C. Discriminación entre una o varias clases categóricas.
D. Ninguna de las respuestas anteriores es correcta.

- La regresión es una técnica estadística que se utiliza para modelar la relación entre una variable dependiente (la que queremos predecir) y una o más variables independientes. Es ideal para situaciones donde queremos predecir un valor numérico, como el precio de una vivienda, las ventas de un producto en función de diversos factores, o la temperatura máxima de un día.

## Videoclase 1. Conceptos claves de la Minería de Datos
## Notas: 
- ¿Qué es la minería de datos y cuál es su propósito principal?
    - Es el proceso de descubrir patrones, asociaciones, clasificaciones y agrupaciones en grandes conjuntos de datos para extraer información valiosa y mejorar la toma de decisiones
-  ¿Cuál es una aplicación típica del algoritmo Apriori en minería de datos?  
    - Identificar asociaciones entre productos en grandes bases de datos para diseñar promociones cruzadas en supermercados.
- ¿Qué técnica de minería de datos, también usada en ML, se podría utilizar para la segmentación de clientes según su comportamiento de compra?
    - Clustering o agrupamiento, que divide los clientes en grupos basados en similitudes en su comportamiento de compra.
- ¿Cómo se utilizan las redes neuronales en la minería de datos?
    - Para reconocer patrones complejos en los datos, como en el reconocimiento de imágenes y predicción de series temporales.
- ¿Qué impacto podría tener la minería de datos en la mejora de procesos industriales?
    - Permite el mantenimiento predictivo usando datos de sensores para anticipar fallos de maquinaria y reducir costos de reparación.

## Videoclase 2. Proceso de descubrimiento de conocimiento
## Notas: 
- ¿Qué etapas principales clave conforman un proceso KDD (Knowledge Discovery in Databases)?
    - Selección de datos, preprocesamiento, transformación, minería de datos, interpretación y evaluación de resultados.
- ¿Cuál es el objetivo principal del preprocesamiento de datos en el proceso KDD?
    - Mejorar la calidad de los datos corrigiendo errores, gestionando valores faltantes y preparándolos adecuadamente e para el análisis.
- ¿Qué se busca lograr en la etapa de transformación de datos del proceso KDD?
    - Aplicar técnicas de visualización para su presentación.
-  ¿Cómo se garantiza la calidad y utilidad de los modelos generados en el proceso KDD?  
    - Validando los modelos con técnicas como la validación cruzada y evaluando métricas de rendimiento.
-  ¿Qué desafío enfrenta el proceso KDD en su aplicación práctica?  
    - Garantizar la calidad de los datos, manejar grandes volúmenes de datos, y asegurar la interpretabilidad de los modelos.

# Tema 9. Aplicación en Aprendizaje Automático


## 9.3. Métodos supervisados y no supervisados

## Aprendizaje Supervizado 
> Los tipos más famosos de este aprendizaje son aquellos que permiten automatizar la toma de decisiones a partir de generalizaciones desde ejemplos aprendidos. En este grupo entra lo que se conoce como aprendizaje supervisado

## Aprendizaje No Supervizado 


## Mapa Conceptual del Aprendizaje Automático

La siguiente imagen representa una clasificación detallada de las diversas técnicas y algoritmos utilizados en el campo del aprendizaje automático.

### Niveles de Abstracción
* **Aprendizaje Automático:** El término general que engloba todas las técnicas que permiten a las computadoras aprender a partir de datos sin ser programadas explícitamente.
* **Aprendizaje Supervisado:** El algoritmo aprende a partir de datos etiquetados.
    * **Clasificación:** Predice a qué categoría pertenece un nuevo dato.
    * **Regresión:** Predice un valor numérico continuo.
* **Aprendizaje No Supervisado:** El algoritmo encuentra patrones en datos no etiquetados.
    * **Clustering:** Agrupa datos similares.
    * **Reducción de Dimensionalidad:** Simplifica datos de alta dimensión.
* **Aprendizaje Semi-Supervisado:** Combina datos etiquetados y no etiquetados.
* **Aprendizaje por Refuerzo:** El agente aprende a tomar decisiones en un entorno para maximizar una recompensa.

### Técnicas Específicas
* **Perceptrones Multicapa (MLP):** Una arquitectura de red neuronal artificial.
* **Redes de Funciones de Base Radial (RBFN):** Otro tipo de red neuronal.
* **Redes Neuronales Recurrentes (RNN):** Utilizadas para procesar secuencias de datos.
* **Redes Neuronales Convolucionales (CNN):** Especializadas en el procesamiento de imágenes.
* **Redes Generativas Adversarias (GAN):** Generan nuevos datos a partir de una distribución de probabilidad.
* **Autoencoders:** Utilizados para la reducción de dimensionalidad y el aprendizaje de representaciones latentes.
* **Redes de Creencias Profundas (DBM):** Modelos gráficos probabilísticos para el aprendizaje profundo.
* **Algoritmos Genéticos:** Inspiran en la evolución biológica para encontrar soluciones óptimas.
* **Aprendizaje Ensamble:** Combina múltiples modelos para mejorar el rendimiento.
    * **Bagging:** Crea múltiples modelos a partir de subconjuntos de datos,A partir del conjunto original de observaciones de entrenamiento se
generan m conjunto nuevos de datos con las mismas propiedades
    * **Boosting:** Combina modelos secuencialmente, dando más peso a los ejemplos mal clasificados.  Consiste en combinar varios clasificadores sencillos para obtener uno
más robusto
    * **Stacking:** Combina los resultados de múltiples modelos utilizando un meta-modelo.

### Aprendizaje Profundo
* **Redes Neuronales y Aprendizaje Profundo:** El uso de redes neuronales con muchas capas para aprender representaciones complejas de datos.
* **Aprendizaje por Refuerzo Profundo:** La combinación de aprendizaje por refuerzo y aprendizaje profundo para tareas más complejas.

### Conceptos Clave
* **Autoaprendizaje:** La capacidad de un sistema de aprender sin intervención humana.
* **Autoensamblado:** La capacidad de un sistema de organizarse a sí mismo.
* **Búsqueda de Patrones:** Identificar regularidades en los datos.

**De forma habitual, un sistema de aprendizaje constará de las siguientes etapas:**
- Selección del objetivo de aprendizaje.
- Selección del conjunto de datos de entrenamiento.
- Selección de una función objetivo y su representación.
- Selección del algoritmo de aprendizaje que aproximará la función objetivo.
- Evaluación y validación de los resultados.



## 9.4. Clasificación, regresión y agrupamiento

## clasificación 
- La clasificación también es una de las herramientas importantes en la toma de decisiones. 
- La clasificación es un problema de predicción de categorías.
- En un problema de predicción el objetivo es estimar una cantidad 
- Un clasificado es básicamente una división del espacio de las variables independientes. 
- A cada división se le asigna una clase o categoría. 
- Es importante indicar que la clasificación binaria se refiere a casos donde solo hay dos categorías o
clases. 
- Los métodos de clasificación se pueden generalizar a un mayor número de categorías

**También podemos encontrar modelos de clasificación basados en:**
- Redes neuronales. Se entrena una red neuronal para que su predicción sea la categoría o clase que corresponde a cada observación.
- Boosting.(Impulsando - Buusten) Consiste en combinar varios clasificadores sencillos para obtener uno más robusto. El aprendizaje es secuencial, cada nuevo clasificador sencillo se
especializa en aprender las observaciones mal clasificadas por los anteriores.
- Bagging.(Harpillera - Bagginn) A partir del conjunto original de observaciones de entrenamiento se generan (m) conjunto nuevos de datos con las mismas propiedades. 
    - Con cado uno de estos conjuntos se entrena un clasificador y finalmente se combinan los (m) clasificadores para obtener uno robusto.


**Medidas de calidad de un modelo de clasificación**
> Es importante tener unas metricas que midan de forma objetiva la precisión de los modelos clasificación para eso se usan ciertas herramientas: 
> Son métricas que nos permiten evaluar qué tan bien está funcionando nuestro modelo al predecir a qué clase pertenece un nuevo dato

**Herramientas para medir la calidad** 
- Matriz de confusión: consiste en medir un conjunto de observaciones etiquetas a su categoria. 
    - se construye creando columnas que representan categorias reales de las observaciones 
    - Se contruye filas que represenatan las categorías predichas por el modelo para cada observación. 
    - Se obtiene una linea diagonal con el número de observaciones con predicción acertada. 
**ejmeplo** 
![alt text](/01_Cuatrimestre/02_CienciaDatosAplicada/info/image_021.png)

- Curva ROC y AUC: La curva ROC muestra la relación entre la tasa de verdaderos positivos y la tasa de falsos positivos para diferentes umbrales de clasificación.
    - El área bajo la curva ROC (AUC) es un único valor que resume el rendimiento del modelo.

- Kappa de Cohen: Mide el acuerdo entre las predicciones del modelo y los valores reales, teniendo en cuenta la probabilidad de acuerdo aleatorio.
- Log loss: Mide la discrepancia entre las probabilidades predichas por el modelo y los valores reales.


## Regresión
> La regresión es un método de aprendizaje automático utilizado para modelar la relación entre una variable dependiente (objetivo) y una o más variables independientes (predictoras). La finalidad principal de la regresión es predecir un valor continúo basándose en las características observadas

**Técnicas de Regresión**
- Regresión Lineal Simple: esta técnica modela la relación entre dos variables
mediante una línea recta.
- Regresión Lineal Múltiple: extiende la regresión lineal simple para incluir múltiples
variables independientes
- Regresión Polinómica: modela la relación entre la variable dependiente y las
variables independientes como un polinomio de n-ésimo grado. Es útil cuando la
relación no es lineal.
- Regresión Logística: aunque su nombre sugiere lo contrario, se utiliza para
problemas de clasificación binaria. Modela la probabilidad de un resultado binario
utilizando una función logística
- Regresión Ridge y Lasso: son técnicas de regularización que modifican la función
de costo para incluir términos que penalizan grandes coeficientes, ayudando a
prevenir el sobreajuste. Ridge utiliza la penalización L2 mientras que Lasso utiliza la
penalización L1


## Agrupamiento o clustering
> El clustering es uno de los métodos de aprendizaje no supervisado más importantes y, como cualquier otro método de aprendizaje no supervisado, busca caracterizar conceptos desconocidos a partir de instancias de estos. En este tipo de problemas de aprendizaje no supervisado, la clase es desconocida y, precisamente, el descubrimiento de esta clase es el objetivo a través de la agrupación de instancias con base en un esquema de similitud.

> permite agrupar objetos en clústeres o agrupamientos, cuyos miembros son similares entre sí en cierto modo.

**A continuación, se enumeran diferentes tipos de algoritmos de clustering en función del tipo de agrupamientos que producen:**

    - Agrupamientos exclusivos. Pueden generarse por métodos que particionan los datos creando un número k determinado de clústeres. Cada uno de los clústeres tiene, al menos, un objeto y los objetos se agrupan de modo exclusivo, pudiendo pertenecer únicamente a un clúster. 
    - Agrupamientos jerárquicos. Existen algoritmos jerárquicos que dan lugar a una estructura jerárquica de clústeres. En el primer nivel de la jerarquía se tiene un único clúster que, en una primera iteración, se divide en clústeres. Cada uno de estos se divide a su vez y se van generando nuevos clústeres en siguientes iteraciones. Esto es lo que se llama una aproximación divisoria.
    - Agrupamientos solapados. Los objetos se agrupan a través de conjuntos difusos y cada objeto puede pertenecer a uno o más clústeres con diferentes grados de pertenencia.
    - Agrupamientos probabilistas. Los clústeres se generan mediante un método probabilístico como es el algoritmo EM (Expectation- Maximization).
    - Las medidas de distancia:  son utilizadas en un importante número de algoritmos de clustering, puesto que muchos algoritmos se basan en medidas de distancia entre objetos para, en función de su «cercanía», incluirlos en el mismo clúster o en clústeres separados.

**Algunos ejemplos de aplicaciones prácticas del clustering en entornos de Industria 4.0 son:**
    - Encontrar objetos representativos de grupos homogéneos. Por ejemplo, en un sistema de ahorro energético, identificar usuarios con características similares para determinar diferencias de comportamientos y consumos
    - En un sistema de gestión de clientes, las técnicas de clustering pueden agrupar los diversos clientes según características comunes
    - Encontrar grupos y describirlos en función de sus propiedades. Por ejemplo, en biología, clasificación de plantas o, en medicina, clasificación de enfermedades raras
    - Detección de casos anómalos. Por ejemplo, en un sistema de detección de errores n líneas de comunicación al detectar patrones de ruido que no correspondan con ningún caso descrito

## 9.5. Aprendizaje Profundo
> El deep learning es un área específica del aprendizaje automático que se basa en la utilización de redes neuronales con un alto número de nodos y capas. Este nuevo concepto dentro del aprendizaje automático lidia con procesos complejos que trabajan con volúmenes elevados de datos, así como con la interconexión necesaria entre los diferentes sistemas que forman parte de la solución completa

>  como una clase de algoritmos de aprendizaje automático que utiliza múltiples capas para extraer progresivamente características de nivel superior de la entrada bruta.

**Dentro de los sistemas deep learning pueden destacarse por su relevancia las siguientes**
    - Redes prealimentadas: 
    - Redes neuronales recurrentes  (RNN, Recurrent Neural Networks): 
    - Redes recurrentes profundas (Deep Recurrent Networks):
    - Autoencoders (AE): 
    - Redes neuronales convolucionales (CNN, Convolutional Neural Networks); 
    - Redes Generativas Antagónicas (GAN, Generative Adversarial Networks): 

**Nota**
 - profundas pueden utilizarse como sustitutos de los métodos supervisados, no supervisados y semisupervisados. 

## 9.6. Ejemplos de aplicaciones de aprendizaje automático

 **Algunos ejemplos de aplicaciones prácticas**
    - Medios Sociales y Personalización 
        - Funcionalidades en Redes Sociales: Plataformas como Facebook utilizan algoritmos de aprendizaje automático para analizar las actividades de los usuarios y ofrecer sugerencias personalizadas de amigos y páginas. Estas funcionalidades mejoran la experiencia del usuario al adaptar el contenido a sus preferencias individuales.
    - Recomendación de Productos 
        - E-commerce: Sitios web como Amazon emplean algoritmos para rastrear el comportamiento de compra, historial de búsquedas y patrones de carrito de compras para recomendar productos que probablemente interesen a los usuarios. Esto no solo incrementa las ventas, sino que también mejora la satisfacción del cliente al proporcionar una experiencia de compra más personalizada. 
    - Reconocimiento de Imágenes
        - Diagnóstico Médico: Algoritmos de redes neuronales convolucionales (CNN) se utilizan en la detección de cáncer de piel, proporcionando tasas de precisión muy altas en comparación con métodos manuales.

    - Análisis de Sentimientos
        - Marketing y Atención al Cliente: Las herramientas de análisis de sentimientos  ayudan a las empresas a entender las opiniones y emociones de los clientes en tiempo real, a partir de reseñas, correos electrónicos y publicaciones en redes sociales. Esto es crucial para mejorar la atención al cliente y adaptar las estrategias de marketing.

    - Optimización del Viaje del Cliente
        - Marketing Digital: Las técnicas de aprendizaje automático optimizan el viaje del cliente al analizar y predecir los puntos de interés en tiempo real, mejorando las estrategias de adquisición y retención de clientes mediante recomendaciones personalizadas.
    - Automatización de Control de Acceso
        - Seguridad en el Lugar de Trabajo: Las organizaciones implementan algoritmos para determinar los niveles de acceso de los empleados basados en sus perfiles de trabajo, mejorando la seguridad y eficiencia en la gestión de recursos humanos.

    - Preservación de Vida Marina
        - Conservación Ambiental: Algoritmos de aprendizaje automático se utilizan para modelar el comportamiento de especies marinas en peligro, ayudando a los científicos a regular y monitorear sus poblaciones de manera más efectiva.

    - Predicción de Fallos Cardíacos
         - Medicina Preventiva: Algoritmos analizan las notas de los médicos y patrones en el historial cardiovascular de los pacientes para identificar riesgos de fallos cardíacos, facilitando diagnósticos más rápidos y precisos. 
    
    - Traducción de Idiomas
        Comunicación Global: Herramientas de traducción de idiomas basadas en aprendizaje automático permiten traducciones precisas y contextuales entre    múltiples idiomas, eliminando barreras lingüísticas y facilitando la comunicación   global.

    - Detección de Bots Maliciosos
        - Ciberseguridad: Los sistemas de detección de bots en plataformas como Twitter utilizan técnicas de aprendizaje supervisado para identificar y clasificar bots buenos y malos, reduciendo la propagación de información falsa y amenazas cibernéticas.
                

## Cuestionario 

1. El aprendizaje automático consiste en extraer conocimiento a partir de los datos.
    A. Verdadero. -> correcta
    B. Falso.

2. Los tres grandes grupos que se pueden definir dentro del aprendizaje automático
son:
A. Aprendizaje por refuerzo.
B. Aprendizaje supervisado y no supervisado.
C. Aprendizaje semisupervisado.
D. Las tres anteriores son correctas. -> correcta

3. Dentro del aprendizaje supervisado existen diferentes técnicas, entre ellas:
A. Clasificación y regresión. -> correcta
B. Agrupamiento y asociación.
C. Aprendizaje por refuerzo.
D. Ninguna de las anteriores.

4. El aprendizaje por refuerzo se basa en el entrenamiento de modelos de
aprendizaje automático para tomar decisiones de forma secuencial.
A. Verdadero. -> correcto 
B. Falso.

5. ¿Cuáles de los siguientes puntos pertenecen al aprendizaje por refuerzo?
A. Agente, acción, entorno.-> correcta 
B. Recompensa, intercambio, acción. 
C. Estado, lugar, tiempo.
D. Ninguna de las anteriores

6. ¿Cuáles de los siguientes puntos pertenecen a las fases del proceso de
extracción de conocimiento visto en clase?
A. Datos, datos objetivo.
B. Datos transformados, patrones, conocimiento.
C. Datos preprocesados.
D. Todas las anteriores. -> correcta 

7. El proceso KDD es interactivo e iterativo.
A. Verdadero. -> correcta 
B. Falso.

8. La elección de los algoritmos de minería de datos incluye:
A. Seleccionar el método o métodos que se utilizarán para buscar patrones en
los datos. -> correcta 
B. Búsqueda de patrones de interés en una forma de representación
particular.
C. La interpretación de los patrones descubiertos.
D. Ninguna de las anteriores.

9. Un sistema de aprendizaje consta, en parte, de las siguientes etapas:
A. Selección del objetivo de aprendizaje.
B. Selección del conjunto de datos de entrenamiento.
C. Selección de una función objetivo.
D. Todas las anteriores -> correcta 

10. El aprendizaje automático tiene múltiples aplicaciones en otro tipo de sistemas,
como en aquellos relacionados con la robótica, o en sistemas de reconocimiento del
habla.
A. Verdadero. -> correcta 
B. Falso.

## Videoclase 1. Introducción al Aprendizaje Automático
## Notas
- ¿Cuál es el objetivo principal del aprendizaje automático en la inteligencia artificial?
    - Aprender y mejorar automáticamente a partir de datos para realizar tareas sin intervención humana constante.
- ¿Cómo contribuye el aprendizaje automático al diagnóstico médico?
    - Aprende de grandes volúmenes de datos médicos para identificar patrones y predecir resultados de salud.
- ¿Cuál es una aplicación del aprendizaje automático en la planificación urbana?  
    - Planificando y desarrollando infraestructuras y servicios urbanos eficientes considerando múltiples variables y datos variados.
- ¿Qué técnica de aprendizaje automático se ajusta mejor al uso de datos sin etiquetas predefinidas?  
    - Clustering o agrupamiento, que organiza los datos en grupos según sus características similares.
- ¿En qué consiste el procesamiento del lenguaje natural (NLP) en el contexto del aprendizaje automático?  
    - Permite que las máquinas comprendan, interpreten y generen lenguaje humano de manera coherente.            

## Videoclase 2. Métodos supervisados y No Supervisados
## Notas
- ¿Cuál es la diferencia principal entre los métodos supervisados y no supervisados en aprendizaje automático?
    - Supervisados utilizan datos etiquetados para entrenar modelos, mientras que no supervisados buscan patrones en datos sin etiquetas.
- ¿Qué técnica de clustering se utiliza para agrupar datos basándose en la densidad de puntos?
    - DBSCAN (Density-Based Spatial Clustering of Applications with Noise).
- ¿Qué aplicación práctica tienen los métodos no supervisados en el análisis de imágenes?
    - Descubrir patrones comunes y características en grandes conjuntos de imágenes sin etiquetas predefinidas. -> La regresión logística se utiliza para predecir eventos binarios
- ¿Cuál es el uso de la regresión logística en métodos supervisados?
    - Predecir la probabilidad de un evento binario, como la compra de un producto basado en el historial del cliente.
- ¿Por qué es importante el preprocesamiento de datos en el aprendizaje automático?  
    - Para mejorar la calidad de los datos eliminando errores, manejando valores faltantes y asegurando su adecuación para el análisis posterior con ML.


## Videoclase 3. Métodos supervisados y No Supervisados
## Notas
- ¿Qué son las Redes Neuronales Artificiales (ANN) y cuál es su función principal?
    - Modelos computacionales inspirados en el cerebro humano que procesan información a través de capas de neuronas para tareas de clasificación y predicción.
- ¿Cuál es la diferencia entre una red neuronal convolucional (CNN) y una red neuronal recurrente (RNN)?
    - CNN se especializa en procesar datos estructurados como imágenes, mientras que RNN maneja datos secuenciales como texto y series temporales.
- ¿Qué función cumple la capa de pooling en una CNN?
    - Reduce la dimensionalidad de las características extraídas, disminuyendo la carga computacional y evitando el sobreajuste. -> La capa de pooling en CNN reduce la dimensionalidad y la carga computacional
- ¿Cuál es la principal ventaja de utilizar LSTM (Long Short-Term Memory) en redes neuronales recurrentes (RNN)? 
    - Permite superar el problema del gradiente desvanecido, facilitando el aprendizaje de relaciones a largo plazo en datos secuenciales. -> LSTM mejora las redes neuronales recurrentes (RNN) al resolver el problema del gradiente desvanecido, lo que permite a la red aprender dependencias a largo plazo en datos secuenciales. 
- ¿Cuál es el propósito del uso de TensorFlow y Keras en Deep Learning?  
    - Facilitar la construcción y entrenamiento de modelos de aprendizaje profundo mediante una interfaz amigable y herramientas de optimización.


# Tema 10. Aplicación en Inteligencia Artificial


## 10.2. Objetivos de la Inteligencia Artificial aplicada

- Automatización de tareas repetitivas y monótonas
    - La automatización de tareas repetitivas y monótonas ha sido uno de los principales motores del desarrollo de la IA

**Aplicaciones:** 
- Industria manufacturera: robots industriales que ensamblan productos en líneas de
producción, como en la fabricación de automóviles.
- Servicios al cliente: chatbots y asistentes virtuales que responden consultas
frecuentes, gestionan reservas y solucionan problemas básicos.
- Oficinas: sistemas de gestión documental que automatizan la clasificación y archivo
de documentos, así como la generación de informes financieros y contables.


- Mejora de la toma de decisiones
    - La capacidad de la IA para analizar grandes conjuntos de datos y extraer patrones y tendencias permite a las organizaciones tomar decisiones más informadas y precisas. 

**Aplicaciones:** 
- Finanzas: algoritmos de IA que analizan datos de mercado y predicen movimientos
financieros para la gestión de inversiones y la detección de fraudes.
- Salud: sistemas de apoyo a la decisión clínica que analizan historiales médicos y
datos de pacientes para proporcionar diagnósticos más precisos y personalizados.
- Marketing: plataformas que segmentan clientes y personalizan campañas
publicitarias basadas en el análisis del comportamiento de los usuarios y sus
preferencias.

- Personalización de experiencias y servicios
    - La IA permite a las empresas ofrecer productos y servicios altamente personalizados, mejorando la experiencia del cliente y aumentando la satisfacción y fidelidad. 

**Aplicaciones:**
- E-commerce: recomendaciones de productos basadas en el historial de navegación
y compras del usuario, como las que ofrece Amazon.
- Educación: plataformas de aprendizaje adaptativo que ajustan el contenido y el ritmo
de enseñanza según el progreso y las necesidades del estudiante.
- Entretenimiento: servicios de streaming como Netflix que sugieren películas y series
basadas en los gustos y hábitos de visualización del usuario. 


- Innovación y desarrollo tecnológico
    - La IA impulsa la innovación al abrir nuevas posibilidades tecnológicas y permitir la creación de soluciones que antes eran inimaginables. Desde el descubrimiento de nuevos materiales hasta avances en biotecnología, la IA está en el centro de las revoluciones tecnológicas contemporáneas


**Aplicaciones:**
- Investigación científica: utilización de algoritmos de IA para modelar fenómenos
complejos y acelerar el descubrimiento de nuevos fármacos y materiales.
- Automoción: desarrollo de vehículos autónomos que utilizan la IA para navegar y
operar de manera segura, reduciendo el riesgo de accidentes y mejorando la
eficiencia del tráfico.
- Energía: optimización de redes eléctricas inteligentes y gestión eficiente de recursos
energéticos mediante el análisis de datos de consumo y producción.


- Mejora de la interacción humano-máquina
    - La mejora de la interacción humano-máquina es fundamental para aumentar la
    accesibilidad y usabilidad de las tecnologías avanzadas. Interfaces intuitivas y
    amigables permiten a los usuarios aprovechar al máximo las capacidades de la IA sin
    necesidad de conocimientos técnicos avanzados


**Aplicaciones:**
- Asistentes de voz: sistemas como Alexa y Google Assistant que mejoran
continuamente en el reconocimiento de voz y la comprensión del lenguaje natural,
permitiendo interacciones más fluidas y naturales.
- Realidad aumentada y virtual: interfaces inmersivas que se utilizan en educación,
entretenimiento y formación profesional, ofreciendo experiencias interactivas y
atractivas.
- Robótica colaborativa: robots diseñados para trabajar junto a humanos en entornos
compartidos, mejorando la eficiencia y seguridad en sectores como la manufactura y
la logística.

- Resolución de problemas complejos
    La capacidad de la IA para analizar grandes volúmenes de datos y realizar cálculos
    avanzados permite abordar problemas complejos en diversas áreas. 

**Aplicaciones:**
- Ciencias ambientales: modelado y predicción de fenómenos climáticos para
entender mejor el cambio climático y desarrollar estrategias de mitigación.
- Biomedicina: análisis de datos genómicos y moleculares para identificar patrones y
desarrollar tratamientos personalizados basados en las características individuales
de cada paciente.
- Seguridad: detección y prevención de ciberataques mediante el análisis de patrones
de comportamiento y la implementación de respuestas automáticas.

- Optimización de recursos
    - La IA permite una gestión más eficiente de los recursos mediante la optimización de
    procesos y la predicción de necesidades.

**Aplicaciones:**
- Logística: optimización de rutas de transporte, gestión de inventarios y reducción de
tiempos de entrega mediante el uso de algoritmos de optimización.
- Agricultura: implementación de agricultura de precisión utilizando drones y sensores
para monitorear cultivos y optimizar el uso de agua, fertilizantes y pesticidas.
- Energía: gestión inteligente de redes eléctricas que permite una distribución eficiente
de la energía, integración de fuentes renovables y reducción de pérdidas.



## 10.3. Datos masivos en la Inteligencia Artificial

La relación entre datos masivos y la IA es simbiótica: los datos masivos proporcionan
la materia prima necesaria para entrenar modelos de IA, mientras que los algoritmos
de IA ofrecen las herramientas necesarias para analizar y extraer valor de estos
datos.

- Salud:
    - Predicción y diagnóstico:
    - Medicina personalizada:
- Finanzas:
    - Detección de fraudes:
    - Análisis de riesgos:
- Marketing y publicidad:
    - Personalización
    - Análisis de sentimientos
- Transporte y logística:
    - Optimización de rutas
    - Mantenimiento predictivo


## 10.4. La visión, el lenguaje natural y conocimiento

La inteligencia artificial (IA) ha logrado avances significativos en tres áreas clave: la
visión por computadora, el procesamiento del lenguaje natural (NLP) y la
representación del conocimiento. Estas tecnologías permiten a los sistemas de IA
interpretar y comprender el mundo visual, comunicarse en lenguaje humano y
organizar información de manera eficiente. 

**Visión por computadora**
La visión por computadora permite a los sistemas de IA interpretar y analizar
información visual del entorno. Esta tecnología se basa en algoritmos avanzados y
redes neuronales convolucionales 

**Procesamiento del lenguaje natural (NLP)**
El procesamiento del lenguaje natural (NLP) permite a las máquinas comprender,
interpretar y generar lenguaje humano, lo cual es crucial para mejorar la interacción
entre humanos y máquinas. 

**Representación y gestión del conocimiento**
La representación del conocimiento implica la organización y estructuración de
información de manera que pueda ser utilizada eficientemente por los sistemas de
IA.




## 10.5. Ejemplos de aplicación de la Inteligencia Artificial

IA Generativa: un ejemplo actual de la aplicación de la IA
La inteligencia artificial generativa, una rama avanzada de la IA, ha emergido como
una tecnología transformadora capaz de crear contenido nuevo a partir de datos
existentes.

Generación de textos
La IA generativa ha revolucionado la creación de contenido textual

Creación de imágenes
Modelos como DALL-E y GANs pueden generar imágenes realistas y creativas a
partir de descripciones textuales.

Música y Audio
La IA generativa también se aplica en la creación de música y contenido de audio.
Modelos como Jukedeck y OpenAI's MuseNet pueden componer música en varios
estilos y géneros, 

Modelado 3D y animación
La generación de contenido 3D ha sido potenciada por la IA generativa, permitiendo
la creación de modelos y animaciones complejas con menor esfuerzo humano

Impacto y futuro de la IA generativa
La IA generativa está transformando diversas industrias al automatizar la creación de
contenido y ofrecer nuevas herramientas creativas. Sin embargo, también plantea
desafíos, como la necesidad de gestionar el uso ético de la tecnología, evitar la
generación de contenido malicioso o engañoso, y garantizar que las creaciones de IA
respeten los derechos de propiedad intelectual.

## Cuestionario 

1. ¿Cuál es una de las principales aplicaciones de la IA generativa en el campo de
la salud?
A. Diagnóstico de enfermedades utilizando imágenes médicas. -> Correcto 
B. Creación de algoritmos de búsqueda.
C. Desarrollo de videojuegos.
D. Automatización de procesos financieros.


2. ¿Cuál de las siguientes es una ventaja clave de la visión por computadora?
A. Requiere menos datos para entrenar modelos precisos.
B. Puede analizar imágenes y videos con gran precisión y velocidad. -> Correcto 
C. Es menos costosa que otros métodos de IA.
D. No plantea preocupaciones de privacidad.


3. ¿Qué tecnología se utiliza comúnmente en el procesamiento del lenguaje natural
para comprender el contexto de las palabras en una oración?
A. Redes convolucionales.
B. Algoritmos genéticos.
C. Redes neuronales recurrentes. -> Correcto 
D. Redes de retropropagación.


4. ¿Cuál es uno de los desafíos principales de los modelos de IA generativa?
A. Requieren una infraestructura de hardware muy simple.
B. No pueden generar datos sintéticos.
C. Pueden perpetuar sesgos presentes en los datos de entrenamiento.-> Correcto 
D. Son fáciles de implementar sin conocimientos técnicos.

5. ¿Cuál de las siguientes opciones describe mejor una red generativa adversaria (GAN)?
A. Un algoritmo que clasifica datos en categorías predefinidas
B. Una red neuronal que traduce texto de un idioma a otro
C. Un modelo de IA que genera datos nuevos y realistas mediante la
competencia entre dos redes neuronales -> Correcto
D. Un sistema que optimiza rutas de transporte


6. ¿Qué es el aprendizaje supervisado en el contexto del aprendizaje automático?
A. Un método donde la máquina aprende a partir de datos no etiquetados.
B. Un enfoque que no requiere datos de entrenamiento.
C. Un método donde la máquina aprende a partir de datos etiquetados
proporcionados por humanos. -> Correcto 
D. Un proceso que implica solo la optimización de hardware.


7. ¿Cuál es una aplicación práctica de los sistemas expertos en la medicina?
A. Automatización de la fabricación de automóviles.
B. Diagnóstico de enfermedades basado en el conocimiento médico -> Correcto 
acumulado.
C. Desarrollo de videojuegos.
D. Análisis de sentimientos en redes sociales.


8. ¿Qué herramienta se utiliza comúnmente para la representación del
conocimiento en IA?
A. Redes convolucionales.
B. Algoritmos genéticos.
C. Grafos de conocimiento. -> Correcto 
D. Redes neuronales recurrentes.

9. ¿Cuál de las siguientes es una desventaja de la IA generativa en el ámbito de la privacidad?
A. Requiere menos datos para entrenar modelos precisos.
B. Puede generar contenido malicioso o engañoso. -> Correcto 
C. Facilita la creación de modelos predictivos.
D. Mejora la personalización de servicios.

10. ¿Qué ventaja ofrece la personalización a través del uso de IA en el marketing?
A. Reducción de costos de producción.
B. Mejora en la precisión del diagnóstico médico.
C. Incremento en la satisfacción y fidelidad del cliente. -> Correcto 
D. Optimización de rutas logísticas.




## Videoclase 1: La inteligencia Artificial en la Ciencia de Datos
## Notas:
- ¿Cuál es el papel del procesamiento del lenguaje natural (NLP) en la inteligencia artificial aplicada?
  - Facilitar la comprensión y generación de lenguaje humano por máquinas para mejorar la interacción con usuarios.
- ¿Qué ventajas ofrece la integración de IA y ciencia de datos para la toma de decisiones en empresas?
  - Permite automatizar decisiones en tiempo real mediante el análisis de grandes volúmenes de datos, mejorando la precisión y eficiencia operativa.
- ¿Cómo mejora la visión por computadora los procesos industriales en manufactura?
  - 
Permite la detección automática de defectos en productos durante la producción, mejorando la calidad y reduciendo los desperdicios.
- ¿Qué beneficio clave proporciona el uso de datos masivos en la IA?
  - Facilita el entrenamiento de modelos de IA al permitir el aprendizaje de grandes cantidades de datos, mejorando la precisión de las predicciones.
- ¿Cuál es una aplicación de la IA generativa en el diseño gráfico y publicidad?
  - Permite crear nuevas obras de arte y variaciones de diseño, mejorando la creatividad y eficiencia en campañas publicitarias.
