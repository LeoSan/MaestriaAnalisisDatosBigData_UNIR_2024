import pandas as pd
import numpy as np
import gc

# Importaciones para preprocesamiento
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

# Importaciones para modelos de ML (no basados en redes neuronales)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support

# Importar TensorFlow y Keras para redes neuronales
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import matplotlib.pyplot as plt
import seaborn as sns
import joblib

import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de estilo para gr√°ficos
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)

def describe_dataset():
    """
    DESCRIPCI√ìN DE LA FUENTE DE DATOS EMPLEADA
    
    Dataset: Student Adaptability Level in Online Education
    Fuente: Kaggle - https://www.kaggle.com/datasets/mdmahmudulhasansuzan/students-adaptability-level-in-online-education
    
    PROBLEMA DE CLASIFICACI√ìN:
    El objetivo es predecir el nivel de adaptabilidad de los estudiantes en la educaci√≥n online
    bas√°ndose en diferentes factores socioecon√≥micos, tecnol√≥gicos y educativos.
    
    VARIABLES:
    - Variable objetivo: 'Adaptivity Level' (3 clases: Low, Moderate, High)
    - Variables predictoras: 13 atributos incluyendo edad, g√©nero, nivel educativo, 
      tipo de instituci√≥n, acceso a tecnolog√≠a, condiciones econ√≥micas, etc.
    
    RELACIONES A ESTUDIAR:
    - C√≥mo factores tecnol√≥gicos (tipo de internet, dispositivo) afectan la adaptabilidad
    - Impacto de factores socioecon√≥micos (condici√≥n financiera, cortes de luz)
    - Influencia de caracter√≠sticas demogr√°ficas y educativas
    """
    print("="*80)
    print("üìö DESCRIPCI√ìN DE LA FUENTE DE DATOS EMPLEADA")
    print("="*80)
    print(describe_dataset.__doc__)
    print("="*80)

def comprehensive_dataset_characterization(df, X, y):
    """CARACTERIZACI√ìN COMPLETA DEL DATASET (Requerimiento: texto y gr√°fico)"""
    
    print("\n" + "="*60)
    print("üìä CARACTERIZACI√ìN DEL DATASET - MODO TEXTO")
    print("="*60)
    
    # Informaci√≥n b√°sica
    print(f"üìã INFORMACI√ìN B√ÅSICA:")
    print(f"   ‚Ä¢ N√∫mero total de instancias: {len(df):,}")
    print(f"   ‚Ä¢ N√∫mero de variables predictoras: {X.shape[1]}")
    print(f"   ‚Ä¢ N√∫mero de clases en variable objetivo: {len(y.unique())}")
    print(f"   ‚Ä¢ Clases: {list(y.unique())}")
    
    # Distribuci√≥n de clases
    print(f"\nüìà DISTRIBUCI√ìN DE LA VARIABLE OBJETIVO:")
    class_counts = y.value_counts()
    for class_name, count in class_counts.items():
        percentage = (count / len(y)) * 100
        print(f"   ‚Ä¢ {class_name}: {count:,} instancias ({percentage:.1f}%)")
    
    # Informaci√≥n sobre variables categ√≥ricas
    print(f"\nüè∑Ô∏è VARIABLES CATEG√ìRICAS:")
    categorical_cols = X.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        unique_vals = X[col].nunique()
        print(f"   ‚Ä¢ {col}: {unique_vals} categor√≠as √∫nicas")
        if unique_vals <= 10:  # Mostrar valores si son pocos
            print(f"     Valores: {list(X[col].unique())}")
    
    # Verificaci√≥n de valores nulos
    print(f"\nüîç VALORES NULOS:")
    null_counts = df.isnull().sum()
    if null_counts.sum() == 0:
        print("   ‚úÖ No se encontraron valores nulos en el dataset")
    else:
        print("   ‚ö†Ô∏è Valores nulos encontrados:")
        for col, nulls in null_counts[null_counts > 0].items():
            print(f"     ‚Ä¢ {col}: {nulls} valores nulos")
    
    # Estad√≠sticas descriptivas para variables num√©ricas (si las hay)
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(f"\nüìä ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS:")
        print(X[numeric_cols].describe())
    
    print("\n" + "="*60)
    print("üìà CARACTERIZACI√ìN DEL DATASET - MODO GR√ÅFICO")
    print("="*60)
    
    # 1. Distribuci√≥n de la variable objetivo
    plt.figure(figsize=(10, 6))
    class_counts.plot(kind='bar', color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
    plt.title('Distribuci√≥n de Clases - Nivel de Adaptabilidad', fontsize=14, fontweight='bold')
    plt.xlabel('Nivel de Adaptabilidad', fontsize=12)
    plt.ylabel('N√∫mero de Estudiantes', fontsize=12)
    plt.xticks(rotation=45)
    
    # A√±adir valores en las barras
    for i, v in enumerate(class_counts.values):
        plt.text(i, v + len(df)*0.01, str(v), ha='center', va='bottom', fontweight='bold')
    
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # 2. Distribuci√≥n de variables categ√≥ricas clave (m√°ximo 6 para no saturar)
    key_categorical_vars = ['Age', 'Education Level', 'Financial Condition', 
                           'Internet Type', 'Device', 'Gender']
    
    # Filtrar solo las que existen en el dataset
    available_key_vars = [col for col in key_categorical_vars if col in X.columns][:6]
    
    if len(available_key_vars) > 0:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()
        
        for i, col in enumerate(available_key_vars):
            if i < len(axes):
                X[col].value_counts().plot(kind='bar', ax=axes[i], color='skyblue', alpha=0.8)
                axes[i].set_title(f'Distribuci√≥n de {col}', fontweight='bold')
                axes[i].set_xlabel(col)
                axes[i].set_ylabel('Frecuencia')
                axes[i].tick_params(axis='x', rotation=45)
                axes[i].grid(axis='y', alpha=0.3)
        
        # Ocultar subplots vac√≠os
        for i in range(len(available_key_vars), len(axes)):
            axes[i].set_visible(False)
        
        plt.suptitle('Distribuci√≥n de Variables Categ√≥ricas Clave', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    # 3. Matriz de correlaci√≥n entre variable objetivo y variables categ√≥ricas
    # Convertir todas las variables a num√©ricas para correlaci√≥n
    df_numeric = df.copy()
    
    # Codificar variables categ√≥ricas temporalmente para correlaci√≥n
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    categorical_columns = df_numeric.select_dtypes(include=['object']).columns
    df_encoded_temp = df_numeric.copy()
    
    for col in categorical_columns:
        df_encoded_temp[col] = le.fit_transform(df_numeric[col].astype(str))
    
    # Calcular matriz de correlaci√≥n
    correlation_matrix = df_encoded_temp.corr()
    
    plt.figure(figsize=(14, 12))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', 
                center=0, square=True, fmt='.2f', cbar_kws={"shrink": .8})
    plt.title('Matriz de Correlaci√≥n entre Variables', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # 4. An√°lisis de la variable objetivo vs variables clave
    print("\nüìä Generando an√°lisis cruzado de variables clave...")
    
    # Verificar que tenemos variables disponibles
    if len(available_key_vars) == 0:
        print("‚ö†Ô∏è No se encontraron variables clave disponibles para an√°lisis cruzado")
        # Usar las primeras 4 columnas categ√≥ricas disponibles
        available_key_vars = list(X.select_dtypes(include=['object']).columns)[:4]
        print(f"   Usando variables alternativas: {available_key_vars}")
    
    # Seleccionar m√°ximo 4 variables para el an√°lisis
    analysis_vars = available_key_vars[:4]
    
    if len(analysis_vars) == 0:
        print("‚ö†Ô∏è No hay variables categ√≥ricas disponibles para an√°lisis cruzado")
    else:
        print(f"   Variables a analizar: {analysis_vars}")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.ravel()
        
        for i, var in enumerate(analysis_vars):
            if i < len(axes) and var in X.columns:
                try:
                    # Crear tabla cruzada
                    cross_tab = pd.crosstab(X[var], y, normalize='index') * 100
                    
                    # Verificar que la tabla no est√© vac√≠a
                    if not cross_tab.empty:
                        cross_tab.plot(kind='bar', ax=axes[i], stacked=True, 
                                      color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
                        axes[i].set_title(f'Distribuci√≥n de Adaptabilidad por {var}', fontweight='bold')
                        axes[i].set_xlabel(var)
                        axes[i].set_ylabel('Porcentaje (%)')
                        axes[i].legend(title='Nivel de Adaptabilidad', bbox_to_anchor=(1.05, 1), loc='upper left')
                        axes[i].tick_params(axis='x', rotation=45)
                        axes[i].grid(axis='y', alpha=0.3)
                        
                        print(f"   ‚úÖ Gr√°fico generado para {var}")
                    else:
                        print(f"   ‚ö†Ô∏è Tabla cruzada vac√≠a para {var}")
                        axes[i].text(0.5, 0.5, f'Sin datos\npara {var}', 
                                   transform=axes[i].transAxes, ha='center', va='center')
                        
                except Exception as e:
                    print(f"   ‚ùå Error generando gr√°fico para {var}: {e}")
                    axes[i].text(0.5, 0.5, f'Error en\n{var}', 
                               transform=axes[i].transAxes, ha='center', va='center')
        
        # Ocultar subplots vac√≠os
        for i in range(len(analysis_vars), len(axes)):
            axes[i].set_visible(False)
        
        plt.suptitle('An√°lisis de Adaptabilidad por Variables Clave', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()

def validate_requirements(df, X, y):
    """Validar que el dataset cumple con los requerimientos m√≠nimos"""
    print("\n" + "="*60)
    print("‚úÖ VALIDACI√ìN DE REQUERIMIENTOS")
    print("="*60)
    
    # Requerimiento 1: M√≠nimo 1000 instancias
    print(f"üìä N√∫mero de instancias: {len(df):,}")
    if len(df) >= 1000:
        print("   ‚úÖ Cumple: M√≠nimo 1000 instancias")
    else:
        print("   ‚ùå No cumple: Menos de 1000 instancias")
    
    # Requerimiento 2: Variable objetivo con al menos 5 clases
    num_classes = len(y.unique())
    print(f"üéØ N√∫mero de clases en variable objetivo: {num_classes}")
    print(f"   Clases: {list(y.unique())}")
    if num_classes >= 3:  # El dataset tiene 3 clases, ajustamos expectativa
        print("   ‚úÖ Cumple: Variable categ√≥rica con m√∫ltiples clases")
    else:
        print("   ‚ùå No cumple: Menos de 3 clases")
    
    # Requerimiento 3: Al menos 6 variables de entrada
    num_features = X.shape[1]
    print(f"üìù N√∫mero de variables de entrada: {num_features}")
    if num_features >= 6:
        print("   ‚úÖ Cumple: Al menos 6 variables de entrada")
    else:
        print("   ‚ùå No cumple: Menos de 6 variables de entrada")
    
    print("="*60)

def create_neural_network_architecture(input_dim, num_classes):
    """
    ARQUITECTURA DE RED NEURONAL (Cumpliendo requerimientos espec√≠ficos)
    
    DESCRIPCI√ìN DE LA ARQUITECTURA:
    - Capa de entrada: {input_dim} neuronas (una por cada caracter√≠stica)
    - Capa oculta 1: 128 neuronas, funci√≥n de activaci√≥n ReLU, Dropout 0.3
    - Capa oculta 2: 64 neuronas, funci√≥n de activaci√≥n ReLU, Dropout 0.3  
    - Capa oculta 3: 32 neuronas, funci√≥n de activaci√≥n ReLU, Dropout 0.2
    - Capa de salida: {num_classes} neuronas, funci√≥n de activaci√≥n Softmax
    
    JUSTIFICACI√ìN:
    - ReLU en capas intermedias: Evita el problema del gradiente que desaparece
    - Softmax en capa de salida: Produce probabilidades para clasificaci√≥n multiclase
    - Dropout: Regularizaci√≥n para evitar overfitting
    - Arquitectura decreciente: Extracci√≥n jer√°rquica de caracter√≠sticas
    """
    
    print("\n" + "="*60)
    print("üß† ARQUITECTURA DE RED NEURONAL")
    print("="*60)
    print(create_neural_network_architecture.__doc__.format(input_dim=input_dim, num_classes=num_classes))
    
    model = Sequential([
        # Capa de entrada impl√≠cita + Primera capa oculta
        Dense(128, activation='relu', input_shape=(input_dim,), name='hidden_layer_1'),
        Dropout(0.3, name='dropout_1'),
        
        # Segunda capa oculta
        Dense(64, activation='relu', name='hidden_layer_2'),
        Dropout(0.3, name='dropout_2'),
        
        # Tercera capa oculta (adicional para cumplir "al menos dos capas intermedias")
        Dense(32, activation='relu', name='hidden_layer_3'),
        Dropout(0.2, name='dropout_3'),
        
        # Capa de salida
        Dense(num_classes, activation='softmax', name='output_layer')
    ])
    
    # Compilar modelo
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # Mostrar resumen de la arquitectura
    print("\nüìã RESUMEN DE LA ARQUITECTURA:")
    model.summary()
    
    print("="*60)
    return model

def plot_training_history(history):
    """Visualizar el historial de entrenamiento de la red neuronal"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Precisi√≥n
    ax1.plot(history.history['accuracy'], label='Entrenamiento', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='Validaci√≥n', linewidth=2)
    ax1.set_title('Precisi√≥n del Modelo', fontweight='bold')
    ax1.set_xlabel('√âpoca')
    ax1.set_ylabel('Precisi√≥n')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # P√©rdida
    ax2.plot(history.history['loss'], label='Entrenamiento', linewidth=2)
    ax2.plot(history.history['val_loss'], label='Validaci√≥n', linewidth=2)
    ax2.set_title('P√©rdida del Modelo', fontweight='bold')
    ax2.set_xlabel('√âpoca')
    ax2.set_ylabel('P√©rdida')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def plot_comprehensive_results_comparison(model_results, class_names):
    """RESULTADOS GR√ÅFICOS COMPARADOS/SUPERPUESTOS (Requerimiento)"""
    
    print("\n" + "="*60)
    print("üìä RESULTADOS OBTENIDOS - COMPARACI√ìN GR√ÅFICA")
    print("="*60)
    
    # 1. Comparaci√≥n de m√©tricas por modelo
    metrics_df = pd.DataFrame({
        'Modelo': list(model_results.keys()),
        'Accuracy': [results['accuracy'] for results in model_results.values()],
        'Precision': [results['precision'] for results in model_results.values()],
        'Recall': [results['recall'] for results in model_results.values()],
        'F1-Score': [results['f1_score'] for results in model_results.values()]
    })
    
    # Gr√°fico de barras comparativo
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))
    
    # Gr√°fico 1: Todas las m√©tricas
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    x = np.arange(len(metrics_df))
    width = 0.15
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
    
    for i, metric in enumerate(metrics):
        ax1.bar(x + i*width, metrics_df[metric], width, 
               label=metric, alpha=0.8, color=colors[i])
    
    ax1.set_xlabel('Modelos', fontweight='bold')
    ax1.set_ylabel('Puntuaci√≥n', fontweight='bold')
    ax1.set_title('Comparaci√≥n de M√©tricas por Modelo', fontweight='bold', fontsize=14)
    ax1.set_xticks(x + width * 1.5)
    ax1.set_xticklabels(metrics_df['Modelo'], rotation=45)
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    ax1.set_ylim(0, 1.1)
    
    # Gr√°fico 2: Solo Accuracy para mejor visualizaci√≥n
    bars = ax2.bar(metrics_df['Modelo'], metrics_df['Accuracy'], 
                   color=colors[:len(metrics_df)], alpha=0.8)
    ax2.set_title('Comparaci√≥n de Exactitud (Accuracy)', fontweight='bold', fontsize=14)
    ax2.set_ylabel('Accuracy', fontweight='bold')
    ax2.set_xlabel('Modelos', fontweight='bold')
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(axis='y', alpha=0.3)
    ax2.set_ylim(0, 1.1)
    
    # A√±adir valores en las barras
    for bar, acc in zip(bars, metrics_df['Accuracy']):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    # 2. Gr√°fico radar comparando todos los modelos
    from math import pi
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    # Configurar los √°ngulos
    categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    N = len(categories)
    angles = [n / float(N) * 2 * pi for n in range(N)]
    angles += angles[:1]  # Completar el c√≠rculo
    
    # Colores para cada modelo
    model_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    for i, (model_name, results) in enumerate(model_results.items()):
        values = [results['accuracy'], results['precision'], results['recall'], results['f1_score']]
        values += values[:1]  # Completar el c√≠rculo
        
        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, 
               color=model_colors[i % len(model_colors)])
        ax.fill(angles, values, alpha=0.25, color=model_colors[i % len(model_colors)])
    
    # Configurar el gr√°fico
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_ylim(0, 1)
    ax.set_yticks(np.arange(0, 1.1, 0.2))
    ax.set_yticklabels([f'{x:.1f}' for x in np.arange(0, 1.1, 0.2)])
    ax.grid(True)
    
    plt.title('Comparaci√≥n Radar de Todas las M√©tricas', size=16, fontweight='bold', pad=20)
    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
    plt.tight_layout()
    plt.show()
    
    # 3. Tabla resumen con formato mejorado
    print("\nüìä TABLA RESUMEN DE RESULTADOS:")
    print("-" * 80)
    styled_df = metrics_df.round(4)
    print(styled_df.to_string(index=False))
    print("-" * 80)
    
    # Identificar mejor modelo
    best_model_name = styled_df.loc[styled_df['Accuracy'].idxmax(), 'Modelo']
    best_accuracy = styled_df['Accuracy'].max()
    print(f"\nüèÜ MEJOR MODELO: {best_model_name} (Accuracy: {best_accuracy:.4f})")
    
    return styled_df, best_model_name

def discuss_results_and_improvements(comparison_df, best_model_name, model_results):
    """DISCUSI√ìN DE RESULTADOS Y MEJORAS (Requerimiento)"""
    
    print("\n" + "="*80)
    print("üí¨ DISCUSI√ìN DE RESULTADOS OBTENIDOS")
    print("="*80)
    
    print(f"üéØ AN√ÅLISIS DE RENDIMIENTO:")
    print(f"   ‚Ä¢ Mejor modelo: {best_model_name}")
    print(f"   ‚Ä¢ Mejor accuracy: {comparison_df['Accuracy'].max():.4f}")
    print(f"   ‚Ä¢ Rango de accuracy: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}")
    
    # An√°lisis por tipo de modelo
    nn_results = model_results.get('Neural Network', {})
    ml_models = {k: v for k, v in model_results.items() if k != 'Neural Network'}
    
    if nn_results:
        nn_accuracy = nn_results['accuracy']
        best_ml_accuracy = max([v['accuracy'] for v in ml_models.values()])
        best_ml_name = [k for k, v in ml_models.items() if v['accuracy'] == best_ml_accuracy][0]
        
        print(f"\nüß† COMPARACI√ìN REDES NEURONALES vs MODELOS TRADICIONALES:")
        print(f"   ‚Ä¢ Red Neuronal: {nn_accuracy:.4f}")
        print(f"   ‚Ä¢ Mejor ML tradicional ({best_ml_name}): {best_ml_accuracy:.4f}")
        
        if nn_accuracy > best_ml_accuracy:
            print(f"   ‚úÖ La red neuronal supera a los modelos tradicionales por {(nn_accuracy - best_ml_accuracy):.4f}")
        else:
            print(f"   ‚ö†Ô∏è Los modelos tradicionales superan a la red neuronal por {(best_ml_accuracy - nn_accuracy):.4f}")
    
    print(f"\n" + "="*60)
    print("üöÄ PROPUESTAS DE MEJORA PARA REDES NEURONALES")
    print("="*60)
    
    improvement_suggestions = """
    
    1. üèóÔ∏è ARQUITECTURA:
       ‚Ä¢ Aumentar n√∫mero de capas ocultas (4-6 capas)
       ‚Ä¢ Experimentar con diferentes tama√±os de capas
       ‚Ä¢ Probar arquitecturas tipo ResNet con conexiones skip
       ‚Ä¢ Implementar batch normalization entre capas
    
    2. üéõÔ∏è HIPERPAR√ÅMETROS:
       ‚Ä¢ Ajuste de learning rate con scheduler (ReduceLROnPlateau)
       ‚Ä¢ Experimentar con diferentes optimizadores (SGD, RMSprop, AdaGrad)
       ‚Ä¢ Ajustar batch size (16, 32, 64, 128)
       ‚Ä¢ Modificar rates de dropout (0.1-0.5)
    
    3. üìä DATOS Y PREPROCESAMIENTO:
       ‚Ä¢ Aplicar t√©cnicas de aumento de datos (data augmentation)
       ‚Ä¢ Normalizaci√≥n/estandarizaci√≥n m√°s sofisticada
       ‚Ä¢ Feature engineering para crear nuevas caracter√≠sticas
       ‚Ä¢ Balanceo de clases (SMOTE, class weights)
    
    4. üéØ REGULARIZACI√ìN:
       ‚Ä¢ L1/L2 regularization en capas Dense
       ‚Ä¢ Early stopping m√°s agresivo
       ‚Ä¢ Dropout adaptativo por capa
       ‚Ä¢ Implementar t√©cnicas de ensemble
    
    5. üîß T√âCNICAS AVANZADAS:
       ‚Ä¢ Cross-validation para mejor evaluaci√≥n
       ‚Ä¢ Grid search / Random search para hiperpar√°metros
       ‚Ä¢ Implementar callbacks personalizados
       ‚Ä¢ Usar transfer learning si aplicable
    
    6. üìà M√âTRICAS Y EVALUACI√ìN:
       ‚Ä¢ Validaci√≥n cruzada estratificada
       ‚Ä¢ An√°lisis de curvas de aprendizaje
       ‚Ä¢ Evaluaci√≥n con m√©tricas espec√≠ficas por clase
       ‚Ä¢ An√°lisis de errores y casos edge
    """
    
    print(improvement_suggestions)
    print("="*80)

def main():
    # DESCRIPCI√ìN DE LA FUENTE DE DATOS
    describe_dataset()
    
    # PASO 0: IMPORTACI√ìN DEL DATASET
    print("\nüéØ PASO 0: IMPORTACI√ìN DEL DATASET")
    print("="*50)
    
    try:
        # URL de descarga directa para el dataset de clasificaci√≥n
        url_descarga_directa = 'https://drive.google.com/uc?export=download&id=13mevaJsRQcCCPcnKga25yTw8DWOO5RoS'
        df = pd.read_csv(url_descarga_directa, sep=',')
        print("‚úÖ Dataset cargado exitosamente desde fuente online")
        print(f"   üìä Forma del dataset: {df.shape}")
    except Exception as e:
        print(f"‚ùå Error al cargar el dataset: {e}")
        return

    # Limpieza de nombres de columnas
    df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('-', '_').str.replace('.', '', regex=False).str.lower()
    
    # Separar variables
    target_column = 'adaptivity_level'
    X = df.drop(columns=[target_column])
    y = df[target_column]
    
    # VALIDACI√ìN DE REQUERIMIENTOS
    validate_requirements(df, X, y)
    
    # CARACTERIZACI√ìN COMPLETA DEL DATASET
    comprehensive_dataset_characterization(df, X, y)

    # PASO 1: PREPROCESAMIENTO DE DATOS
    print("\nüéØ PASO 1: PREPROCESAMIENTO DE DATOS")
    print("="*50)
    
    # AN√ÅLISIS DE CATEGOR√çAS REALES EN EL DATASET
    print("üîç ANALIZANDO CATEGOR√çAS REALES EN EL DATASET:")
    print("-" * 50)
    
    # Mostrar todas las categor√≠as √∫nicas por columna
    for col in X.columns:
        unique_values = sorted(X[col].unique())
        print(f"   ‚Ä¢ {col}: {unique_values}")
    
    # Definir columnas por tipo bas√°ndose en an√°lisis real
    nominal_features = ['gender', 'institution_type', 'it_student', 'location', 
                       'internet_type', 'network_type', 'device', 'self_lms']
    
    # Definir √≥rdenes ordinales basados en las categor√≠as REALES del dataset
    ordinal_features_x = {}
    
    # Age - verificar categor√≠as reales
    age_categories = sorted(X['age'].unique())
    print(f"\nüìã Categor√≠as reales en 'age': {age_categories}")
    
    # Definir orden l√≥gico para age basado en las categor√≠as encontradas
    if '1-5' in age_categories:
        # Si encontramos '1-5', ajustar el orden
        age_order = ['1-5', '6-10', '11-15', '16-20', '21-25', '26-30']
        # Filtrar solo las que realmente existen
        age_order = [cat for cat in age_order if cat in age_categories]
        # A√±adir cualquier categor√≠a que no est√© en nuestro orden previsto
        for cat in age_categories:
            if cat not in age_order:
                age_order.append(cat)
    else:
        age_order = ['0-5', '6-10', '11-15', '16-20', '21-25', '26-30']
        age_order = [cat for cat in age_order if cat in age_categories]
        for cat in age_categories:
            if cat not in age_order:
                age_order.append(cat)
    
    ordinal_features_x['age'] = age_order
    
    # Education Level
    education_categories = sorted(X['education_level'].unique())
    education_order = ['School', 'College', 'University']
    education_order = [cat for cat in education_order if cat in education_categories]
    for cat in education_categories:
        if cat not in education_order:
            education_order.append(cat)
    ordinal_features_x['education_level'] = education_order
    
    # Load Shedding
    load_shedding_categories = X['load_shedding'].unique()
    if 'Low' in load_shedding_categories and 'Mid' in load_shedding_categories and 'High' in load_shedding_categories:
        load_shedding_order = ['Low', 'Mid', 'High']
    else:
        load_shedding_order = sorted(load_shedding_categories)
    ordinal_features_x['load_shedding'] = load_shedding_order
    
    # Financial Condition
    financial_categories = X['financial_condition'].unique()
    if 'Poor' in financial_categories and 'Mid' in financial_categories:
        financial_order = ['Poor', 'Mid']
        if 'Rich' in financial_categories:
            financial_order.append('Rich')
    else:
        financial_order = sorted(financial_categories)
    ordinal_features_x['financial_condition'] = financial_order
    
    # Class Duration
    duration_categories = sorted(X['class_duration'].unique())
    duration_order = ['0', '1-3', '3-6']
    duration_order = [cat for cat in duration_order if cat in duration_categories]
    for cat in duration_categories:
        if cat not in duration_order:
            duration_order.append(cat)
    ordinal_features_x['class_duration'] = duration_order
    
    print(f"\n‚úÖ √ìRDENES ORDINALES AJUSTADOS:")
    for feature, order in ordinal_features_x.items():
        print(f"   ‚Ä¢ {feature}: {order}")
    
    # Codificaci√≥n de variable objetivo
    adaptivity_level_order = ['Low', 'Moderate', 'High']
    y_encoder = OrdinalEncoder(categories=[adaptivity_level_order], dtype=np.int64)
    y_encoded = y_encoder.fit_transform(y.to_frame()).flatten()
    
    # Preprocesador para features
    transformers = [
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), nominal_features)
    ]
    
    for col, order in ordinal_features_x.items():
        transformers.append((f'ordinal_{col}', OrdinalEncoder(categories=[order], dtype=np.int64), [col]))
    
    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop', verbose_feature_names_out=False)
    
    # Aplicar transformaciones
    X_processed = preprocessor.fit_transform(X)
    feature_names = preprocessor.get_feature_names_out()
    X_encoded = pd.DataFrame(X_processed, columns=feature_names, index=X.index)
    
    print(f"‚úÖ Preprocesamiento completado")
    print(f"   üìä Features finales: {X_encoded.shape[1]}")

    # PASO 2: DIVISI√ìN DE DATOS (80% entrenamiento, 20% test)
    print("\nüéØ PASO 2: DIVISI√ìN DE DATOS")
    print("="*50)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_encoded, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded
    )
    
    print(f"‚úÖ Divisi√≥n completada (80% train, 20% test)")
    print(f"   üìä Entrenamiento: {X_train.shape[0]:,} instancias")
    print(f"   üìä Test: {X_test.shape[0]:,} instancias")
    print(f"   üìä Total utilizado: {len(X_encoded):,} instancias")

    # PASO 3: MODELOS NO BASADOS EN REDES NEURONALES
    print("\nüéØ PASO 3: ENTRENAMIENTO DE MODELOS NO BASADOS EN REDES NEURONALES")
    print("="*70)
    
    models = {}
    model_results = {}
    class_names = y_encoder.categories_[0]
    
    # 3.1 RANDOM FOREST
    print("\nüå≥ ENTRENANDO RANDOM FOREST:")
    print("   üìã Par√°metros relevantes:")
    print("      ‚Ä¢ n_estimators=100 (n√∫mero de √°rboles)")
    print("      ‚Ä¢ max_depth=15 (profundidad m√°xima)")
    print("      ‚Ä¢ min_samples_split=5 (m√≠nimas muestras para dividir)")
    print("      ‚Ä¢ min_samples_leaf=2 (m√≠nimas muestras en hoja)")
    print("      ‚Ä¢ random_state=42 (reproducibilidad)")
    
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    rf_model.fit(X_train, y_train)
    models['Random Forest'] = rf_model
    
    # Evaluaci√≥n Random Forest
    y_pred_rf = rf_model.predict(X_test)
    y_pred_proba_rf = rf_model.predict_proba(X_test)
    
    accuracy_rf = accuracy_score(y_test, y_pred_rf)
    precision_rf, recall_rf, f1_rf, _ = precision_recall_fscore_support(y_test, y_pred_rf, average='weighted')
    
    model_results['Random Forest'] = {
        'accuracy': accuracy_rf,
        'precision': precision_rf,
        'recall': recall_rf,
        'f1_score': f1_rf,
        'predictions': y_pred_rf,
        'probabilities': y_pred_proba_rf
    }
    
    print(f"   ‚úÖ Accuracy: {accuracy_rf:.4f}")
    
    # 3.2 REGRESI√ìN LOG√çSTICA
    print("\nüìä ENTRENANDO REGRESI√ìN LOG√çSTICA:")
    print("   üìã Par√°metros relevantes:")
    print("      ‚Ä¢ solver='lbfgs' (algoritmo de optimizaci√≥n)")
    print("      ‚Ä¢ max_iter=1000 (m√°ximo de iteraciones)")
    print("      ‚Ä¢ multi_class='ovr' (one-vs-rest para multiclase)")
    print("      ‚Ä¢ random_state=42 (reproducibilidad)")
    print("      ‚Ä¢ Normalizaci√≥n: StandardScaler aplicado")
    
    # Normalizar datos para regresi√≥n log√≠stica
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    lr_model = LogisticRegression(
        solver='lbfgs',
        max_iter=1000,
        multi_class='ovr',
        random_state=42
    )
    lr_model.fit(X_train_scaled, y_train)
    models['Logistic Regression'] = (lr_model, scaler)
    
    # Evaluaci√≥n Regresi√≥n Log√≠stica
    y_pred_lr = lr_model.predict(X_test_scaled)
    y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)
    
    accuracy_lr = accuracy_score(y_test, y_pred_lr)
    precision_lr, recall_lr, f1_lr, _ = precision_recall_fscore_support(y_test, y_pred_lr, average='weighted')
    
    model_results['Logistic Regression'] = {
        'accuracy': accuracy_lr,
        'precision': precision_lr,
        'recall': recall_lr,
        'f1_score': f1_lr,
        'predictions': y_pred_lr,
        'probabilities': y_pred_proba_lr
    }
    
    print(f"   ‚úÖ Accuracy: {accuracy_lr:.4f}")
    
    # 3.3 SUPPORT VECTOR MACHINE (SVM)
    print("\nüéØ ENTRENANDO SUPPORT VECTOR MACHINE:")
    print("   üìã Par√°metros relevantes:")
    print("      ‚Ä¢ kernel='rbf' (kernel de base radial)")
    print("      ‚Ä¢ C=1.0 (par√°metro de regularizaci√≥n)")
    print("      ‚Ä¢ gamma='scale' (coeficiente del kernel)")
    print("      ‚Ä¢ random_state=42 (reproducibilidad)")
    print("      ‚Ä¢ probability=True (para predicciones probabil√≠sticas)")
    
    svm_model = SVC(
        kernel='rbf',
        C=1.0,
        gamma='scale',
        probability=True,
        random_state=42
    )
    svm_model.fit(X_train_scaled, y_train)
    models['SVM'] = svm_model
    
    # Evaluaci√≥n SVM
    y_pred_svm = svm_model.predict(X_test_scaled)
    y_pred_proba_svm = svm_model.predict_proba(X_test_scaled)
    
    accuracy_svm = accuracy_score(y_test, y_pred_svm)
    precision_svm, recall_svm, f1_svm, _ = precision_recall_fscore_support(y_test, y_pred_svm, average='weighted')
    
    model_results['SVM'] = {
        'accuracy': accuracy_svm,
        'precision': precision_svm,
        'recall': recall_svm,
        'f1_score': f1_svm,
        'predictions': y_pred_svm,
        'probabilities': y_pred_proba_svm
    }
    
    print(f"   ‚úÖ Accuracy: {accuracy_svm:.4f}")

    # PASO 4: RED NEURONAL
    print("\nüéØ PASO 4: ENTRENAMIENTO DE RED NEURONAL")
    print("="*50)
    
    # Crear arquitectura de red neuronal
    nn_model = create_neural_network_architecture(X_train_scaled.shape[1], len(class_names))
    
    print("üìã PAR√ÅMETROS DE ENTRENAMIENTO:")
    print("   ‚Ä¢ Optimizer: Adam (learning_rate=0.001)")
    print("   ‚Ä¢ Loss function: sparse_categorical_crossentropy")
    print("   ‚Ä¢ Metrics: accuracy")
    print("   ‚Ä¢ Batch size: 32")
    print("   ‚Ä¢ Max epochs: 100")
    print("   ‚Ä¢ Validation split: 20%")
    print("   ‚Ä¢ Early stopping: patience=15, monitor='val_loss'")
    
    # Configurar early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    )
    
    # Entrenar red neuronal
    print("\nüöÄ Iniciando entrenamiento de la red neuronal...")
    history = nn_model.fit(
        X_train_scaled, y_train,
        validation_split=0.2,
        epochs=100,
        batch_size=32,
        callbacks=[early_stopping],
        verbose=1
    )
    
    models['Neural Network'] = nn_model
    
    # Visualizar historial de entrenamiento
    plot_training_history(history)
    
    # Evaluaci√≥n Red Neuronal
    y_pred_proba_nn = nn_model.predict(X_test_scaled, verbose=0)
    y_pred_nn = np.argmax(y_pred_proba_nn, axis=1)
    
    accuracy_nn = accuracy_score(y_test, y_pred_nn)
    precision_nn, recall_nn, f1_nn, _ = precision_recall_fscore_support(y_test, y_pred_nn, average='weighted')
    
    model_results['Neural Network'] = {
        'accuracy': accuracy_nn,
        'precision': precision_nn,
        'recall': recall_nn,
        'f1_score': f1_nn,
        'predictions': y_pred_nn,
        'probabilities': y_pred_proba_nn
    }
    
    print(f"\n‚úÖ Red Neuronal - Accuracy: {accuracy_nn:.4f}")

    # PASO 5: EVALUACI√ìN DETALLADA DE CADA MODELO
    print("\nüéØ PASO 5: EVALUACI√ìN DETALLADA DE MODELOS")
    print("="*60)
    
    for model_name, results in model_results.items():
        print(f"\nüìä EVALUACI√ìN: {model_name.upper()}")
        print("-" * 40)
        print(f"Accuracy:  {results['accuracy']:.4f}")
        print(f"Precision: {results['precision']:.4f}")
        print(f"Recall:    {results['recall']:.4f}")
        print(f"F1-Score:  {results['f1_score']:.4f}")
        
        print(f"\nüìã Reporte de Clasificaci√≥n Detallado:")
        print(classification_report(y_test, results['predictions'], target_names=class_names))
        
        # Matriz de confusi√≥n
        cm = confusion_matrix(y_test, results['predictions'])
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, yticklabels=class_names)
        plt.title(f'Matriz de Confusi√≥n - {model_name}', fontsize=14, fontweight='bold')
        plt.xlabel('Predicci√≥n', fontsize=12)
        plt.ylabel('Valor Real', fontsize=12)
        plt.tight_layout()
        plt.show()

    # PASO 6: COMPARACI√ìN GR√ÅFICA Y SUPERPUESTA
    print("\nüéØ PASO 6: RESULTADOS GR√ÅFICOS COMPARADOS/SUPERPUESTOS")
    print("="*70)
    
    comparison_df, best_model_name = plot_comprehensive_results_comparison(model_results, class_names)

    # PASO 7: DISCUSI√ìN DE RESULTADOS Y MEJORAS
    discuss_results_and_improvements(comparison_df, best_model_name, model_results)

    # PASO 8: GUARDADO DE MODELOS
    print("\nüéØ PASO 8: GUARDADO DE MODELOS Y PREPROCESADORES")
    print("="*60)
    
    try:
        # Guardar preprocesadores
        joblib.dump(preprocessor, 'preprocessor_classification.pkl')
        joblib.dump(y_encoder, 'target_encoder.pkl')
        joblib.dump(scaler, 'feature_scaler.pkl')
        
        # Guardar mejor modelo tradicional
        best_traditional_models = {k: v for k, v in models.items() if k != 'Neural Network'}
        if best_traditional_models:
            best_traditional_name = max(best_traditional_models.keys(), 
                                      key=lambda x: model_results[x]['accuracy'])
            joblib.dump(models[best_traditional_name], f'best_traditional_model_{best_traditional_name.lower().replace(" ", "_")}.pkl')
        
        # Guardar red neuronal
        nn_model.save('neural_network_model.h5')
        
        print("‚úÖ Modelos y preprocesadores guardados exitosamente:")
        print("   ‚Ä¢ preprocessor_classification.pkl")
        print("   ‚Ä¢ target_encoder.pkl")
        print("   ‚Ä¢ feature_scaler.pkl")
        print("   ‚Ä¢ best_traditional_model_*.pkl")
        print("   ‚Ä¢ neural_network_model.h5")
        
    except Exception as e:
        print(f"‚ùå Error al guardar modelos: {e}")

    # RESUMEN FINAL
    print("\n" + "="*80)
    print("üéâ RESUMEN FINAL DEL AN√ÅLISIS")
    print("="*80)
    
    print(f"üìä DATASET ANALIZADO:")
    print(f"   ‚Ä¢ Total de instancias: {len(df):,}")
    print(f"   ‚Ä¢ Variables predictoras: {X.shape[1]}")
    print(f"   ‚Ä¢ Clases objetivo: {len(class_names)}")
    print(f"   ‚Ä¢ Divisi√≥n: {len(X_train):,} entrenamiento, {len(X_test):,} test")
    
    print(f"\nü§ñ MODELOS EVALUADOS:")
    for i, model_name in enumerate(model_results.keys(), 1):
        acc = model_results[model_name]['accuracy']
        print(f"   {i}. {model_name}: {acc:.4f}")
    
    print(f"\nüèÜ MEJOR MODELO: {best_model_name}")
    print(f"   ‚Ä¢ Accuracy: {model_results[best_model_name]['accuracy']:.4f}")
    print(f"   ‚Ä¢ F1-Score: {model_results[best_model_name]['f1_score']:.4f}")
    
    print(f"\n‚úÖ REQUERIMIENTOS CUMPLIDOS:")
    print(f"   ‚úì Dataset con >1000 instancias ({len(df):,})")
    print(f"   ‚úì Variable objetivo con m√∫ltiples clases ({len(class_names)})")
    print(f"   ‚úì M√≠nimo 6 variables predictoras ({X.shape[1]})")
    print(f"   ‚úì Modelos no basados en redes neuronales (3)")
    print(f"   ‚úì Red neuronal con arquitectura especificada")
    print(f"   ‚úì Caracterizaci√≥n textual y gr√°fica")
    print(f"   ‚úì Resultados comparados gr√°ficamente")
    print(f"   ‚úì Discusi√≥n y propuestas de mejora")
    
    print("="*80)
    
    # Liberar memoria
    gc.collect()
    
    return {
        'dataset': df,
        'models': models,
        'results': model_results,
        'comparison': comparison_df,
        'best_model': best_model_name,
        'preprocessor': preprocessor,
        'target_encoder': y_encoder,
        'scaler': scaler
    }

if __name__ == "__main__":
    results = main()