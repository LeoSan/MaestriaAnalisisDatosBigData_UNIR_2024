## TEMA 1: Introducción a la estadística

## 1.2. ¿Qué es la estadística?

## ¿Qué es la Estadística?

A menudo, la palabra "estadística" se usa de forma casual para referirse a cualquier colección de datos, como las "estadísticas de desempleo". Sin embargo, para tu maestría, es crucial que entiendas la estadística como una **ciencia**.

En pocas palabras, la **estadística es la ciencia que nos permite aprender de los datos**. Su objetivo principal es **obtener una comprensión profunda de un fenómeno** a partir de la información que tenemos.

### El Proceso Estadístico: Un Viaje por los Datos

Imagina la estadística como un viaje que los datos hacen, desde su origen hasta las conclusiones. Este viaje generalmente tiene las siguientes **fases**:

1.  **Diseño del estudio:** Aquí se planifica cómo se van a obtener los datos. Es como trazar el mapa antes de un viaje.
2.  **Recogida de datos:** Se obtienen los datos de acuerdo con el diseño. Es la parte de recolectar la información.
3.  **Análisis de datos:** Se examinan los datos para encontrar patrones, tendencias y relaciones. Es como interpretar lo que el mapa y los elementos recolectados nos dicen.
4.  **Organización, resumen y presentación:** Se prepara la información para que sea clara y comprensible, mostrando los hallazgos. Es como organizar tu mochila y presentar tus recuerdos del viaje.
5.  **Extracción de conclusiones:** Se interpretan los resultados para tomar decisiones o entender mejor el fenómeno. Es el objetivo final del viaje.

**Importante:** Aunque todas las fases son cruciales, la **recogida de datos es fundamental**. Si los datos son de mala calidad, no importa qué tan bueno sea tu análisis, las conclusiones no serán fiables. Como dice el texto, "no hay buen análisis posible si los datos han sido recogidos de cualquier manera".

### ¿Todos hacen estadística de la misma forma?

No necesariamente. Como el ejemplo de la empresa, diferentes personas pueden trabajar en distintas fases del proceso estadístico. Alguien podría diseñar el experimento, otro recoger los datos, uno más analizarlos y otro presentarlos. ¡Todos están haciendo estadística a su manera! Incluso si solo recibes datos ya recopilados para analizarlos, estás haciendo estadística.

### Tipos de Estadística: Descriptiva e Inferencial

La estadística se divide en dos ramas principales:

* **Estadística Descriptiva:** Esta rama se centra en **organizar, resumir y presentar los datos** de manera que sean comprensibles. Piensa en tablas, gráficos y medidas que te ayudan a describir lo que ya tienes. Los primeros temas de tu curso probablemente se enfocarán aquí.

* **Estadística Inferencial:** Esta rama va un paso más allá. Utiliza la probabilidad y técnicas matemáticas para **sacar conclusiones sobre una población más grande a partir de una muestra de datos**. Es decir, te permite hacer predicciones o generalizaciones. Verás esto más adelante en tu curso.

---

### Para Memorizar:

Piensa en la estadística como un **detective de datos**. Su trabajo es:

1.  **Planear** cómo encontrar pistas (diseño del estudio).
2.  **Recopilar** esas pistas (recogida de datos).
3.  **Analizar** las pistas para encontrar patrones (análisis de datos).
4.  **Organizar y presentar** el caso (organización, resumen y presentación).
5.  **Sacar conclusiones** sobre lo que realmente sucedió (conclusiones).

Y recuerda, la calidad de las **pistas** (datos) es lo más importante.

---

## 1.3. Población, muestra y muestreo

## Población, Muestra y Muestreo: Entendiendo de Dónde Vienen los Datos

Cuando hablamos de **estadística**, no solo nos referimos a números, sino a **datos** que nos dan información sobre algo. Ese "algo" es lo que nos interesa estudiar.

### El Individuo y la Población

Imagina que quieres saber la altura promedio de todos los estudiantes de tu universidad.

* Cada **estudiante** es un **individuo**.
* El **colectivo** de **todos los estudiantes** de la universidad es la **población**.

La estadística se enfoca en estudiar fenómenos que son **colectivos** (es decir, que involucran a una población) y que tienen algún grado de **incertidumbre**. No estudia fenómenos deterministas (como las leyes físicas), sino aquellos donde no podemos predecir el resultado exacto de cada individuo.

### La Muestra: Un Pequeño Trozo de la Realidad

A menudo, es imposible o demasiado costoso estudiar a cada individuo de una **población** completa. Piensa en el ejemplo del transporte en Madrid: no podían preguntar a todos los habitantes.

Aquí es donde entra la **muestra**:

* Una **muestra** es un **subconjunto de individuos** seleccionados de una población.
* El objetivo es que esta muestra sea **representativa** de la población. Esto significa que la muestra debe reflejar la diversidad y las características clave de la población original. Si tu población es 50% hombres y 50% mujeres, una muestra representativa debería tener una proporción similar.

### Muestreo: El Arte de Seleccionar una Muestra

El proceso de **seleccionar** a los individuos que formarán parte de tu muestra se llama **muestreo**. Este proceso es **crucial** porque la calidad de tus conclusiones dependerá directamente de cómo obtengas tus datos. Un buen muestreo asegura que tu muestra sea lo más representativa posible.

### El Inevitable Error de Muestreo e Inferencia Estadística

Siempre que trabajamos con una **muestra** en lugar de con la **población completa**, existe el riesgo de cometer un **error de muestreo**. Este error surge porque estamos tratando de inferir (o extrapolar) las características de toda la **población** a partir de solo un "pedazo" de ella (la **muestra**).

La clave en estadística es **reducir este error al mínimo**, ya que es inherente al proceso.

El proceso de **extrapolar las características y propiedades de la muestra a las de la población** se conoce como **inferencia estadística**. Debido a su importancia, la inferencia estadística es una rama fundamental de la estadística (junto con la estadística descriptiva que vimos antes). Es la parte de la estadística que te permite hacer generalizaciones y predicciones sobre la población basándote en tu muestra.

---

### Para Memorizar:

Imagina que quieres conocer el sabor de una sopa grande:

* La **sopa completa** es tu **población**.
* Una **cucharada** que tomas para probarla es tu **muestra**.
* El acto de **tomar la cucharada** es el **muestreo**.
* Si la cucharada no es un buen reflejo del sabor de toda la sopa (quizás solo tomaste la parte de arriba y el condimento se fue al fondo), entonces tienes un **error de muestreo**.
* Cuando dices "toda la sopa sabe así" basándote en tu cucharada, estás haciendo **inferencia estadística**.

La clave es que esa cucharada sea lo más parecida posible al resto de la sopa para que tu juicio sea acertado.

---

¿Hay algo más en esta sección que te gustaría que te explicara o quieres que pasemos al siguiente tema?


## 1.4. Tipos de variables estadísticas

## Tipos de Variables Estadísticas: Entendiendo la Naturaleza de tus Datos

En estadística, una **variable** es una característica o atributo que puede variar entre los individuos de una población o muestra. Entender los tipos de variables es crucial porque cada tipo requiere diferentes métodos de análisis.

Hay dos grandes categorías para clasificar las variables:

### 1. Variables Categóricas (Cualitativas)

Estas variables representan **cualidades o categorías** y no se miden numéricamente.

* **Nominales:** Son categorías que **no tienen un orden** inherente. Son simplemente etiquetas.
    * **Ejemplos:** Género (Masculino, Femenino), Grupo sanguíneo (A, B, AB, O), Tipo de auto (Sedán, SUV, Camioneta).
* **Ordinales:** Son categorías que **sí tienen un orden o jerarquía** natural, pero la distancia entre las categorías no es necesariamente uniforme o medible.
    * **Ejemplos:** Nivel educativo (Primaria, Secundaria, Universidad), Grado de satisfacción (Muy insatisfecho, Insatisfecho, Neutro, Satisfecho, Muy satisfecho), Curso escolar (1º ESO, 2º ESO, etc.).

### 2. Variables Cuantitativas (Numéricas)

Estas variables representan **cantidades** y se miden con números.

* **Discretas:** Toman un **número finito o contable de valores** (generalmente números enteros). No pueden tener valores intermedios.
    * **Ejemplos:** Número de hijos, Número de asignaturas suspendidas, Número de autos en una casa.
* **Continuas:** Pueden tomar **infinitos valores** dentro de un rango determinado. Se miden, no se cuentan. Suelen ser magnitudes físicas.
    * **Ejemplos:** Altura, Peso, Tiempo empleado en una tarea, Temperatura.

**Un dato importante:** Una variable que en teoría es continua (como la edad) puede ser **recodificada** como categórica ordinal si la agrupamos en intervalos (ej: Menor de 18, 18-25, Mayor de 25). Esto se hace a menudo en encuestas.

---

### Clasificación Según el Enfoque Metodológico

Además de la clasificación anterior, las variables también se pueden clasificar según el **rol que juegan en un estudio o modelo estadístico**:

* **Variables Dependientes (o de Respuesta / Explicada):** Son las variables cuyos valores **dependen o son influenciados** por otras variables en el estudio. Es el **resultado** que queremos explicar o predecir.
    * **Ejemplo:** El "aprobado en Lengua" (variable dependiente) podría depender del "número de horas de estudio" (variable independiente).
* **Variables Independientes (o Explicativas / Predictoras):** Son las variables que se cree que **influyen o causan un cambio** en la variable dependiente. Son los **factores** que usamos para explicar o predecir el resultado.
    * **Ejemplo:** El "número de horas de estudio" (variable independiente) se usa para explicar el "aprobado en Lengua" (variable dependiente).

    * **Sinónimos:** En diferentes disciplinas, puedes encontrar otros términos como **variables endógenas** (para dependientes) y **variables exógenas** (para independientes).

* **Variables Intermediarias / Omitidas (o Confusoras / Latentes):** Estas son variables que **no se incluyen en el estudio o modelo**, pero que en realidad están afectando tanto a la variable independiente como a la dependiente, o solo a la dependiente de forma oculta. No contemplarlas puede llevar a conclusiones erróneas sobre la causalidad.
    * **Ejemplos:** Si estudias la relación entre "horas de estudio" y "rendimiento escolar", la "renta familiar" podría ser una variable omitida. Una mayor renta familiar podría influir tanto en las horas de estudio (acceso a mejores recursos) como en el rendimiento escolar (menos preocupaciones, mejor alimentación), creando una asociación aparente entre horas de estudio y rendimiento que no es tan directa.
    * Es crucial identificarlas para **evitar establecer asociaciones o causalidades infundadas**. Técnicas avanzadas como el Análisis de Covarianza (ANCOVA) permiten "controlar" el efecto de estas variables.

* **Variables Dicotómicas:** Son un tipo especial de variable categórica que solo puede tomar **dos valores posibles**, generalmente representados como 0 y 1. Son muy útiles para indicar la presencia (1) o ausencia (0) de una característica o evento.
    * **Ejemplos:** ¿Fuma? (Sí/No), ¿Aprobado? (Sí/No), Género (Masculino/Femenino).

---

### Para Memorizar:

Piensa en tus datos como piezas de Lego.

* **Variables Categóricas:** Son piezas que tienen una forma específica (cuadrada, redonda, etc.).
    * **Nominales:** Solo se diferencian por el color (rojo, azul, verde). No hay un orden natural.
    * **Ordinales:** Tienen un orden de tamaño (pieza pequeña, mediana, grande). Sabes cuál es más grande, pero no cuánto más grande.
* **Variables Cuantitativas:** Son piezas que tienen un número.
    * **Discretas:** Cuentas las piezas (1 pieza, 2 piezas, 3 piezas). Solo números enteros.
    * **Continuas:** Mides la longitud de la pieza (2.5 cm, 3.1 cm). Puede ser cualquier valor dentro de un rango.

Cuando construyes algo con Lego:

* La **Variable Dependiente** es lo que estás construyendo (tu modelo final).
* Las **Variables Independientes** son las piezas que usas para construirlo.
* Las **Variables Intermediarias/Omitidas** son piezas que no sabías que existían o que no usaste, pero que si las hubieras puesto, el modelo final (variable dependiente) se habría visto diferente o mejor explicado.


## 1.5. Diseño de experimentos

## 1.5. Diseño de Experimentos: Observacionales vs. Experimentales

La forma en que se recogen los datos para un estudio es fundamental y determina qué tipo de conclusiones se pueden sacar. Los estudios estadísticos se dividen principalmente en dos clases:

### 1. Estudios Observacionales

* **¿Qué son?** Son aquellos donde simplemente **observamos y recogemos datos** sin intervenir ni alterar a los individuos de ninguna manera. Es como ser un "observador pasivo".
* **Características Clave:**
    * **No hay intervención:** El investigador no manipula ninguna variable ni aplica tratamientos.
    * **Recolección de información existente:** Se basa en lo que ya está ocurriendo o ha ocurrido.
    * **Ejemplos:**
        * **Encuestas:** Como las que ya vimos, donde se pregunta a la gente sobre sus opiniones, hábitos, etc., sin modificar su comportamiento.
        * Observar el rendimiento académico de estudiantes sin cambiar sus métodos de estudio.
        * Analizar datos históricos de ventas de una empresa.
* **Limitación importante:** Con un estudio observacional, **no se pueden establecer relaciones de causa y efecto**. Solo puedes identificar asociaciones o correlaciones. Por ejemplo, podrías observar que las personas que toman más café tienden a ser más activas, pero no puedes afirmar que el café *causa* esa mayor actividad (podría haber otras razones).

### 2. Estudios Experimentales

* **¿Qué son?** Son aquellos donde el investigador **aplica un tratamiento (o tratamientos)** a un grupo de individuos (llamados aquí "unidades experimentales") y luego **observa los efectos** de dicho tratamiento.
* **Características Clave:**
    * **Intervención activa:** El investigador manipula una o más variables (los tratamientos).
    * **Control de variables:** Se intenta controlar otras variables que podrían influir en los resultados para aislar el efecto del tratamiento.
    * **Asignación aleatoria (idealmente):** Para asegurar que los grupos sean comparables, los individuos se asignan aleatoriamente a los diferentes grupos de tratamiento (o al grupo de control, que no recibe tratamiento).
    * **Ejemplos:**
        * **En bioestadística:** Probar la eficacia de un nuevo medicamento, donde un grupo recibe el medicamento y otro un placebo, observando las diferencias en su salud.
        * Un estudio para ver si un nuevo método de enseñanza mejora las calificaciones, asignando aleatoriamente a los estudiantes a diferentes métodos.
        * Probar diferentes versiones de un sitio web (pruebas A/B) para ver cuál genera más clics.
* **Ventaja clave:** Un estudio experimental bien diseñado **permite establecer relaciones de causa y efecto**. Si el grupo con tratamiento muestra un cambio significativo que el grupo de control no, y las demás condiciones se mantuvieron iguales, se puede inferir que el tratamiento causó el efecto.

### En Resumen y Para Memorizar:

Piensa en la diferencia así:

* **Estudio Observacional = Eres un "espía" o un "historiador".** Solo miras lo que pasa o lo que ya pasó, y registras. No puedes decir que A causó B, solo que A y B a menudo van juntos.
    * **Ejemplo:** Observas que los estudiantes que usan gorra suelen sacar mejores notas. (¿La gorra causa mejores notas? Probablemente no, solo hay una asociación).
* **Estudio Experimental = Eres un "científico en un laboratorio".** Tú creas las condiciones, aplicas algo (el "tratamiento") y luego ves qué sucede. Si lo haces bien, puedes decir que lo que aplicaste *causó* el efecto.
    * **Ejemplo:** Haces que un grupo de estudiantes use una "gorra de la suerte" y otro grupo no, y luego comparas sus notas para ver si la gorra realmente tiene un efecto. (Suponiendo que todo lo demás sea igual).

### Complementando el Tema:

* **Confusión y Variables Confounding (Confusoras):** En los estudios observacionales, es muy fácil que haya variables "confusoras" que expliquen la asociación observada, en lugar de una causalidad directa. Por ejemplo, en el caso del café y la actividad, quizás las personas que beben más café también tienen trabajos más demandantes o estilos de vida más activos en general (estas serían variables confusoras). Los estudios experimentales, especialmente con asignación aleatoria, ayudan a minimizar el efecto de estas variables confusoras.
* **Ética:** La elección entre un estudio observacional y uno experimental a menudo está limitada por consideraciones éticas. No siempre es éticamente aceptable aplicar ciertos "tratamientos" (por ejemplo, exponer a personas a sustancias dañinas) o retener tratamientos beneficiosos. En esos casos, los estudios observacionales son la única opción viable.
* **Validez interna y externa:** Los experimentos bien controlados suelen tener alta **validez interna** (puedes estar seguro de que la causa que investigaste produjo el efecto observado). Sin embargo, a veces tienen menor **validez externa** (los resultados podrían no ser generalizables a la población real fuera del entorno controlado del experimento). Los estudios observacionales, aunque no prueben causalidad, a menudo tienen mayor validez externa porque se realizan en entornos naturales.

## 1.6. Razonamiento estadístico
¡Absolutamente! El razonamiento estadístico es la base para cualquier analista de datos. No se trata solo de aplicar fórmulas, sino de pensar críticamente sobre los datos y las conclusiones.

---

## 1.6. Razonamiento Estadístico: Pensando Críticamente con los Datos

El razonamiento estadístico es la capacidad de entender y evaluar la información estadística de manera crítica. Implica hacerse las preguntas correctas para asegurar que las conclusiones sean válidas y útiles. No es solo un conjunto de herramientas, sino una forma de pensar.

Para desarrollar un pensamiento estadístico sólido, es fundamental plantearse las siguientes preguntas clave (adaptadas de Triola, 2009):

1.  **¿Cuál es el objetivo del estudio?**
    * Antes de sumergirte en los números, debes tener claro qué se busca responder con el estudio. Sin un objetivo claro, los análisis pueden ser inútiles o misleading.

2.  **¿Quién es la fuente de los datos?**
    * Esta es una pregunta crucial. La **fuente de los datos** (quién recopiló, financió o presenta el estudio) puede tener un **interés propio** en los resultados. Si la fuente no es neutral, puede haber una manipulación consciente o inconsciente de los datos o las conclusiones para beneficiar sus propios intereses. A esto se le conoce coloquialmente como el "cocinado" de datos.

3.  **¿Con qué tipo de muestreo se han obtenido los datos?**
    * Como vimos en la unidad anterior, el método de muestreo es vital. Un mal muestreo puede llevar a una **muestra no representativa** de la población, lo que invalida cualquier inferencia posterior. ¿Fue un muestreo aleatorio? ¿Hay sesgos en la selección?

4.  **¿Existen variables que influyan en los resultados y que se hayan omitido?**
    * Ya hablamos de las **variables intermediarias u omitidas**. Son factores que no se tuvieron en cuenta en el estudio, pero que podrían estar influyendo significativamente tanto en las variables explicativas como en las de respuesta. Ignorarlas puede llevar a conclusiones engañosas sobre causalidad.

5.  **¿Las gráficas resumen adecuadamente los datos?**
    * Las visualizaciones son poderosas, pero también pueden ser engañosas. Una gráfica mal diseñada puede distorsionar la percepción de los datos (ejes truncados, escalas inapropiadas, etc.). Un buen razonador estadístico siempre examina la forma en que se presentan los datos.

6.  **¿Las conclusiones se extraen directa y naturalmente de los datos?**
    * Esto significa que las conclusiones deben estar respaldadas por la evidencia presentada. Evita saltar a conclusiones que no se derivan lógicamente de los resultados del análisis. No inventes narrativas que no soporten los datos.

7.  **¿Se ha cumplido el objetivo marcado al principio del estudio y tienen sentido y utilidad práctica las conclusiones obtenidas?**
    * Al final del día, el estudio debe ser útil. Las conclusiones no solo deben ser estadísticamente válidas, sino que también deben tener sentido en el contexto del problema y ofrecer un valor práctico. ¿Realmente ayudaron a responder la pregunta original y a tomar mejores decisiones?

### El Concepto Fundamental de Sesgo

El concepto central de todas estas preguntas es el **sesgo**.

* El **sesgo** en estadística se refiere a cualquier factor que distorsiona los resultados de un estudio, llevándolos a desviarse sistemáticamente de la verdad. Un estudio sesgado no proporciona una imagen precisa de la realidad.

* **Fuentes de Sesgo Comunes:**
    * **Sesgo de la fuente:** Como se mencionó, el interés propio de quien encarga o realiza el estudio.
    * **Sesgo de muestreo:** Cuando la muestra no es representativa de la población (ej., solo encuestar a personas en un lugar específico o a una hora determinada).
    * **Sesgo de respuesta/no respuesta:** Cuando algunas personas son más propensas a responder que otras, o cuando las respuestas son intencionalmente inexactas.
    * **Sesgo de medición:** Cuando los instrumentos de medición no son precisos o las preguntas del cuestionario son capciosas o confusas (como el "efecto de redacción" en el Ejemplo 3).
    * **Sesgo de confirmación:** Cuando los investigadores interpretan los resultados de una manera que confirma sus hipótesis o creencias preexistentes.
    * **Sesgo de publicación:** Tendencia a publicar solo resultados "significativos" o "interesantes", dejando de lado estudios con resultados menos llamativos.

* El Ejemplo 3 ilustra varias formas en que un estudio de opinión pública puede estar sesgado: el momento de la encuesta, la publicación, la ideología de la agencia, y el diseño de las preguntas.

* El Ejemplo 4 resalta la importancia del **contexto**. Un analista de datos que recibe un archivo Excel con solo "X" e "Y" sin ninguna otra información (objetivo del estudio, cómo se recogieron, qué representan X e Y) no puede hacer un análisis estadístico con sentido. Sin contexto, los datos son solo números vacíos. Saber que X e Y son las calificaciones de dos grupos (mañana y tarde) en una asignatura universitaria y que es una muestra aleatoria simple de 10 alumnos por grupo, es el tipo de contexto necesario para empezar a razonar estadísticamente.

---

### Para Memorizar:

Piensa en el razonamiento estadístico como si fueras un **abogado defensor** o un **juez** de los datos.

* **No te creas todo lo que ves a primera vista.**
* **Pregunta SIEMPRE:**
    * ¿Quién lo dice? (Fuente/Sesgo)
    * ¿Cómo lo sabe? (Muestreo/Diseño)
    * ¿Falta información? (Variables omitidas/Contexto)
    * ¿Lo están mostrando de forma justa? (Gráficas)
    * ¿Realmente se desprende de lo que dicen? (Conclusiones)
    * ¿Sirve para algo? (Utilidad práctica)

El **sesgo** es el "pecado capital" de la estadística. Siempre busca dónde podría estar el sesgo.

---

Este es un tema crucial para tu maestría, ya que te permitirá evaluar críticamente no solo tus propios análisis, sino también los de otros. ¿Te gustaría que profundicemos en algún aspecto del sesgo o pasamos al siguiente tema?


## 1.7. Representando los datos: distribución de frecuencias

¡Perfecto! Esta unidad es fundamental para empezar a organizar y entender la estructura de tus datos. La **distribución de frecuencias** es la primera herramienta que usamos para darle sentido a un conjunto de datos brutos.

---

## 1.7. Representando los Datos: Distribución de Frecuencias

Una vez que hemos recolectado los datos, el primer paso para organizarlos y representarlos es contarlos y ver cuántas veces se repite cada valor o categoría de una variable. A estas cuentas las llamamos **frecuencias**.

La **distribución de frecuencias** es una tabla o resumen que muestra cómo los datos se distribuyen entre las diferentes categorías o valores que puede tomar una variable, junto con el número o proporción de veces que cada uno aparece.

Existen cuatro tipos principales de frecuencias:

### 1. Frecuencia Absoluta ($f_i$ o $n_i$)

* **Definición:** Es el **número de veces** que se repite un valor específico o una categoría (modalidad) de una variable en el conjunto de datos.
* **Notación:** Generalmente se denota como $f_i$ o $n_i$, donde el subíndice $i$ hace referencia a la i-ésima categoría o valor.
* **Propiedad Importante:** La suma de todas las frecuencias absolutas de todas las modalidades debe ser igual al **tamaño total de la muestra ($N$)**.
    $$\sum_{i=1}^{k} f_i = N$$
    (Donde $k$ es el número total de categorías o valores únicos).

### 2. Frecuencia Relativa ($h_i$ o $f_i\%$)

* **Definición:** Es la **proporción** de veces que se repite un valor o categoría en relación con el tamaño total de la muestra ($N$). Se obtiene dividiendo la frecuencia absoluta de una modalidad entre el tamaño total de la muestra.
* **Cálculo:**
    $$h_i = \frac{f_i}{N}$$
* **Propiedad Importante:** La suma de todas las frecuencias relativas de todas las modalidades debe ser igual a **1** (o 100% si se expresa en porcentaje).
    $$\sum_{i=1}^{k} h_i = 1$$
* **Utilidad:** Es muy útil para comparar distribuciones de datos de diferentes tamaños, ya que ofrece una perspectiva proporcional.

### 3. Frecuencia Absoluta Acumulada ($F_i$ o $N_i$)

* **Definición:** Es la **suma de las frecuencias absolutas** de una modalidad dada y todas las modalidades anteriores (las que tienen valores menores o iguales).
* **Notación:** Generalmente se denota con letras mayúsculas, como $F_i$ o $N_i$.
* **Cálculo:**
    $$F_i = f_1 + f_2 + \dots + f_i = \sum_{j=1}^{i} f_j$$
* **Propiedad Importante:** La última frecuencia absoluta acumulada (la del último valor o categoría) debe ser igual al **tamaño total de la muestra ($N$)**. Es decir, $F_k = N$.
* **Utilidad:** Nos indica cuántos datos son menores o iguales a un cierto valor.

### 4. Frecuencia Relativa Acumulada ($H_i$)

* **Definición:** Es la **suma de las frecuencias relativas** de una modalidad dada y todas las modalidades anteriores. Por analogía con las frecuencias absolutas acumuladas, se obtienen sumando las frecuencias relativas hasta una determinada modalidad.
* **Cálculo:**
    $$H_i = h_1 + h_2 + \dots + h_i = \sum_{j=1}^{i} h_j$$
* **Propiedad Importante:** La última frecuencia relativa acumulada (la del último valor o categoría) debe ser igual a **1** (o 100%).
* **Utilidad:** Nos indica qué proporción o porcentaje de los datos son menores o iguales a un cierto valor. Esto es especialmente útil para calcular percentiles o cuartiles.

### Ejemplo Práctico (para entender las fórmulas):

Imaginemos las calificaciones (del 1 al 5) de 10 estudiantes en un examen:
2, 3, 3, 4, 4, 4, 5, 5, 1, 4

**Paso 1: Organizar los datos y calcular frecuencias absolutas ($f_i$)**

| Calificación ($i$) | Conteo | Frecuencia Absoluta ($f_i$) |
| :----------------- | :----- | :-------------------------- |
| 1                  | 1      | 1                           |
| 2                  | 1      | 1                           |
| 3                  | 2      | 2                           |
| 4                  | 4      | 4                           |
| 5                  | 2      | 2                           |
| **Total** |        | **$N = 10$** |

**Paso 2: Calcular frecuencias relativas ($h_i$)**

| Calificación ($i$) | $f_i$ | $h_i = f_i / N$ |
| :----------------- | :---- | :-------------- |
| 1                  | 1     | $1/10 = 0.1$    |
| 2                  | 1     | $1/10 = 0.1$    |
| 3                  | 2     | $2/10 = 0.2$    |
| 4                  | 4     | $4/10 = 0.4$    |
| 5                  | 2     | $2/10 = 0.2$    |
| **Total** | **10** | **1.0** |

**Paso 3: Calcular frecuencias absolutas acumuladas ($F_i$)**

| Calificación ($i$) | $f_i$ | $F_i$ |
| :----------------- | :---- | :---- |
| 1                  | 1     | 1     |
| 2                  | 1     | $1+1=2$ |
| 3                  | 2     | $2+2=4$ |
| 4                  | 4     | $4+4=8$ |
| 5                  | 2     | $8+2=10$|
| **Total** | **10** | **$N=10$** |

**Paso 4: Calcular frecuencias relativas acumuladas ($H_i$)**

| Calificación ($i$) | $f_i$ | $h_i$ | $H_i$ |
| :----------------- | :---- | :---- | :---- |
| 1                  | 1     | 0.1   | 0.1   |
| 2                  | 1     | 0.1   | $0.1+0.1=0.2$ |
| 3                  | 2     | 0.2   | $0.2+0.2=0.4$ |
| 4                  | 4     | 0.4   | $0.4+0.4=0.8$ |
| 5                  | 2     | 0.2   | $0.8+0.2=1.0$ |
| **Total** | **10** | **1.0** | **1.0** |

### Para Memorizar:

Piensa en una **lista de asistencia a clase**:

* **Frecuencia Absoluta:** ¿Cuántos estudiantes vinieron hoy? (El número exacto).
* **Frecuencia Relativa:** ¿Qué porcentaje de la clase vino hoy? (La proporción respecto al total).
* **Frecuencia Absoluta Acumulada:** Al final de la semana, ¿cuántos días ha venido cada estudiante hasta el día de hoy? (La suma de asistencias hasta ese punto).
* **Frecuencia Relativa Acumulada:** ¿Qué porcentaje del total de días de clase ha asistido cada estudiante hasta ahora? (La proporción acumulada).

Esta tabla de frecuencias es el primer paso para visualizaciones de datos como histogramas o diagramas de barras, y es crucial para calcular medidas de tendencia central y dispersión que verás más adelante.


## 1.8. Tabulación de variables

## 1.8. Tabulación de Variables: Construyendo Tablas de Frecuencias

La **tabulación de variables** se refiere al proceso de organizar y presentar las frecuencias (absolutas, relativas, acumuladas) de una variable en una estructura clara y fácil de leer, que generalmente es una **tabla de frecuencias**.

### La Tabla de Frecuencias: Estructura Básica

Una tabla de frecuencias típicamente consta de `k` filas, donde cada fila corresponde a una **modalidad** (un valor único o una categoría) de la variable en estudio. Las columnas de la tabla contendrán la información sobre las diferentes frecuencias.

La forma más común y práctica de una tabla de frecuencias incluye al menos:

1.  **Columna de Valores / Categorías:** Donde se listan todas las modalidades posibles de la variable.
2.  **Columna de Frecuencias Absolutas ($f_i$):** El número de veces que aparece cada modalidad.
3.  **Columna de Frecuencias Relativas (%):** La proporción de cada modalidad, expresada como porcentaje.

**Ejemplo 5 (Reafirmando el uso de porcentajes):**
Es muy común ver que en lugar de una columna de frecuencia relativa (en tanto por uno, ej. 0.25), se use directamente una columna de **porcentajes**. Esto es porque son equivalentes ($0.25 = 25\%$), y los porcentajes suelen ser más intuitivos para el público general. La suma de los porcentajes debe ser 100%.

### Manejo de Valores Perdidos (Missing Values)

Es muy frecuente que en un conjunto de datos no todos los individuos tengan un valor registrado para cada variable. Cuando esto ocurre, decimos que el individuo presenta un **valor perdido** o **missing value** en esa variable.

* **¿Cómo se gestionan?** En las tablas de frecuencias, es habitual añadir una columna extra llamada "válidos" o "N válido". Esta columna (o un total en el pie de tabla) indica el número de observaciones que realmente tienen un valor para esa variable, excluyendo los perdidos.
* **"No Aplicable" como un caso especial:** A veces, un valor "no aplicable" (N/A o NA) o "no procede" se considera un tipo de valor perdido. Esto ocurre cuando una pregunta no tiene sentido para ciertos individuos, dependiendo de sus respuestas a preguntas anteriores.
    * **Ejemplo 6:** Si preguntas "¿Tiene hijos?" (Sí/No) y luego "¿Cuántos hijos tiene?", para las personas que respondieron "No" a la primera pregunta, la segunda pregunta sería "no aplicable". Estos "no aplicables" deben ser identificados y tratados adecuadamente para no distorsionar las frecuencias de las personas que sí tienen hijos.

### Tablas de Frecuencias para Variables Continuas (Agrupamiento por Intervalos)

Cuando trabajamos con **variables continuas** (como edad, altura, peso, tiempo), que pueden tomar un número infinito de valores, no es práctico listar cada valor único y su frecuencia. En su lugar, agrupamos los datos en **intervalos de clase** (o rangos).

Cuando se agrupan en intervalos, aparecen dos conceptos importantes:

1.  **Límite Inferior y Superior del Intervalo:**
    * Cada intervalo tiene un límite inferior (el valor más bajo que puede tomar un dato en ese intervalo) y un límite superior (el valor más alto).
    * **Importante:** La forma en que se definen los límites es crucial para evitar ambigüedad. Por ejemplo, un intervalo puede ser $]10, 20]$ (valores mayores de 10 hasta 20 inclusive) o $[10, 20[$ (valores desde 10 inclusive hasta menos de 20). La notación y la convención deben ser claras.

2.  **Marca de Clase del Intervalo:**
    * La **marca de clase** es el **valor que representa al intervalo**. Generalmente, es el **punto medio** del intervalo.
    * **Cálculo:** Se calcula sumando el límite inferior y el límite superior del intervalo y dividiendo el resultado por 2.
        $$\text{Marca de Clase} = \frac{\text{Límite Inferior} + \text{Límite Superior}}{2}$$
    * **Utilidad:** Esta marca de clase es muy importante porque se utiliza como un valor promedio o representante del intervalo en cálculos posteriores, especialmente para medidas resumen como la media aritmética, que veremos en el siguiente tema.

### Ejemplo de Tabla con Intervalos:

| Intervalo de Edad | Frecuencia Absoluta | Marca de Clase |
| :---------------- | :------------------ | :------------- |
| $[0, 10[$         | 5                   | $(0+10)/2 = 5$ |
| $[10, 20[$        | 8                   | $(10+20)/2 = 15$ |
| $[20, 30]$        | 12                  | $(20+30)/2 = 25$ |
| **Total** | **25** | |

---

### Para Memorizar:

Piensa en la **tabulación** como la creación de un **reporte organizado** de tus datos.

* **Tabla de Frecuencias:** Es el formato estándar de ese reporte, mostrando cuántas veces ocurre cada cosa (frecuencias).
* **Porcentajes:** Son la forma más amigable de presentar las frecuencias relativas.
* **Valores Perdidos:** Son datos "ausentes". Es vital contarlos aparte y tener una columna de "válidos" para no engañarse con los totales.
* **Intervalos (para Continuas):** Si tus datos son muy variados (ej. edades exactas), los agrupas en "cajas" (intervalos).
    * **Marca de Clase:** Es la "etiqueta central" de cada caja, el valor que usas para representarla.


## 1.9. Gráficas básicas

Como dice el dicho, "más vale un buen gráfico que mil tablas de frecuencias". 

## 1.9. Gráficas Básicas: El Arte de la Visualización

El objetivo de las gráficas es comunicar información de manera rápida y efectiva. La elección del gráfico ideal depende principalmente del **tipo de variable** que queremos representar.

### Gráficos para Variables Categóricas (Cualitativas y Cuantitativas Discretas)

Para variables categóricas (nominales, ordinales) y cuantitativas discretas (donde cada valor discreto se puede tratar como una categoría), los gráficos más comunes son:

1.  **Diagramas de Barras:**
    * **Cuándo usarlo:** Es el gráfico más versátil para representar frecuencias o proporciones de diferentes categorías. Cada barra representa una categoría, y la altura de la barra indica su frecuencia (absoluta o relativa/porcentaje).
    * **Tipos:**
        * **Diagrama de Barras Simples:** El más común, con barras separadas para cada categoría.
        * **Diagrama de Barras Apiladas:** Un caso especial donde las barras se dividen en segmentos para mostrar la composición de una categoría más grande (útil para comparar subcategorías dentro de una categoría principal).
    * **Lo esencial:** **Todas las variables (excepto las continuas)** pueden ser representadas con diagramas de barras.

2.  **Gráficos de Sectores (Gráficos Circulares, "Pie Charts"):**
    * **Cuándo usarlo:** Exclusivo para variables cualitativas (categóricas). Muestra la proporción de cada categoría como una "rebanada" de un pastel. El área de cada sector es proporcional a su porcentaje sobre el total.
    * **Requisito clave:** Las porciones deben representar porcentajes y la suma total siempre debe ser **100%**.
    * **Mejor uso:** Es más efectivo cuando el **número de categorías no es excesivo** (idealmente 2 a 5 categorías). Si hay demasiadas categorías o las diferencias de porcentaje son muy pequeñas, puede ser difícil de leer y un diagrama de barras es una mejor opción.

3.  **Pictogramas:**
    * **Cuándo usarlo:** Para variables cualitativas. Utiliza **dibujos o íconos** relacionados con el tema para representar las frecuencias. Por ejemplo, si se representa el número de coches, se usarían íconos de coches.
    * **Ventaja:** Puede ser visualmente muy atractivo y efectivo para comunicar un mensaje específico, especialmente si los íconos son simbólicos y potentes.
    * **Error común a evitar:** **El tamaño de los dibujos debe ser proporcional al área, no a la altura o la longitud.** Si el valor es el doble, el *área* del dibujo debe ser el doble, no solo su altura. Esto es crucial para no distorsionar la percepción de las magnitudes.
    * **Limitación:** Su uso es limitado en software estadístico común y su correcta elaboración requiere atención al detalle del área para evitar engaños visuales.

### Gráficos para Variables Cuantitativas Continuas

Para variables cuantitativas continuas (como edad, peso, tiempo), que generalmente se agrupan en intervalos, usamos gráficos que comunican esa continuidad:

1.  **Histograma:**
    * **Cuándo usarlo:** Es el gráfico equivalente al diagrama de barras para variables cuantitativas continuas. Las barras se **dibujan juntas**, sin espacios entre ellas, para enfatizar la continuidad de la variable.
    * **Ejes:** El eje horizontal (abscisas) muestra los intervalos de la variable (a menudo representados por sus marcas de clase), y el eje vertical (ordenadas) muestra la frecuencia (absoluta o relativa) de los datos dentro de cada intervalo.
    * **Utilidad:** Nos permite visualizar la forma de la distribución de los datos (simétrica, asimétrica, unimodal, bimodal), la dispersión y la presencia de valores atípicos.

2.  **Polígono de Frecuencias:**
    * **Cuándo usarlo:** También se usa para variables cuantitativas continuas (y a veces discretas cuando hay muchos valores). Se construye uniendo los **puntos medios de la parte superior de las barras de un histograma** con líneas. Se puede mostrar con o sin el histograma subyacente.
    * **Ventaja:** Es útil para comparar la forma de dos o más distribuciones en el mismo gráfico, ya que las líneas son menos "masivas" que las barras.

### Complementando el Tema: La Importancia de la Elección Correcta

Elegir el gráfico adecuado es una habilidad crítica para un analista de datos. Una buena visualización puede revelar patrones ocultos, mientras que una elección incorrecta puede ocultar información o incluso engañar.

* **Siempre ten en cuenta la audiencia:** ¿Quién va a ver este gráfico? ¿Es para un público general o para expertos? Esto puede influir en la complejidad y el tipo de gráfico.
* **Claridad y Simplicidad:** Un buen gráfico es claro, conciso y no está sobrecargado de información innecesaria.
* **Etiquetas y Títulos:** Siempre incluye títulos claros para el gráfico y los ejes, así como etiquetas para las categorías o valores, para que el gráfico sea autoexplicativo.

---

### Para Memorizar:

Piensa en los gráficos como **herramientas visuales** en tu caja de herramientas de análisis de datos.

* **¿Categóricas (cualitativas y discretas)?**
    * **Barras:** Tu herramienta más versátil, casi siempre funciona. Como un martillo multiusos.
    * **Sectores (Pastel):** Solo si tienes POCAS categorías y quieres mostrar PARTES de un TODO (como cortar un pastel).
    * **Pictogramas:** Para un efecto visual impactante, pero ¡cuidado con las ÁREAS de los dibujos!

* **¿Cuantitativas (continuas)?**
    * **Histograma:** Para ver la FORMA de la distribución de tus datos continuos. Las barras van JUNTAS.
    * **Polígono de Frecuencias:** Para suavizar el histograma o comparar varias distribuciones.



## 1.10. El Arte de Elegir el Gráfico Adecuado

Seleccionar el gráfico correcto no es solo una cuestión de estética, sino de **eficacia en la comunicación**. Un buen gráfico es aquel que permite a tu audiencia entender rápidamente la historia que tus datos quieren contar, sin distorsiones ni confusiones.

El "arte" de elegir el gráfico adecuado se reduce principalmente a entender dos cosas:

1.  **¿Qué tipo de variable(s) estás intentando representar?** (Este es el factor más importante).
2.  **¿Qué mensaje o relación quieres comunicar?** (Comparación, distribución, composición, tendencia, relación entre variables).

Aunque el texto menciona una tabla que aún no me has proporcionado, puedo inferir y complementar la lógica detrás de esa tabla basándome en lo que ya hemos discutido sobre los tipos de variables y los gráficos básicos.

### La Pista Esencial: El Tipo de Variable

Como ya hemos visto, el tipo de variable es la guía fundamental. Reafirmemos:

* **Variables Categóricas (Nominales u Ordinales):** Representan cualidades o categorías.
    * **Gráficos principales:** Diagramas de barras, gráficos de sectores (pie charts), pictogramas.
    * **¿Cuál elegir?**
        * **Diagrama de Barras:** Siempre una opción segura y flexible. Útil para comparar frecuencias entre categorías o para mostrar categorías con muchas opciones. Es el "caballito de batalla".
        * **Gráfico de Sectores:** Bueno para mostrar **partes de un todo** (composición) cuando tienes **pocas categorías** (idealmente 2-5) y cada una representa una porción significativa del 100%. Se vuelve ineficaz con muchas categorías o porcentajes muy similares.
        * **Pictogramas:** Para un impacto visual fuerte o para audiencias menos técnicas, pero con la precaución de la proporcionalidad del área.

* **Variables Cuantitativas Discretas:** Aunque son numéricas, si tienen un número limitado de valores o si cada valor se trata como una categoría, a menudo se pueden manejar como categóricas.
    * **Gráficos principales:** Diagramas de barras.
    * **¿Por qué no siempre un histograma?** Un histograma es más para rangos continuos. Si tienes "número de hijos" (0, 1, 2, 3), un diagrama de barras muestra claramente la frecuencia de cada número específico.

* **Variables Cuantitativas Continuas:** Representan magnitudes que pueden tomar cualquier valor dentro de un rango.
    * **Gráficos principales:** Histogramas, polígonos de frecuencias.
    * **¿Cuál elegir?**
        * **Histograma:** Es el estándar de oro para visualizar la **distribución** de una sola variable continua. Te muestra la forma de los datos, dónde se concentran, si son simétricos o asimétricos, y si hay valores atípicos.
        * **Polígono de Frecuencias:** Útil para comparar la distribución de dos o más conjuntos de datos continuos en el mismo gráfico, o para una visualización más "suave" de la distribución.

### Más Allá del Tipo de Variable: El Mensaje que Quieres Transmitir

Aunque el tipo de variable es la guía principal, también debes considerar qué quieres comunicar:

* **Comparación:**
    * ¿Comparar categorías? $\rightarrow$ Barras.
    * ¿Comparar distribuciones? $\rightarrow$ Histogramas o polígonos de frecuencias superpuestos.
* **Composición (Partes de un todo):**
    * ¿Cómo se distribuye un total en sus partes? $\rightarrow$ Gráfico de sectores (si pocas categorías), Barras apiladas (si muchas categorías o quieres comparar subtotales).
* **Distribución (Forma de los datos):**
    * ¿Cómo se distribuyen los valores de una variable? $\rightarrow$ Histograma (cuantitativas), Diagrama de barras (categóricas/discretas).
* **Relación entre dos variables:** (Esto lo verás en unidades futuras)
    * ¿Una variable cuantitativa vs. otra cuantitativa? $\rightarrow$ Diagrama de dispersión (scatter plot).
    * ¿Una cuantitativa vs. una categórica? $\rightarrow$ Diagramas de caja (box plots) o de violín.
* **Tendencia a lo largo del tiempo:**
    * ¿Cómo cambia algo con el tiempo? $\rightarrow$ Gráfico de líneas (útil para series temporales).

### El Proceso de Elección (La "Tabla Mental" que se Interioriza)

Cuando te enfrentes a un conjunto de datos, tu proceso mental debería ser algo así:

1.  **Identifica la variable(s) principal(es) que quieres visualizar.**
2.  **Determina el tipo de cada variable:** ¿Categórica (nominal/ordinal)? ¿Cuantitativa (discreta/continua)?
3.  **Piensa en el mensaje clave:** ¿Quieres mostrar frecuencias? ¿Proporciones? ¿La forma de la distribución? ¿Una relación?
4.  **Consulta la "tabla mental":** Basándote en el tipo de variable y el mensaje, ¿qué gráficos son las opciones más fuertes?
5.  **Evalúa las alternativas:** ¿Un gráfico de barras o de sectores? ¿Un histograma o un polígono? Considera la claridad, la simplicidad y el impacto para tu audiencia.
6.  **Crea el gráfico y revísalo:** ¿Es claro? ¿Está bien etiquetado? ¿Comunica el mensaje deseado sin ambigüedad?

---

### Para Memorizar:

Piensa en elegir un gráfico como elegir la **ropa adecuada para una ocasión**:

* **La Variable:** Es el "clima" o el "código de vestimenta" (formal, informal, etc.). Te da la primera pista sobre qué tipo de ropa (gráfico) es apropiada.
    * **Categóricas/Discretas:** Ropa casual (barras, pastel, pictogramas).
    * **Continuas:** Ropa específica para el clima (histogramas, polígonos).
* **El Mensaje:** Es el "propósito" de la ocasión (una fiesta, una reunión de negocios, un evento deportivo). Esto afina tu elección dentro de las opciones apropiadas.
    * ¿Mostrar partes de un todo? $\rightarrow$ Traje de gala (gráfico de pastel).
    * ¿Comparar elementos? $\rightarrow$ Ropa a juego (barras).
    * ¿Mostrar la forma de tu cuerpo (distribución)? $\rightarrow$ Ropa ajustada (histograma).

La práctica constante te ayudará a **interiorizar** esta tabla y tomar decisiones de visualización de forma intuitiva.


## 1.11. Retos de la estadística en el Big Data

¡Fantástico! Hemos llegado a la última unidad del Tema 1, que es un excelente cierre porque te introduce a los desafíos y la relevancia actual de la estadística en el mundo del **Big Data**, un concepto central en tu maestría.

---

## 1.11. Retos de la Estadística en el Big Data

La estadística, aunque es una disciplina clásica con raíces en el siglo XIX, se ha visto profundamente transformada por la era de los computadores, Internet y, especialmente, el **Big Data**. El cambio más drástico es la **cantidad masiva de información** disponible. Antes, los conjuntos de datos eran relativamente pequeños; ahora, son tan grandes que a menudo no sabemos cómo analizarlos, lo que lleva a la paradoja de tener muchos datos pero ser incapaces de aprender de ellos.

Para que la estadística siga siendo relevante y útil en este nuevo entorno, debe adaptarse y enfrentar varios desafíos clave:

### 1. Excesiva Cantidad de Información y Datos (Volumen)

* **El Problema:** Los métodos estadísticos clásicos no fueron diseñados para manejar volúmenes de datos tan grandes de manera eficiente. Aplicarlos directamente puede llevar a tiempos de cómputo inviables.
* **La Solución:**
    * **Creación de códigos eficientes:** Necesitamos algoritmos y software optimizados que permitan aplicar los métodos estadísticos existentes a grandes volúmenes de datos de forma rápida.
    * **Desarrollo de nuevos métodos estadísticos:** Investigar y crear técnicas estadísticas novedosas que sean inherentemente escalables y capaces de trabajar con cantidades masivas de información desde su concepción.
* **El Reto de los Outliers (Valores Atípicos):** En el Big Data, los "outliers" (datos muy diferentes al resto) ya no son casos aislados que se pueden eliminar fácilmente. Pueden ser miles o millones de observaciones que, aunque atípicas, contienen información valiosa. Eliminarlos o ignorarlos masivamente ya no es una solución adecuada, y la estadística debe desarrollar métodos más robustos para tratarlos sin descartar información relevante.

### 2. Complejidad de los Datos (Variedad y Veracidad)

* **El Problema:** El Big Data no solo es "muchos datos", sino también datos **extremadamente complejos y heterogéneos**. Gran parte de estos datos provienen de la "huella digital" de los usuarios de Internet (clics, interacciones en redes sociales, transacciones, sensores, etc.). Esta diversidad de formatos y fuentes hace que sean difíciles de interpretar directamente.
* **La Solución:** Los métodos de análisis estadístico necesitan incorporar procesos de **transformación de datos (preprocesamiento)** que permitan limpiar, estructurar y unificar esta información compleja para que pueda ser interpretada y analizada de forma efectiva. Esto incluye técnicas de normalización, estandarización, imputación de valores faltantes y más.

### 3. Necesidad de Infraestructuras Potentes de Análisis (Velocidad)

* **El Problema:** Analizar grandes y complejos volúmenes de datos requiere una capacidad de procesamiento computacional inmensa para obtener resultados en tiempos razonables.
* **La Solución:** Afortunadamente, los avances en la computación han proporcionado soluciones:
    * **Clusters de computadoras:** Redes de ordenadores que trabajan juntos para procesar tareas complejas.
    * **Computación en la nube:** Permite acceder a recursos computacionales masivos (servidores, almacenamiento) a través de Internet, sin necesidad de tener la infraestructura física.
    * **Métodos paralelizados:** Es crucial que los algoritmos estadísticos sean diseñados para ser "paralelizables", es decir, que puedan dividirse en subtareas que se ejecuten simultáneamente en múltiples procesadores. Esto maximiza el uso de la infraestructura y acelera significativamente la obtención de resultados.

### 4. Políticas de Privacidad (Valor)

* **El Problema:** Gran parte de la "huella digital" y otros datos masivos son **privativos**; pertenecen a empresas o están protegidos por leyes de privacidad (como GDPR o CCPA). No están libremente disponibles para cualquier estudio.
* **La Solución:** Para acceder a esta información, es necesario:
    * **Pedir autorización a los usuarios.**
    * **Solicitar o comprar los datos a las empresas** que los han recolectado.
    * A veces, es necesario **cruzar datos de varias empresas**, lo que añade capas de complejidad en la gestión y los acuerdos.
* **Implicación:** Esto significa que, aunque hay una "alta cantidad de información disponible", el acceso a ella no es universal y a menudo tiene un costo o requiere acuerdos complejos. Las empresas ven estos datos como un activo valioso y buscan monetizarlos.

### 5. Recogida de Datos sin Previa Especificación del Problema (Cambio de Paradigma)

* **El Problema:** La estadística clásica sigue un flujo: **primero se diseña el estudio/problema, luego se recogen los datos** (a menudo a través de encuestas u experimentos controlados). Sin embargo, en el entorno Big Data, es común que los datos **ya existan** (generados pasivamente por usuarios, sensores, etc.) *antes* de que se formule una pregunta específica. Esto significa que a menudo se tiene una montaña de datos y luego se busca qué preguntas se pueden responder con ellos.
* **La Implicación:** Esto invierte el flujo tradicional del proceso estadístico. El analista de datos a menudo comienza con un vasto conjunto de datos y debe explorarlos para descubrir posibles preguntas y problemas, en lugar de diseñar una recolección de datos específica para un problema predefinido. Esto requiere habilidades de exploración de datos y descubrimiento de patrones.

---

### Para Memorizar:

Piensa en el **Big Data como un océano inmenso y turbulento** y la **estadística como un barco**.

* **Volumen (Excesiva Cantidad):** El océano es tan grande que tu barco tradicional (métodos clásicos) se ahoga. Necesitas un superpetrolero (código eficiente, nuevos métodos).
* **Variedad/Complejidad:** El océano no es solo agua; tiene algas, rocas, peces de todo tipo (datos heterogéneos). Necesitas herramientas para limpiarlo y entenderlo (preprocesamiento).
* **Velocidad (Infraestructura):** Navegar este océano requiere motores potentes (clusters, nube, paralelización). No puedes ir a remo.
* **Veracidad/Valor (Privacidad):** No todo el océano es de libre acceso. Hay zonas privadas (datos protegidos) a las que necesitas permiso o pago para entrar.
* **Cambio de Paradigma (Recogida sin problema previo):** Antes, tú decidías a qué parte del océano ir para pescar un tipo específico de pez (diseño y recolección). Ahora, ya tienes una red enorme llena de peces de todo tipo, y tienes que ver qué puedes hacer con lo que ya capturaste.


# TEMA 2: Estadística computacional

## 2.2. Principios básicos
¡Hola! ¡Claro que sí, con gusto te ayudaré a comprender y memorizar estos temas para tu maestría en análisis de datos! La **Estadística Computacional** es un campo fascinante y fundamental hoy en día.

---

## ¿Qué es la Estadística Computacional?

Imagina que tienes una cantidad gigantesca de datos, como la información de millones de transacciones bancarias o los patrones de comportamiento de miles de usuarios en una red social. Analizar esto con los métodos tradicionales sería casi imposible, o tomaría una eternidad. Aquí es donde entra la **Estadística Computacional**.

En pocas palabras, la **Estadística Computacional** es como el "cerebro" y los "músculos" de la estadística moderna. Es la ciencia que **combina los principios de la estadística y las matemáticas con el poder de las computadoras y la programación** para resolver problemas complejos que involucran una gran cantidad de datos.

Piensa en ella como una herramienta que te permite:
* **Recopilar y organizar** datos masivos.
* **Visualizarlos** de formas que te permitan entenderlos.
* **Analizarlos** usando algoritmos y programas informáticos.
* **Encontrar patrones y soluciones** que serían imposibles de detectar manualmente.

---

### ¿Por qué es tan importante la Estadística Computacional?

Hoy en día, el mundo está lleno de **Big Data** (grandes volúmenes de datos). Desde tu teléfono hasta los sensores de un coche autónomo, todo genera datos constantemente. Estos datos son enormes, complejos y a menudo tienen interacciones "ocultas" que no son evidentes a simple vista.

La estadística computacional es crucial porque nos permite **entender y extraer valor de estos sistemas de Big Data** en un entorno que es inherentemente complejo. Sin ella, nos ahogaríamos en la información.

---

### Los 3 Pilares Fundamentales de la Estadística Computacional

Para dominar la estadística computacional, te basarás en tres áreas técnicas clave:

1.  **Programación:** Aquí es donde pones tus ideas en acción. Aprenderás a escribir código, muy probablemente en el lenguaje **R** (que es muy popular para la estadística), para implementar las fórmulas y algoritmos estadísticos. No se trata de ser un experto programador al principio, sino de entender cómo usar el código para resolver problemas y, muy importante, cómo **reutilizar código** que ya funciona para adaptar soluciones a nuevas necesidades. Piensa en ello como tener un "kit de herramientas" de código que puedes modificar.

2.  **Análisis Numérico:** Muchos problemas complejos, especialmente con Big Data, no tienen una solución matemática "exacta" y simple. O si la tienen, calcularla tomaría demasiado tiempo. El **análisis numérico** se encarga de encontrar **soluciones aproximadas pero muy precisas** utilizando métodos computacionales. Por ejemplo, en lugar de resolver una ecuación enorme de forma manual, una computadora puede usar técnicas numéricas para llegar a una respuesta muy cercana en segundos. Esto es vital en campos como la biología, medicina o ciberseguridad, donde los datos son masivos y las interacciones complejas.

3.  **Estadística Clásica:** Aunque uses computadoras, la base sigue siendo la estadística de toda la vida. Esto incluye:
    * **Estadística Descriptiva:** Cómo resumir y visualizar datos univariados (de una sola variable), como la media, la mediana o la desviación estándar.
    * **Estadística Inferencial:** Cómo sacar conclusiones sobre una población más grande basándote en una muestra de datos.
    * **Combinatoria y Teoría de Probabilidades:** Conceptos básicos que son el fundamento para entender cómo se comportan los datos.
    * **Modelos de Regresión:** Especialmente importantes para el análisis de sistemas con muchas variables. La **regresión lineal** (que busca relaciones directas entre variables) y los **modelos de regresión no paramétricos** (más flexibles y útiles para Big Data) son herramientas poderosas para entender dependencias y hacer predicciones.

---

### ¿Cómo se resuelven los problemas con Estadística Computacional?

Cuando te enfrentas a un problema de análisis de datos con estadística computacional, típicamente sigues estos tres pasos:

1.  **Modelado del Problema:** Primero, entiendes el problema en sí y lo traduces a un "modelo" o una representación que la computadora pueda entender. Esto implica definir qué datos necesitas, qué quieres lograr y qué tipo de análisis es el más adecuado.
2.  **Propuesta de Solución Numérica:** Luego, diseñas la estrategia para resolver ese modelo utilizando métodos numéricos. Piensas en los algoritmos y las técnicas computacionales que te permitirán llegar a una solución aproximada.
3.  **Implementación Numérica:** Finalmente, escribes el código (por ejemplo, en R) para llevar a cabo esa solución. Aquí es donde la computadora hace el trabajo pesado de calcular, procesar y visualizar los datos.

---

### En Resumen: La Fórmula del Éxito

La **estadística computacional** es la poderosa **unión de las matemáticas, la estadística y la programación** para **resolver problemas complejos y de alta dimensionalidad** (muchos datos y mucha complejidad) en el mundo real, especialmente en la era del Big Data. Te permite transformar grandes volúmenes de datos en información útil y actionable.

## 2.3. Ámbitos de aplicación

### ¿Qué es la Estadística Computacional en pocas palabras?

La **estadística computacional** es justo eso: el uso de computadoras, algoritmos y programación para resolver problemas estadísticos que serían imposibles o muy lentos de hacer a mano. No es solo usar una calculadora avanzada, sino que permite abordar retos nuevos y complejos que antes no tenían solución, ¡y esto a su vez genera nuevas preguntas e ideas!

---

### Ámbitos de Aplicación: ¿Dónde la vemos en acción?

Este texto destaca que la estadística computacional es un campo **nuevo, dinámico y en constante crecimiento**, con muchas oportunidades. Se está volviendo esencial en diversas áreas:

#### 1. Biología y Salud: Bioestadística Computacional y Medicina Personalizada

* **¿Qué es?** Es la unión de la estadística computacional con campos como la bioinformática (manejo de datos biológicos), la genómica (estudio de genes) y la biotecnología (uso de organismos vivos para tecnología). También se le conoce como **informática o ingeniería de datos para biomedicina**.
* **¿Por qué es importante aquí?** En estas áreas se generan **enormes volúmenes de datos** (Big Data) y de tipos muy variados (por ejemplo, secuencias genéticas, resultados de pruebas médicas, imágenes). Analizar estos datos requiere herramientas muy eficientes que la estadística computacional provee.
* **Ejemplo práctico:**
    * **Análisis del genoma humano:** Imagina que los científicos quieren predecir si una persona tiene mayor riesgo de desarrollar una enfermedad en el futuro basándose en su ADN. Necesitan comparar el genoma de esa persona con el de miles o millones de otras personas. La bioestadística computacional, junto con técnicas de **machine learning**, permite procesar y analizar esta gigantesca cantidad de información genética para encontrar patrones y hacer predicciones. Esto lleva a la **medicina personalizada**, donde los tratamientos se adaptan al perfil genético único de cada paciente.

---

#### 2. Ciberseguridad

* **¿Qué es?** La ciberseguridad se encarga de proteger los sistemas informáticos y los datos de ataques, daños o accesos no autorizados. Con la inmensa cantidad de datos que se mueven en internet (tráfico de datos), es crucial asegurar esta información.
* **¿Por qué es importante aquí?** Para proteger los datos, es necesario **monitorear, registrar y codificar** las transferencias de información en la red. Estos procesos generan **series de números que cambian con el tiempo** (piensa en el volumen de datos que entra y sale de una red en cada segundo). La estadística computacional es vital para:
    * **Detectar anomalías:** Identificar patrones inusuales en el tráfico de datos que podrían indicar un ciberataque, malware (software malicioso) o intentos de desestabilización (como noticias falsas o ataques de denegación de servicio).
    * **Desarrollar métodos de seguridad:** Crear algoritmos y sistemas que cifren la información, autentiquen usuarios y detecten amenazas de forma proactiva.
* **Ejemplo práctico:**
    * Imagina un banco que necesita proteger las transacciones de sus clientes. Un estadístico computacional podría desarrollar un modelo que analice el volumen, la frecuencia y el tipo de transacciones en tiempo real. Si de repente hay un pico inusual en transacciones pequeñas y rápidas desde una ubicación geográfica extraña, el sistema, usando estadística computacional, podría alertar de un posible fraude o ataque, bloqueando las transacciones sospechosas y protegiendo a los clientes. Esto implica analizar **series temporales de datos** (cómo los datos evolucionan a lo largo del tiempo) para encontrar patrones de comportamiento normal y desviaciones.

---

### Puntos Clave para Recordar y Aplicar

* **Estadística Computacional (EC) = Estadística + Computación.** Permite resolver problemas complejos y afrontar desafíos con **grandes volúmenes de datos (Big Data)** que antes eran imposibles.
* Es un **campo en crecimiento** con muchas oportunidades y un impacto significativo.
* **Áreas principales mencionadas:**
    * **Bioestadística Computacional / Medicina Personalizada:** Crucial para analizar datos biológicos y genéticos masivos, llevando a avances en salud y tratamientos personalizados (ej. predicción de enfermedades por genoma).
    * **Ciberseguridad:** Indispensable para monitorear y proteger el tráfico de datos, detectando anomalías y desarrollando métodos de seguridad basados en el análisis de datos que cambian con el tiempo (series temporales).
* La EC no solo resuelve problemas existentes, sino que también **genera nuevos desafíos y oportunidades de desarrollo**.

---


## 2.4. Técnicas básicas de programación

### Técnicas Básicas de Programación: Escribiendo Código Inteligente

Cuando hablamos de "buenas prácticas de programación", nos referimos a un conjunto de reglas y consejos que te ayudan a escribir código que no solo funciona, sino que es **fácil de entender, mantener y reutilizar**. Esto es crucial, especialmente cuando estás desarrollando soluciones estadísticas, ya que tus modelos y análisis pueden volverse complejos rápidamente.

El texto menciona tres pilares fundamentales que son aplicables a cualquier lenguaje de programación (como R, Python, etc.):

-----

### 1\. Prestar Atención a la Sintaxis: La "Expresividad" del Lenguaje

  * **¿Qué significa?** Se refiere a hacer que tu código sea lo más **claro y legible** posible. Es como escribir un libro: no solo quieres que las oraciones tengan sentido gramatical, sino que también fluyan y sean fáciles de seguir para el lector. En programación, esto se logra usando nombres de variables, funciones y comentarios que reflejen lo que realmente hacen.
  * **¿Por qué es importante?** Un código expresivo puede ser entendido por ti (meses después de haberlo escrito) y por otros colegas, incluso si no son expertos en programación. Esto reduce errores y acelera el desarrollo.
  * **Ejemplo práctico:**
      * **Mal ejemplo:** `x = c(85, 92, 78, 65, 90)`
          * ¿Qué significa `x`? ¿Son edades, pesos, o algo más? No es claro.
      * **Buen ejemplo:** `notas_alumnos = c(85, 92, 78, 65, 90)`
          * Aquí, **`notas_alumnos`** deja muy claro qué tipo de datos contiene la variable.
      * **Consejo adicional:** Para códigos más largos, al inicio del archivo, es una excelente práctica incluir un "encabezado" o "cuadro de texto" con comentarios que describan brevemente:
          * Qué hace el código.
          * Quién lo escribió y la fecha.
          * Una lista de las variables principales y su significado.

-----

### 2\. Seccionar el Programa: Facilitando la Validación

  * **¿Qué significa?** Consiste en dividir tu código en **partes lógicas más pequeñas**. En lugar de tener un bloque gigante de código que hace muchas cosas a la vez, lo separas en secciones que realizan tareas específicas.
  * **¿Por qué es importante?** Facilita la **validación y depuración (debugging)** del código. Si solo una parte falla, sabes exactamente dónde buscar el error. Si todo está junto, encontrar el problema es como buscar una aguja en un pajar.
  * **Ejemplo práctico:**
      * Imagina que estás desarrollando un análisis estadístico que involucra los siguientes pasos:

        1.  **Cargar los datos.**
        2.  **Limpiar y preprocesar los datos** (manejar valores faltantes, transformar variables).
        3.  **Calcular estadísticas descriptivas** (media, mediana, desviación estándar).
        4.  **Realizar un análisis de regresión.**
        5.  **Generar gráficos y reportes.**

      * En lugar de escribir todo en un solo bloque, deberías usar comentarios (o incluso funciones, como veremos en el siguiente punto) para indicar claramente cada sección:

        ```r
        # --- 1. Carga de Datos ---
        datos_ventas <- read.csv("ventas.csv")

        # --- 2. Preprocesamiento de Datos ---
        datos_ventas_limpios <- na.omit(datos_ventas) # Eliminar filas con NA
        datos_ventas_limpios$mes <- factor(datos_ventas_limpios$mes) # Convertir a factor

        # --- 3. Cálculo de Estadísticas Descriptivas ---
        media_ventas <- mean(datos_ventas_limpios$monto)
        mediana_ventas <- median(datos_ventas_limpios$monto)

        # ... y así sucesivamente para las otras secciones
        ```

      * Si el cálculo de la media está mal, sabes que debes revisar la sección "Cálculo de Estadísticas Descriptivas".

-----

### 3\. Facilitar la "Modularidad": Reutilización de Código

  * **¿Qué significa?** Es un paso más allá de seccionar el programa. La modularidad implica diseñar tu código de forma que las partes (los "módulos") sean **independientes y puedan ser usadas en diferentes contextos o programas**. La forma más común de lograr modularidad en programación es a través de **funciones**.
  * **¿Por qué es importante?**
      * **Validación:** Al igual que seccionar, facilita encontrar errores porque cada función se puede probar por separado.
      * **Reutilización:** Una vez que has escrito y probado una función para una tarea específica, puedes usarla una y otra vez en diferentes proyectos sin tener que reescribirla. Esto ahorra tiempo y reduce la probabilidad de introducir nuevos errores.
      * **Mantenimiento:** Si necesitas cambiar cómo se realiza una tarea, solo tienes que modificar la función una vez, y el cambio se reflejará en todos los lugares donde se use esa función.
  * **Ejemplo práctico:**
      * Imagina que frecuentemente necesitas calcular el **coeficiente de variación** (una medida de dispersión relativa) para diferentes conjuntos de datos. En lugar de escribir la fórmula cada vez, puedes crear una función:

        ```r
        # Función para calcular el coeficiente de variación
        calcular_coef_variacion <- function(datos) {
          media <- mean(datos, na.rm = TRUE)
          desviacion_estandar <- sd(datos, na.rm = TRUE)
          if (media == 0) {
            return(NA) # Evitar división por cero
          }
          coef_variacion <- (desviacion_estandar / media) * 100
          return(coef_variacion)
        }

        # Ahora puedes usar esta función en cualquier parte de tu código o en otros proyectos:
        ventas_mes1 <- c(100, 120, 110, 130, 90)
        ventas_mes2 <- c(50, 55, 60, 45, 52)

        cv_mes1 <- calcular_coef_variacion(ventas_mes1)
        cv_mes2 <- calcular_coef_variacion(ventas_mes2)

        print(paste("Coeficiente de Variación Mes 1:", cv_mes1, "%"))
        print(paste("Coeficiente de Variación Mes 2:", cv_mes2, "%"))
        ```

      * Si en el futuro quieres cambiar cómo se calcula el coeficiente de variación (quizás quieras añadir un manejo diferente para valores atípicos), solo tienes que modificar la función `calcular_coef_variacion` y todos los lugares donde la uses se actualizarán automáticamente.

-----

### Puntos Clave para Recordar y Aplicar

  * **Buenas Prácticas = Código más claro, mantenible y eficiente.**
  * **Expresividad:** Usa nombres descriptivos para variables y elementos del código (`notas_alumnos` en lugar de `x`). Incluye comentarios claros, especialmente al inicio del código, para explicar su propósito y las variables clave.
  * **Seccionar el Programa:** Divide tu código en bloques lógicos y etiquétalos (usando comentarios) para facilitar la revisión y encontrar errores rápidamente.
  * **Modularidad:** Crea **funciones** para tareas específicas. Esto permite **reutilizar** código, simplifica la validación y mejora el mantenimiento del software.
  * Estas prácticas son la base para empezar a programar de manera efectiva, especialmente en el desarrollo de software estadístico.

-----

## 2.5. Presentación del software «R»


# Tema 3. Medidas que resumen la información

##  3.2. Medidas de tendencia central
## 3.3. Medidas de tendencia central robustas
## 3.4. Medidas de dispersión
## 3.5. Medidas de dispersión robustas
## 3.6. Medidas de posición y forma
## 3.7 Gráficos de caja
## 3.8 Datos atípicos y análisis exploratorio de datos

