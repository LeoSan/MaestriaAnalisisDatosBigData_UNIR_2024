## TEMA 1: Introducción a la estadística

## 1.2. ¿Qué es la estadística?

## ¿Qué es la Estadística?

A menudo, la palabra "estadística" se usa de forma casual para referirse a cualquier colección de datos, como las "estadísticas de desempleo". Sin embargo, para tu maestría, es crucial que entiendas la estadística como una **ciencia**.

En pocas palabras, la **estadística es la ciencia que nos permite aprender de los datos**. Su objetivo principal es **obtener una comprensión profunda de un fenómeno** a partir de la información que tenemos.

### El Proceso Estadístico: Un Viaje por los Datos

Imagina la estadística como un viaje que los datos hacen, desde su origen hasta las conclusiones. Este viaje generalmente tiene las siguientes **fases**:

1.  **Diseño del estudio:** Aquí se planifica cómo se van a obtener los datos. Es como trazar el mapa antes de un viaje.
2.  **Recogida de datos:** Se obtienen los datos de acuerdo con el diseño. Es la parte de recolectar la información.
3.  **Análisis de datos:** Se examinan los datos para encontrar patrones, tendencias y relaciones. Es como interpretar lo que el mapa y los elementos recolectados nos dicen.
4.  **Organización, resumen y presentación:** Se prepara la información para que sea clara y comprensible, mostrando los hallazgos. Es como organizar tu mochila y presentar tus recuerdos del viaje.
5.  **Extracción de conclusiones:** Se interpretan los resultados para tomar decisiones o entender mejor el fenómeno. Es el objetivo final del viaje.

**Importante:** Aunque todas las fases son cruciales, la **recogida de datos es fundamental**. Si los datos son de mala calidad, no importa qué tan bueno sea tu análisis, las conclusiones no serán fiables. Como dice el texto, "no hay buen análisis posible si los datos han sido recogidos de cualquier manera".

### ¿Todos hacen estadística de la misma forma?

No necesariamente. Como el ejemplo de la empresa, diferentes personas pueden trabajar en distintas fases del proceso estadístico. Alguien podría diseñar el experimento, otro recoger los datos, uno más analizarlos y otro presentarlos. ¡Todos están haciendo estadística a su manera! Incluso si solo recibes datos ya recopilados para analizarlos, estás haciendo estadística.

### Tipos de Estadística: Descriptiva e Inferencial

La estadística se divide en dos ramas principales:

* **Estadística Descriptiva:** Esta rama se centra en **organizar, resumir y presentar los datos** de manera que sean comprensibles. Piensa en tablas, gráficos y medidas que te ayudan a describir lo que ya tienes. Los primeros temas de tu curso probablemente se enfocarán aquí.

* **Estadística Inferencial:** Esta rama va un paso más allá. Utiliza la probabilidad y técnicas matemáticas para **sacar conclusiones sobre una población más grande a partir de una muestra de datos**. Es decir, te permite hacer predicciones o generalizaciones. Verás esto más adelante en tu curso.

---

### Para Memorizar:

Piensa en la estadística como un **detective de datos**. Su trabajo es:

1.  **Planear** cómo encontrar pistas (diseño del estudio).
2.  **Recopilar** esas pistas (recogida de datos).
3.  **Analizar** las pistas para encontrar patrones (análisis de datos).
4.  **Organizar y presentar** el caso (organización, resumen y presentación).
5.  **Sacar conclusiones** sobre lo que realmente sucedió (conclusiones).

Y recuerda, la calidad de las **pistas** (datos) es lo más importante.

---

## 1.3. Población, muestra y muestreo

## Población, Muestra y Muestreo: Entendiendo de Dónde Vienen los Datos

Cuando hablamos de **estadística**, no solo nos referimos a números, sino a **datos** que nos dan información sobre algo. Ese "algo" es lo que nos interesa estudiar.

### El Individuo y la Población

Imagina que quieres saber la altura promedio de todos los estudiantes de tu universidad.

* Cada **estudiante** es un **individuo**.
* El **colectivo** de **todos los estudiantes** de la universidad es la **población**.

La estadística se enfoca en estudiar fenómenos que son **colectivos** (es decir, que involucran a una población) y que tienen algún grado de **incertidumbre**. No estudia fenómenos deterministas (como las leyes físicas), sino aquellos donde no podemos predecir el resultado exacto de cada individuo.

### La Muestra: Un Pequeño Trozo de la Realidad

A menudo, es imposible o demasiado costoso estudiar a cada individuo de una **población** completa. Piensa en el ejemplo del transporte en Madrid: no podían preguntar a todos los habitantes.

Aquí es donde entra la **muestra**:

* Una **muestra** es un **subconjunto de individuos** seleccionados de una población.
* El objetivo es que esta muestra sea **representativa** de la población. Esto significa que la muestra debe reflejar la diversidad y las características clave de la población original. Si tu población es 50% hombres y 50% mujeres, una muestra representativa debería tener una proporción similar.

### Muestreo: El Arte de Seleccionar una Muestra

El proceso de **seleccionar** a los individuos que formarán parte de tu muestra se llama **muestreo**. Este proceso es **crucial** porque la calidad de tus conclusiones dependerá directamente de cómo obtengas tus datos. Un buen muestreo asegura que tu muestra sea lo más representativa posible.

### El Inevitable Error de Muestreo e Inferencia Estadística

Siempre que trabajamos con una **muestra** en lugar de con la **población completa**, existe el riesgo de cometer un **error de muestreo**. Este error surge porque estamos tratando de inferir (o extrapolar) las características de toda la **población** a partir de solo un "pedazo" de ella (la **muestra**).

La clave en estadística es **reducir este error al mínimo**, ya que es inherente al proceso.

El proceso de **extrapolar las características y propiedades de la muestra a las de la población** se conoce como **inferencia estadística**. Debido a su importancia, la inferencia estadística es una rama fundamental de la estadística (junto con la estadística descriptiva que vimos antes). Es la parte de la estadística que te permite hacer generalizaciones y predicciones sobre la población basándote en tu muestra.

---

### Para Memorizar:

Imagina que quieres conocer el sabor de una sopa grande:

* La **sopa completa** es tu **población**.
* Una **cucharada** que tomas para probarla es tu **muestra**.
* El acto de **tomar la cucharada** es el **muestreo**.
* Si la cucharada no es un buen reflejo del sabor de toda la sopa (quizás solo tomaste la parte de arriba y el condimento se fue al fondo), entonces tienes un **error de muestreo**.
* Cuando dices "toda la sopa sabe así" basándote en tu cucharada, estás haciendo **inferencia estadística**.

La clave es que esa cucharada sea lo más parecida posible al resto de la sopa para que tu juicio sea acertado.

---

¿Hay algo más en esta sección que te gustaría que te explicara o quieres que pasemos al siguiente tema?


## 1.4. Tipos de variables estadísticas

## Tipos de Variables Estadísticas: Entendiendo la Naturaleza de tus Datos

En estadística, una **variable** es una característica o atributo que puede variar entre los individuos de una población o muestra. Entender los tipos de variables es crucial porque cada tipo requiere diferentes métodos de análisis.

Hay dos grandes categorías para clasificar las variables:

### 1. Variables Categóricas (Cualitativas)

Estas variables representan **cualidades o categorías** y no se miden numéricamente.

* **Nominales:** Son categorías que **no tienen un orden** inherente. Son simplemente etiquetas.
    * **Ejemplos:** Género (Masculino, Femenino), Grupo sanguíneo (A, B, AB, O), Tipo de auto (Sedán, SUV, Camioneta).
* **Ordinales:** Son categorías que **sí tienen un orden o jerarquía** natural, pero la distancia entre las categorías no es necesariamente uniforme o medible.
    * **Ejemplos:** Nivel educativo (Primaria, Secundaria, Universidad), Grado de satisfacción (Muy insatisfecho, Insatisfecho, Neutro, Satisfecho, Muy satisfecho), Curso escolar (1º ESO, 2º ESO, etc.).

### 2. Variables Cuantitativas (Numéricas)

Estas variables representan **cantidades** y se miden con números.

* **Discretas:** Toman un **número finito o contable de valores** (generalmente números enteros). No pueden tener valores intermedios.
    * **Ejemplos:** Número de hijos, Número de asignaturas suspendidas, Número de autos en una casa.
* **Continuas:** Pueden tomar **infinitos valores** dentro de un rango determinado. Se miden, no se cuentan. Suelen ser magnitudes físicas.
    * **Ejemplos:** Altura, Peso, Tiempo empleado en una tarea, Temperatura.

**Un dato importante:** Una variable que en teoría es continua (como la edad) puede ser **recodificada** como categórica ordinal si la agrupamos en intervalos (ej: Menor de 18, 18-25, Mayor de 25). Esto se hace a menudo en encuestas.

---

### Clasificación Según el Enfoque Metodológico

Además de la clasificación anterior, las variables también se pueden clasificar según el **rol que juegan en un estudio o modelo estadístico**:

* **Variables Dependientes (o de Respuesta / Explicada):** Son las variables cuyos valores **dependen o son influenciados** por otras variables en el estudio. Es el **resultado** que queremos explicar o predecir.
    * **Ejemplo:** El "aprobado en Lengua" (variable dependiente) podría depender del "número de horas de estudio" (variable independiente).
* **Variables Independientes (o Explicativas / Predictoras):** Son las variables que se cree que **influyen o causan un cambio** en la variable dependiente. Son los **factores** que usamos para explicar o predecir el resultado.
    * **Ejemplo:** El "número de horas de estudio" (variable independiente) se usa para explicar el "aprobado en Lengua" (variable dependiente).

    * **Sinónimos:** En diferentes disciplinas, puedes encontrar otros términos como **variables endógenas** (para dependientes) y **variables exógenas** (para independientes).

* **Variables Intermediarias / Omitidas (o Confusoras / Latentes):** Estas son variables que **no se incluyen en el estudio o modelo**, pero que en realidad están afectando tanto a la variable independiente como a la dependiente, o solo a la dependiente de forma oculta. No contemplarlas puede llevar a conclusiones erróneas sobre la causalidad.
    * **Ejemplos:** Si estudias la relación entre "horas de estudio" y "rendimiento escolar", la "renta familiar" podría ser una variable omitida. Una mayor renta familiar podría influir tanto en las horas de estudio (acceso a mejores recursos) como en el rendimiento escolar (menos preocupaciones, mejor alimentación), creando una asociación aparente entre horas de estudio y rendimiento que no es tan directa.
    * Es crucial identificarlas para **evitar establecer asociaciones o causalidades infundadas**. Técnicas avanzadas como el Análisis de Covarianza (ANCOVA) permiten "controlar" el efecto de estas variables.

* **Variables Dicotómicas:** Son un tipo especial de variable categórica que solo puede tomar **dos valores posibles**, generalmente representados como 0 y 1. Son muy útiles para indicar la presencia (1) o ausencia (0) de una característica o evento.
    * **Ejemplos:** ¿Fuma? (Sí/No), ¿Aprobado? (Sí/No), Género (Masculino/Femenino).

---

### Para Memorizar:

Piensa en tus datos como piezas de Lego.

* **Variables Categóricas:** Son piezas que tienen una forma específica (cuadrada, redonda, etc.).
    * **Nominales:** Solo se diferencian por el color (rojo, azul, verde). No hay un orden natural.
    * **Ordinales:** Tienen un orden de tamaño (pieza pequeña, mediana, grande). Sabes cuál es más grande, pero no cuánto más grande.
* **Variables Cuantitativas:** Son piezas que tienen un número.
    * **Discretas:** Cuentas las piezas (1 pieza, 2 piezas, 3 piezas). Solo números enteros.
    * **Continuas:** Mides la longitud de la pieza (2.5 cm, 3.1 cm). Puede ser cualquier valor dentro de un rango.

Cuando construyes algo con Lego:

* La **Variable Dependiente** es lo que estás construyendo (tu modelo final).
* Las **Variables Independientes** son las piezas que usas para construirlo.
* Las **Variables Intermediarias/Omitidas** son piezas que no sabías que existían o que no usaste, pero que si las hubieras puesto, el modelo final (variable dependiente) se habría visto diferente o mejor explicado.


## 1.5. Diseño de experimentos

## 1.5. Diseño de Experimentos: Observacionales vs. Experimentales

La forma en que se recogen los datos para un estudio es fundamental y determina qué tipo de conclusiones se pueden sacar. Los estudios estadísticos se dividen principalmente en dos clases:

### 1. Estudios Observacionales

* **¿Qué son?** Son aquellos donde simplemente **observamos y recogemos datos** sin intervenir ni alterar a los individuos de ninguna manera. Es como ser un "observador pasivo".
* **Características Clave:**
    * **No hay intervención:** El investigador no manipula ninguna variable ni aplica tratamientos.
    * **Recolección de información existente:** Se basa en lo que ya está ocurriendo o ha ocurrido.
    * **Ejemplos:**
        * **Encuestas:** Como las que ya vimos, donde se pregunta a la gente sobre sus opiniones, hábitos, etc., sin modificar su comportamiento.
        * Observar el rendimiento académico de estudiantes sin cambiar sus métodos de estudio.
        * Analizar datos históricos de ventas de una empresa.
* **Limitación importante:** Con un estudio observacional, **no se pueden establecer relaciones de causa y efecto**. Solo puedes identificar asociaciones o correlaciones. Por ejemplo, podrías observar que las personas que toman más café tienden a ser más activas, pero no puedes afirmar que el café *causa* esa mayor actividad (podría haber otras razones).

### 2. Estudios Experimentales

* **¿Qué son?** Son aquellos donde el investigador **aplica un tratamiento (o tratamientos)** a un grupo de individuos (llamados aquí "unidades experimentales") y luego **observa los efectos** de dicho tratamiento.
* **Características Clave:**
    * **Intervención activa:** El investigador manipula una o más variables (los tratamientos).
    * **Control de variables:** Se intenta controlar otras variables que podrían influir en los resultados para aislar el efecto del tratamiento.
    * **Asignación aleatoria (idealmente):** Para asegurar que los grupos sean comparables, los individuos se asignan aleatoriamente a los diferentes grupos de tratamiento (o al grupo de control, que no recibe tratamiento).
    * **Ejemplos:**
        * **En bioestadística:** Probar la eficacia de un nuevo medicamento, donde un grupo recibe el medicamento y otro un placebo, observando las diferencias en su salud.
        * Un estudio para ver si un nuevo método de enseñanza mejora las calificaciones, asignando aleatoriamente a los estudiantes a diferentes métodos.
        * Probar diferentes versiones de un sitio web (pruebas A/B) para ver cuál genera más clics.
* **Ventaja clave:** Un estudio experimental bien diseñado **permite establecer relaciones de causa y efecto**. Si el grupo con tratamiento muestra un cambio significativo que el grupo de control no, y las demás condiciones se mantuvieron iguales, se puede inferir que el tratamiento causó el efecto.

### En Resumen y Para Memorizar:

Piensa en la diferencia así:

* **Estudio Observacional = Eres un "espía" o un "historiador".** Solo miras lo que pasa o lo que ya pasó, y registras. No puedes decir que A causó B, solo que A y B a menudo van juntos.
    * **Ejemplo:** Observas que los estudiantes que usan gorra suelen sacar mejores notas. (¿La gorra causa mejores notas? Probablemente no, solo hay una asociación).
* **Estudio Experimental = Eres un "científico en un laboratorio".** Tú creas las condiciones, aplicas algo (el "tratamiento") y luego ves qué sucede. Si lo haces bien, puedes decir que lo que aplicaste *causó* el efecto.
    * **Ejemplo:** Haces que un grupo de estudiantes use una "gorra de la suerte" y otro grupo no, y luego comparas sus notas para ver si la gorra realmente tiene un efecto. (Suponiendo que todo lo demás sea igual).

### Complementando el Tema:

* **Confusión y Variables Confounding (Confusoras):** En los estudios observacionales, es muy fácil que haya variables "confusoras" que expliquen la asociación observada, en lugar de una causalidad directa. Por ejemplo, en el caso del café y la actividad, quizás las personas que beben más café también tienen trabajos más demandantes o estilos de vida más activos en general (estas serían variables confusoras). Los estudios experimentales, especialmente con asignación aleatoria, ayudan a minimizar el efecto de estas variables confusoras.
* **Ética:** La elección entre un estudio observacional y uno experimental a menudo está limitada por consideraciones éticas. No siempre es éticamente aceptable aplicar ciertos "tratamientos" (por ejemplo, exponer a personas a sustancias dañinas) o retener tratamientos beneficiosos. En esos casos, los estudios observacionales son la única opción viable.
* **Validez interna y externa:** Los experimentos bien controlados suelen tener alta **validez interna** (puedes estar seguro de que la causa que investigaste produjo el efecto observado). Sin embargo, a veces tienen menor **validez externa** (los resultados podrían no ser generalizables a la población real fuera del entorno controlado del experimento). Los estudios observacionales, aunque no prueben causalidad, a menudo tienen mayor validez externa porque se realizan en entornos naturales.

## 1.6. Razonamiento estadístico
¡Absolutamente! El razonamiento estadístico es la base para cualquier analista de datos. No se trata solo de aplicar fórmulas, sino de pensar críticamente sobre los datos y las conclusiones.

---

## 1.6. Razonamiento Estadístico: Pensando Críticamente con los Datos

El razonamiento estadístico es la capacidad de entender y evaluar la información estadística de manera crítica. Implica hacerse las preguntas correctas para asegurar que las conclusiones sean válidas y útiles. No es solo un conjunto de herramientas, sino una forma de pensar.

Para desarrollar un pensamiento estadístico sólido, es fundamental plantearse las siguientes preguntas clave (adaptadas de Triola, 2009):

1.  **¿Cuál es el objetivo del estudio?**
    * Antes de sumergirte en los números, debes tener claro qué se busca responder con el estudio. Sin un objetivo claro, los análisis pueden ser inútiles o misleading.

2.  **¿Quién es la fuente de los datos?**
    * Esta es una pregunta crucial. La **fuente de los datos** (quién recopiló, financió o presenta el estudio) puede tener un **interés propio** en los resultados. Si la fuente no es neutral, puede haber una manipulación consciente o inconsciente de los datos o las conclusiones para beneficiar sus propios intereses. A esto se le conoce coloquialmente como el "cocinado" de datos.

3.  **¿Con qué tipo de muestreo se han obtenido los datos?**
    * Como vimos en la unidad anterior, el método de muestreo es vital. Un mal muestreo puede llevar a una **muestra no representativa** de la población, lo que invalida cualquier inferencia posterior. ¿Fue un muestreo aleatorio? ¿Hay sesgos en la selección?

4.  **¿Existen variables que influyan en los resultados y que se hayan omitido?**
    * Ya hablamos de las **variables intermediarias u omitidas**. Son factores que no se tuvieron en cuenta en el estudio, pero que podrían estar influyendo significativamente tanto en las variables explicativas como en las de respuesta. Ignorarlas puede llevar a conclusiones engañosas sobre causalidad.

5.  **¿Las gráficas resumen adecuadamente los datos?**
    * Las visualizaciones son poderosas, pero también pueden ser engañosas. Una gráfica mal diseñada puede distorsionar la percepción de los datos (ejes truncados, escalas inapropiadas, etc.). Un buen razonador estadístico siempre examina la forma en que se presentan los datos.

6.  **¿Las conclusiones se extraen directa y naturalmente de los datos?**
    * Esto significa que las conclusiones deben estar respaldadas por la evidencia presentada. Evita saltar a conclusiones que no se derivan lógicamente de los resultados del análisis. No inventes narrativas que no soporten los datos.

7.  **¿Se ha cumplido el objetivo marcado al principio del estudio y tienen sentido y utilidad práctica las conclusiones obtenidas?**
    * Al final del día, el estudio debe ser útil. Las conclusiones no solo deben ser estadísticamente válidas, sino que también deben tener sentido en el contexto del problema y ofrecer un valor práctico. ¿Realmente ayudaron a responder la pregunta original y a tomar mejores decisiones?

### El Concepto Fundamental de Sesgo

El concepto central de todas estas preguntas es el **sesgo**.

* El **sesgo** en estadística se refiere a cualquier factor que distorsiona los resultados de un estudio, llevándolos a desviarse sistemáticamente de la verdad. Un estudio sesgado no proporciona una imagen precisa de la realidad.

* **Fuentes de Sesgo Comunes:**
    * **Sesgo de la fuente:** Como se mencionó, el interés propio de quien encarga o realiza el estudio.
    * **Sesgo de muestreo:** Cuando la muestra no es representativa de la población (ej., solo encuestar a personas en un lugar específico o a una hora determinada).
    * **Sesgo de respuesta/no respuesta:** Cuando algunas personas son más propensas a responder que otras, o cuando las respuestas son intencionalmente inexactas.
    * **Sesgo de medición:** Cuando los instrumentos de medición no son precisos o las preguntas del cuestionario son capciosas o confusas (como el "efecto de redacción" en el Ejemplo 3).
    * **Sesgo de confirmación:** Cuando los investigadores interpretan los resultados de una manera que confirma sus hipótesis o creencias preexistentes.
    * **Sesgo de publicación:** Tendencia a publicar solo resultados "significativos" o "interesantes", dejando de lado estudios con resultados menos llamativos.

* El Ejemplo 3 ilustra varias formas en que un estudio de opinión pública puede estar sesgado: el momento de la encuesta, la publicación, la ideología de la agencia, y el diseño de las preguntas.

* El Ejemplo 4 resalta la importancia del **contexto**. Un analista de datos que recibe un archivo Excel con solo "X" e "Y" sin ninguna otra información (objetivo del estudio, cómo se recogieron, qué representan X e Y) no puede hacer un análisis estadístico con sentido. Sin contexto, los datos son solo números vacíos. Saber que X e Y son las calificaciones de dos grupos (mañana y tarde) en una asignatura universitaria y que es una muestra aleatoria simple de 10 alumnos por grupo, es el tipo de contexto necesario para empezar a razonar estadísticamente.

---

### Para Memorizar:

Piensa en el razonamiento estadístico como si fueras un **abogado defensor** o un **juez** de los datos.

* **No te creas todo lo que ves a primera vista.**
* **Pregunta SIEMPRE:**
    * ¿Quién lo dice? (Fuente/Sesgo)
    * ¿Cómo lo sabe? (Muestreo/Diseño)
    * ¿Falta información? (Variables omitidas/Contexto)
    * ¿Lo están mostrando de forma justa? (Gráficas)
    * ¿Realmente se desprende de lo que dicen? (Conclusiones)
    * ¿Sirve para algo? (Utilidad práctica)

El **sesgo** es el "pecado capital" de la estadística. Siempre busca dónde podría estar el sesgo.

---

Este es un tema crucial para tu maestría, ya que te permitirá evaluar críticamente no solo tus propios análisis, sino también los de otros. ¿Te gustaría que profundicemos en algún aspecto del sesgo o pasamos al siguiente tema?


## 1.7. Representando los datos: distribución de frecuencias

¡Perfecto! Esta unidad es fundamental para empezar a organizar y entender la estructura de tus datos. La **distribución de frecuencias** es la primera herramienta que usamos para darle sentido a un conjunto de datos brutos.

---

## 1.7. Representando los Datos: Distribución de Frecuencias

Una vez que hemos recolectado los datos, el primer paso para organizarlos y representarlos es contarlos y ver cuántas veces se repite cada valor o categoría de una variable. A estas cuentas las llamamos **frecuencias**.

La **distribución de frecuencias** es una tabla o resumen que muestra cómo los datos se distribuyen entre las diferentes categorías o valores que puede tomar una variable, junto con el número o proporción de veces que cada uno aparece.

Existen cuatro tipos principales de frecuencias:

### 1. Frecuencia Absoluta ($f_i$ o $n_i$)

* **Definición:** Es el **número de veces** que se repite un valor específico o una categoría (modalidad) de una variable en el conjunto de datos.
* **Notación:** Generalmente se denota como $f_i$ o $n_i$, donde el subíndice $i$ hace referencia a la i-ésima categoría o valor.
* **Propiedad Importante:** La suma de todas las frecuencias absolutas de todas las modalidades debe ser igual al **tamaño total de la muestra ($N$)**.
    $$\sum_{i=1}^{k} f_i = N$$
    (Donde $k$ es el número total de categorías o valores únicos).

### 2. Frecuencia Relativa ($h_i$ o $f_i\%$)

* **Definición:** Es la **proporción** de veces que se repite un valor o categoría en relación con el tamaño total de la muestra ($N$). Se obtiene dividiendo la frecuencia absoluta de una modalidad entre el tamaño total de la muestra.
* **Cálculo:**
    $$h_i = \frac{f_i}{N}$$
* **Propiedad Importante:** La suma de todas las frecuencias relativas de todas las modalidades debe ser igual a **1** (o 100% si se expresa en porcentaje).
    $$\sum_{i=1}^{k} h_i = 1$$
* **Utilidad:** Es muy útil para comparar distribuciones de datos de diferentes tamaños, ya que ofrece una perspectiva proporcional.

### 3. Frecuencia Absoluta Acumulada ($F_i$ o $N_i$)

* **Definición:** Es la **suma de las frecuencias absolutas** de una modalidad dada y todas las modalidades anteriores (las que tienen valores menores o iguales).
* **Notación:** Generalmente se denota con letras mayúsculas, como $F_i$ o $N_i$.
* **Cálculo:**
    $$F_i = f_1 + f_2 + \dots + f_i = \sum_{j=1}^{i} f_j$$
* **Propiedad Importante:** La última frecuencia absoluta acumulada (la del último valor o categoría) debe ser igual al **tamaño total de la muestra ($N$)**. Es decir, $F_k = N$.
* **Utilidad:** Nos indica cuántos datos son menores o iguales a un cierto valor.

### 4. Frecuencia Relativa Acumulada ($H_i$)

* **Definición:** Es la **suma de las frecuencias relativas** de una modalidad dada y todas las modalidades anteriores. Por analogía con las frecuencias absolutas acumuladas, se obtienen sumando las frecuencias relativas hasta una determinada modalidad.
* **Cálculo:**
    $$H_i = h_1 + h_2 + \dots + h_i = \sum_{j=1}^{i} h_j$$
* **Propiedad Importante:** La última frecuencia relativa acumulada (la del último valor o categoría) debe ser igual a **1** (o 100%).
* **Utilidad:** Nos indica qué proporción o porcentaje de los datos son menores o iguales a un cierto valor. Esto es especialmente útil para calcular percentiles o cuartiles.

### Ejemplo Práctico (para entender las fórmulas):

Imaginemos las calificaciones (del 1 al 5) de 10 estudiantes en un examen:
2, 3, 3, 4, 4, 4, 5, 5, 1, 4

**Paso 1: Organizar los datos y calcular frecuencias absolutas ($f_i$)**

| Calificación ($i$) | Conteo | Frecuencia Absoluta ($f_i$) |
| :----------------- | :----- | :-------------------------- |
| 1                  | 1      | 1                           |
| 2                  | 1      | 1                           |
| 3                  | 2      | 2                           |
| 4                  | 4      | 4                           |
| 5                  | 2      | 2                           |
| **Total** |        | **$N = 10$** |

**Paso 2: Calcular frecuencias relativas ($h_i$)**

| Calificación ($i$) | $f_i$ | $h_i = f_i / N$ |
| :----------------- | :---- | :-------------- |
| 1                  | 1     | $1/10 = 0.1$    |
| 2                  | 1     | $1/10 = 0.1$    |
| 3                  | 2     | $2/10 = 0.2$    |
| 4                  | 4     | $4/10 = 0.4$    |
| 5                  | 2     | $2/10 = 0.2$    |
| **Total** | **10** | **1.0** |

**Paso 3: Calcular frecuencias absolutas acumuladas ($F_i$)**

| Calificación ($i$) | $f_i$ | $F_i$ |
| :----------------- | :---- | :---- |
| 1                  | 1     | 1     |
| 2                  | 1     | $1+1=2$ |
| 3                  | 2     | $2+2=4$ |
| 4                  | 4     | $4+4=8$ |
| 5                  | 2     | $8+2=10$|
| **Total** | **10** | **$N=10$** |

**Paso 4: Calcular frecuencias relativas acumuladas ($H_i$)**

| Calificación ($i$) | $f_i$ | $h_i$ | $H_i$ |
| :----------------- | :---- | :---- | :---- |
| 1                  | 1     | 0.1   | 0.1   |
| 2                  | 1     | 0.1   | $0.1+0.1=0.2$ |
| 3                  | 2     | 0.2   | $0.2+0.2=0.4$ |
| 4                  | 4     | 0.4   | $0.4+0.4=0.8$ |
| 5                  | 2     | 0.2   | $0.8+0.2=1.0$ |
| **Total** | **10** | **1.0** | **1.0** |

### Para Memorizar:

Piensa en una **lista de asistencia a clase**:

* **Frecuencia Absoluta:** ¿Cuántos estudiantes vinieron hoy? (El número exacto).
* **Frecuencia Relativa:** ¿Qué porcentaje de la clase vino hoy? (La proporción respecto al total).
* **Frecuencia Absoluta Acumulada:** Al final de la semana, ¿cuántos días ha venido cada estudiante hasta el día de hoy? (La suma de asistencias hasta ese punto).
* **Frecuencia Relativa Acumulada:** ¿Qué porcentaje del total de días de clase ha asistido cada estudiante hasta ahora? (La proporción acumulada).

Esta tabla de frecuencias es el primer paso para visualizaciones de datos como histogramas o diagramas de barras, y es crucial para calcular medidas de tendencia central y dispersión que verás más adelante.


## 1.8. Tabulación de variables

## 1.8. Tabulación de Variables: Construyendo Tablas de Frecuencias

La **tabulación de variables** se refiere al proceso de organizar y presentar las frecuencias (absolutas, relativas, acumuladas) de una variable en una estructura clara y fácil de leer, que generalmente es una **tabla de frecuencias**.

### La Tabla de Frecuencias: Estructura Básica

Una tabla de frecuencias típicamente consta de `k` filas, donde cada fila corresponde a una **modalidad** (un valor único o una categoría) de la variable en estudio. Las columnas de la tabla contendrán la información sobre las diferentes frecuencias.

La forma más común y práctica de una tabla de frecuencias incluye al menos:

1.  **Columna de Valores / Categorías:** Donde se listan todas las modalidades posibles de la variable.
2.  **Columna de Frecuencias Absolutas ($f_i$):** El número de veces que aparece cada modalidad.
3.  **Columna de Frecuencias Relativas (%):** La proporción de cada modalidad, expresada como porcentaje.

**Ejemplo 5 (Reafirmando el uso de porcentajes):**
Es muy común ver que en lugar de una columna de frecuencia relativa (en tanto por uno, ej. 0.25), se use directamente una columna de **porcentajes**. Esto es porque son equivalentes ($0.25 = 25\%$), y los porcentajes suelen ser más intuitivos para el público general. La suma de los porcentajes debe ser 100%.

### Manejo de Valores Perdidos (Missing Values)

Es muy frecuente que en un conjunto de datos no todos los individuos tengan un valor registrado para cada variable. Cuando esto ocurre, decimos que el individuo presenta un **valor perdido** o **missing value** en esa variable.

* **¿Cómo se gestionan?** En las tablas de frecuencias, es habitual añadir una columna extra llamada "válidos" o "N válido". Esta columna (o un total en el pie de tabla) indica el número de observaciones que realmente tienen un valor para esa variable, excluyendo los perdidos.
* **"No Aplicable" como un caso especial:** A veces, un valor "no aplicable" (N/A o NA) o "no procede" se considera un tipo de valor perdido. Esto ocurre cuando una pregunta no tiene sentido para ciertos individuos, dependiendo de sus respuestas a preguntas anteriores.
    * **Ejemplo 6:** Si preguntas "¿Tiene hijos?" (Sí/No) y luego "¿Cuántos hijos tiene?", para las personas que respondieron "No" a la primera pregunta, la segunda pregunta sería "no aplicable". Estos "no aplicables" deben ser identificados y tratados adecuadamente para no distorsionar las frecuencias de las personas que sí tienen hijos.

### Tablas de Frecuencias para Variables Continuas (Agrupamiento por Intervalos)

Cuando trabajamos con **variables continuas** (como edad, altura, peso, tiempo), que pueden tomar un número infinito de valores, no es práctico listar cada valor único y su frecuencia. En su lugar, agrupamos los datos en **intervalos de clase** (o rangos).

Cuando se agrupan en intervalos, aparecen dos conceptos importantes:

1.  **Límite Inferior y Superior del Intervalo:**
    * Cada intervalo tiene un límite inferior (el valor más bajo que puede tomar un dato en ese intervalo) y un límite superior (el valor más alto).
    * **Importante:** La forma en que se definen los límites es crucial para evitar ambigüedad. Por ejemplo, un intervalo puede ser $]10, 20]$ (valores mayores de 10 hasta 20 inclusive) o $[10, 20[$ (valores desde 10 inclusive hasta menos de 20). La notación y la convención deben ser claras.

2.  **Marca de Clase del Intervalo:**
    * La **marca de clase** es el **valor que representa al intervalo**. Generalmente, es el **punto medio** del intervalo.
    * **Cálculo:** Se calcula sumando el límite inferior y el límite superior del intervalo y dividiendo el resultado por 2.
        $$\text{Marca de Clase} = \frac{\text{Límite Inferior} + \text{Límite Superior}}{2}$$
    * **Utilidad:** Esta marca de clase es muy importante porque se utiliza como un valor promedio o representante del intervalo en cálculos posteriores, especialmente para medidas resumen como la media aritmética, que veremos en el siguiente tema.

### Ejemplo de Tabla con Intervalos:

| Intervalo de Edad | Frecuencia Absoluta | Marca de Clase |
| :---------------- | :------------------ | :------------- |
| $[0, 10[$         | 5                   | $(0+10)/2 = 5$ |
| $[10, 20[$        | 8                   | $(10+20)/2 = 15$ |
| $[20, 30]$        | 12                  | $(20+30)/2 = 25$ |
| **Total** | **25** | |

---

### Para Memorizar:

Piensa en la **tabulación** como la creación de un **reporte organizado** de tus datos.

* **Tabla de Frecuencias:** Es el formato estándar de ese reporte, mostrando cuántas veces ocurre cada cosa (frecuencias).
* **Porcentajes:** Son la forma más amigable de presentar las frecuencias relativas.
* **Valores Perdidos:** Son datos "ausentes". Es vital contarlos aparte y tener una columna de "válidos" para no engañarse con los totales.
* **Intervalos (para Continuas):** Si tus datos son muy variados (ej. edades exactas), los agrupas en "cajas" (intervalos).
    * **Marca de Clase:** Es la "etiqueta central" de cada caja, el valor que usas para representarla.


## 1.9. Gráficas básicas

Como dice el dicho, "más vale un buen gráfico que mil tablas de frecuencias". 

## 1.9. Gráficas Básicas: El Arte de la Visualización

El objetivo de las gráficas es comunicar información de manera rápida y efectiva. La elección del gráfico ideal depende principalmente del **tipo de variable** que queremos representar.

### Gráficos para Variables Categóricas (Cualitativas y Cuantitativas Discretas)

Para variables categóricas (nominales, ordinales) y cuantitativas discretas (donde cada valor discreto se puede tratar como una categoría), los gráficos más comunes son:

1.  **Diagramas de Barras:**
    * **Cuándo usarlo:** Es el gráfico más versátil para representar frecuencias o proporciones de diferentes categorías. Cada barra representa una categoría, y la altura de la barra indica su frecuencia (absoluta o relativa/porcentaje).
    * **Tipos:**
        * **Diagrama de Barras Simples:** El más común, con barras separadas para cada categoría.
        * **Diagrama de Barras Apiladas:** Un caso especial donde las barras se dividen en segmentos para mostrar la composición de una categoría más grande (útil para comparar subcategorías dentro de una categoría principal).
    * **Lo esencial:** **Todas las variables (excepto las continuas)** pueden ser representadas con diagramas de barras.

2.  **Gráficos de Sectores (Gráficos Circulares, "Pie Charts"):**
    * **Cuándo usarlo:** Exclusivo para variables cualitativas (categóricas). Muestra la proporción de cada categoría como una "rebanada" de un pastel. El área de cada sector es proporcional a su porcentaje sobre el total.
    * **Requisito clave:** Las porciones deben representar porcentajes y la suma total siempre debe ser **100%**.
    * **Mejor uso:** Es más efectivo cuando el **número de categorías no es excesivo** (idealmente 2 a 5 categorías). Si hay demasiadas categorías o las diferencias de porcentaje son muy pequeñas, puede ser difícil de leer y un diagrama de barras es una mejor opción.

3.  **Pictogramas:**
    * **Cuándo usarlo:** Para variables cualitativas. Utiliza **dibujos o íconos** relacionados con el tema para representar las frecuencias. Por ejemplo, si se representa el número de coches, se usarían íconos de coches.
    * **Ventaja:** Puede ser visualmente muy atractivo y efectivo para comunicar un mensaje específico, especialmente si los íconos son simbólicos y potentes.
    * **Error común a evitar:** **El tamaño de los dibujos debe ser proporcional al área, no a la altura o la longitud.** Si el valor es el doble, el *área* del dibujo debe ser el doble, no solo su altura. Esto es crucial para no distorsionar la percepción de las magnitudes.
    * **Limitación:** Su uso es limitado en software estadístico común y su correcta elaboración requiere atención al detalle del área para evitar engaños visuales.

### Gráficos para Variables Cuantitativas Continuas

Para variables cuantitativas continuas (como edad, peso, tiempo), que generalmente se agrupan en intervalos, usamos gráficos que comunican esa continuidad:

1.  **Histograma:**
    * **Cuándo usarlo:** Es el gráfico equivalente al diagrama de barras para variables cuantitativas continuas. Las barras se **dibujan juntas**, sin espacios entre ellas, para enfatizar la continuidad de la variable.
    * **Ejes:** El eje horizontal (abscisas) muestra los intervalos de la variable (a menudo representados por sus marcas de clase), y el eje vertical (ordenadas) muestra la frecuencia (absoluta o relativa) de los datos dentro de cada intervalo.
    * **Utilidad:** Nos permite visualizar la forma de la distribución de los datos (simétrica, asimétrica, unimodal, bimodal), la dispersión y la presencia de valores atípicos.

2.  **Polígono de Frecuencias:**
    * **Cuándo usarlo:** También se usa para variables cuantitativas continuas (y a veces discretas cuando hay muchos valores). Se construye uniendo los **puntos medios de la parte superior de las barras de un histograma** con líneas. Se puede mostrar con o sin el histograma subyacente.
    * **Ventaja:** Es útil para comparar la forma de dos o más distribuciones en el mismo gráfico, ya que las líneas son menos "masivas" que las barras.

### Complementando el Tema: La Importancia de la Elección Correcta

Elegir el gráfico adecuado es una habilidad crítica para un analista de datos. Una buena visualización puede revelar patrones ocultos, mientras que una elección incorrecta puede ocultar información o incluso engañar.

* **Siempre ten en cuenta la audiencia:** ¿Quién va a ver este gráfico? ¿Es para un público general o para expertos? Esto puede influir en la complejidad y el tipo de gráfico.
* **Claridad y Simplicidad:** Un buen gráfico es claro, conciso y no está sobrecargado de información innecesaria.
* **Etiquetas y Títulos:** Siempre incluye títulos claros para el gráfico y los ejes, así como etiquetas para las categorías o valores, para que el gráfico sea autoexplicativo.

---

### Para Memorizar:

Piensa en los gráficos como **herramientas visuales** en tu caja de herramientas de análisis de datos.

* **¿Categóricas (cualitativas y discretas)?**
    * **Barras:** Tu herramienta más versátil, casi siempre funciona. Como un martillo multiusos.
    * **Sectores (Pastel):** Solo si tienes POCAS categorías y quieres mostrar PARTES de un TODO (como cortar un pastel).
    * **Pictogramas:** Para un efecto visual impactante, pero ¡cuidado con las ÁREAS de los dibujos!

* **¿Cuantitativas (continuas)?**
    * **Histograma:** Para ver la FORMA de la distribución de tus datos continuos. Las barras van JUNTAS.
    * **Polígono de Frecuencias:** Para suavizar el histograma o comparar varias distribuciones.



## 1.10. El Arte de Elegir el Gráfico Adecuado

Seleccionar el gráfico correcto no es solo una cuestión de estética, sino de **eficacia en la comunicación**. Un buen gráfico es aquel que permite a tu audiencia entender rápidamente la historia que tus datos quieren contar, sin distorsiones ni confusiones.

El "arte" de elegir el gráfico adecuado se reduce principalmente a entender dos cosas:

1.  **¿Qué tipo de variable(s) estás intentando representar?** (Este es el factor más importante).
2.  **¿Qué mensaje o relación quieres comunicar?** (Comparación, distribución, composición, tendencia, relación entre variables).

Aunque el texto menciona una tabla que aún no me has proporcionado, puedo inferir y complementar la lógica detrás de esa tabla basándome en lo que ya hemos discutido sobre los tipos de variables y los gráficos básicos.

### La Pista Esencial: El Tipo de Variable

Como ya hemos visto, el tipo de variable es la guía fundamental. Reafirmemos:

* **Variables Categóricas (Nominales u Ordinales):** Representan cualidades o categorías.
    * **Gráficos principales:** Diagramas de barras, gráficos de sectores (pie charts), pictogramas.
    * **¿Cuál elegir?**
        * **Diagrama de Barras:** Siempre una opción segura y flexible. Útil para comparar frecuencias entre categorías o para mostrar categorías con muchas opciones. Es el "caballito de batalla".
        * **Gráfico de Sectores:** Bueno para mostrar **partes de un todo** (composición) cuando tienes **pocas categorías** (idealmente 2-5) y cada una representa una porción significativa del 100%. Se vuelve ineficaz con muchas categorías o porcentajes muy similares.
        * **Pictogramas:** Para un impacto visual fuerte o para audiencias menos técnicas, pero con la precaución de la proporcionalidad del área.

* **Variables Cuantitativas Discretas:** Aunque son numéricas, si tienen un número limitado de valores o si cada valor se trata como una categoría, a menudo se pueden manejar como categóricas.
    * **Gráficos principales:** Diagramas de barras.
    * **¿Por qué no siempre un histograma?** Un histograma es más para rangos continuos. Si tienes "número de hijos" (0, 1, 2, 3), un diagrama de barras muestra claramente la frecuencia de cada número específico.

* **Variables Cuantitativas Continuas:** Representan magnitudes que pueden tomar cualquier valor dentro de un rango.
    * **Gráficos principales:** Histogramas, polígonos de frecuencias.
    * **¿Cuál elegir?**
        * **Histograma:** Es el estándar de oro para visualizar la **distribución** de una sola variable continua. Te muestra la forma de los datos, dónde se concentran, si son simétricos o asimétricos, y si hay valores atípicos.
        * **Polígono de Frecuencias:** Útil para comparar la distribución de dos o más conjuntos de datos continuos en el mismo gráfico, o para una visualización más "suave" de la distribución.

### Más Allá del Tipo de Variable: El Mensaje que Quieres Transmitir

Aunque el tipo de variable es la guía principal, también debes considerar qué quieres comunicar:

* **Comparación:**
    * ¿Comparar categorías? $\rightarrow$ Barras.
    * ¿Comparar distribuciones? $\rightarrow$ Histogramas o polígonos de frecuencias superpuestos.
* **Composición (Partes de un todo):**
    * ¿Cómo se distribuye un total en sus partes? $\rightarrow$ Gráfico de sectores (si pocas categorías), Barras apiladas (si muchas categorías o quieres comparar subtotales).
* **Distribución (Forma de los datos):**
    * ¿Cómo se distribuyen los valores de una variable? $\rightarrow$ Histograma (cuantitativas), Diagrama de barras (categóricas/discretas).
* **Relación entre dos variables:** (Esto lo verás en unidades futuras)
    * ¿Una variable cuantitativa vs. otra cuantitativa? $\rightarrow$ Diagrama de dispersión (scatter plot).
    * ¿Una cuantitativa vs. una categórica? $\rightarrow$ Diagramas de caja (box plots) o de violín.
* **Tendencia a lo largo del tiempo:**
    * ¿Cómo cambia algo con el tiempo? $\rightarrow$ Gráfico de líneas (útil para series temporales).

### El Proceso de Elección (La "Tabla Mental" que se Interioriza)

Cuando te enfrentes a un conjunto de datos, tu proceso mental debería ser algo así:

1.  **Identifica la variable(s) principal(es) que quieres visualizar.**
2.  **Determina el tipo de cada variable:** ¿Categórica (nominal/ordinal)? ¿Cuantitativa (discreta/continua)?
3.  **Piensa en el mensaje clave:** ¿Quieres mostrar frecuencias? ¿Proporciones? ¿La forma de la distribución? ¿Una relación?
4.  **Consulta la "tabla mental":** Basándote en el tipo de variable y el mensaje, ¿qué gráficos son las opciones más fuertes?
5.  **Evalúa las alternativas:** ¿Un gráfico de barras o de sectores? ¿Un histograma o un polígono? Considera la claridad, la simplicidad y el impacto para tu audiencia.
6.  **Crea el gráfico y revísalo:** ¿Es claro? ¿Está bien etiquetado? ¿Comunica el mensaje deseado sin ambigüedad?

---

### Para Memorizar:

Piensa en elegir un gráfico como elegir la **ropa adecuada para una ocasión**:

* **La Variable:** Es el "clima" o el "código de vestimenta" (formal, informal, etc.). Te da la primera pista sobre qué tipo de ropa (gráfico) es apropiada.
    * **Categóricas/Discretas:** Ropa casual (barras, pastel, pictogramas).
    * **Continuas:** Ropa específica para el clima (histogramas, polígonos).
* **El Mensaje:** Es el "propósito" de la ocasión (una fiesta, una reunión de negocios, un evento deportivo). Esto afina tu elección dentro de las opciones apropiadas.
    * ¿Mostrar partes de un todo? $\rightarrow$ Traje de gala (gráfico de pastel).
    * ¿Comparar elementos? $\rightarrow$ Ropa a juego (barras).
    * ¿Mostrar la forma de tu cuerpo (distribución)? $\rightarrow$ Ropa ajustada (histograma).

La práctica constante te ayudará a **interiorizar** esta tabla y tomar decisiones de visualización de forma intuitiva.


## 1.11. Retos de la estadística en el Big Data

¡Fantástico! Hemos llegado a la última unidad del Tema 1, que es un excelente cierre porque te introduce a los desafíos y la relevancia actual de la estadística en el mundo del **Big Data**, un concepto central en tu maestría.

---

## 1.11. Retos de la Estadística en el Big Data

La estadística, aunque es una disciplina clásica con raíces en el siglo XIX, se ha visto profundamente transformada por la era de los computadores, Internet y, especialmente, el **Big Data**. El cambio más drástico es la **cantidad masiva de información** disponible. Antes, los conjuntos de datos eran relativamente pequeños; ahora, son tan grandes que a menudo no sabemos cómo analizarlos, lo que lleva a la paradoja de tener muchos datos pero ser incapaces de aprender de ellos.

Para que la estadística siga siendo relevante y útil en este nuevo entorno, debe adaptarse y enfrentar varios desafíos clave:

### 1. Excesiva Cantidad de Información y Datos (Volumen)

* **El Problema:** Los métodos estadísticos clásicos no fueron diseñados para manejar volúmenes de datos tan grandes de manera eficiente. Aplicarlos directamente puede llevar a tiempos de cómputo inviables.
* **La Solución:**
    * **Creación de códigos eficientes:** Necesitamos algoritmos y software optimizados que permitan aplicar los métodos estadísticos existentes a grandes volúmenes de datos de forma rápida.
    * **Desarrollo de nuevos métodos estadísticos:** Investigar y crear técnicas estadísticas novedosas que sean inherentemente escalables y capaces de trabajar con cantidades masivas de información desde su concepción.
* **El Reto de los Outliers (Valores Atípicos):** En el Big Data, los "outliers" (datos muy diferentes al resto) ya no son casos aislados que se pueden eliminar fácilmente. Pueden ser miles o millones de observaciones que, aunque atípicas, contienen información valiosa. Eliminarlos o ignorarlos masivamente ya no es una solución adecuada, y la estadística debe desarrollar métodos más robustos para tratarlos sin descartar información relevante.

### 2. Complejidad de los Datos (Variedad y Veracidad)

* **El Problema:** El Big Data no solo es "muchos datos", sino también datos **extremadamente complejos y heterogéneos**. Gran parte de estos datos provienen de la "huella digital" de los usuarios de Internet (clics, interacciones en redes sociales, transacciones, sensores, etc.). Esta diversidad de formatos y fuentes hace que sean difíciles de interpretar directamente.
* **La Solución:** Los métodos de análisis estadístico necesitan incorporar procesos de **transformación de datos (preprocesamiento)** que permitan limpiar, estructurar y unificar esta información compleja para que pueda ser interpretada y analizada de forma efectiva. Esto incluye técnicas de normalización, estandarización, imputación de valores faltantes y más.

### 3. Necesidad de Infraestructuras Potentes de Análisis (Velocidad)

* **El Problema:** Analizar grandes y complejos volúmenes de datos requiere una capacidad de procesamiento computacional inmensa para obtener resultados en tiempos razonables.
* **La Solución:** Afortunadamente, los avances en la computación han proporcionado soluciones:
    * **Clusters de computadoras:** Redes de ordenadores que trabajan juntos para procesar tareas complejas.
    * **Computación en la nube:** Permite acceder a recursos computacionales masivos (servidores, almacenamiento) a través de Internet, sin necesidad de tener la infraestructura física.
    * **Métodos paralelizados:** Es crucial que los algoritmos estadísticos sean diseñados para ser "paralelizables", es decir, que puedan dividirse en subtareas que se ejecuten simultáneamente en múltiples procesadores. Esto maximiza el uso de la infraestructura y acelera significativamente la obtención de resultados.

### 4. Políticas de Privacidad (Valor)

* **El Problema:** Gran parte de la "huella digital" y otros datos masivos son **privativos**; pertenecen a empresas o están protegidos por leyes de privacidad (como GDPR o CCPA). No están libremente disponibles para cualquier estudio.
* **La Solución:** Para acceder a esta información, es necesario:
    * **Pedir autorización a los usuarios.**
    * **Solicitar o comprar los datos a las empresas** que los han recolectado.
    * A veces, es necesario **cruzar datos de varias empresas**, lo que añade capas de complejidad en la gestión y los acuerdos.
* **Implicación:** Esto significa que, aunque hay una "alta cantidad de información disponible", el acceso a ella no es universal y a menudo tiene un costo o requiere acuerdos complejos. Las empresas ven estos datos como un activo valioso y buscan monetizarlos.

### 5. Recogida de Datos sin Previa Especificación del Problema (Cambio de Paradigma)

* **El Problema:** La estadística clásica sigue un flujo: **primero se diseña el estudio/problema, luego se recogen los datos** (a menudo a través de encuestas u experimentos controlados). Sin embargo, en el entorno Big Data, es común que los datos **ya existan** (generados pasivamente por usuarios, sensores, etc.) *antes* de que se formule una pregunta específica. Esto significa que a menudo se tiene una montaña de datos y luego se busca qué preguntas se pueden responder con ellos.
* **La Implicación:** Esto invierte el flujo tradicional del proceso estadístico. El analista de datos a menudo comienza con un vasto conjunto de datos y debe explorarlos para descubrir posibles preguntas y problemas, en lugar de diseñar una recolección de datos específica para un problema predefinido. Esto requiere habilidades de exploración de datos y descubrimiento de patrones.

---

### Para Memorizar:

Piensa en el **Big Data como un océano inmenso y turbulento** y la **estadística como un barco**.

* **Volumen (Excesiva Cantidad):** El océano es tan grande que tu barco tradicional (métodos clásicos) se ahoga. Necesitas un superpetrolero (código eficiente, nuevos métodos).
* **Variedad/Complejidad:** El océano no es solo agua; tiene algas, rocas, peces de todo tipo (datos heterogéneos). Necesitas herramientas para limpiarlo y entenderlo (preprocesamiento).
* **Velocidad (Infraestructura):** Navegar este océano requiere motores potentes (clusters, nube, paralelización). No puedes ir a remo.
* **Veracidad/Valor (Privacidad):** No todo el océano es de libre acceso. Hay zonas privadas (datos protegidos) a las que necesitas permiso o pago para entrar.
* **Cambio de Paradigma (Recogida sin problema previo):** Antes, tú decidías a qué parte del océano ir para pescar un tipo específico de pez (diseño y recolección). Ahora, ya tienes una red enorme llena de peces de todo tipo, y tienes que ver qué puedes hacer con lo que ya capturaste.


# TEMA 2: Estadística computacional

## 2.2. Principios básicos
¡Hola! ¡Claro que sí, con gusto te ayudaré a comprender y memorizar estos temas para tu maestría en análisis de datos! La **Estadística Computacional** es un campo fascinante y fundamental hoy en día.

---

## ¿Qué es la Estadística Computacional?

Imagina que tienes una cantidad gigantesca de datos, como la información de millones de transacciones bancarias o los patrones de comportamiento de miles de usuarios en una red social. Analizar esto con los métodos tradicionales sería casi imposible, o tomaría una eternidad. Aquí es donde entra la **Estadística Computacional**.

En pocas palabras, la **Estadística Computacional** es como el "cerebro" y los "músculos" de la estadística moderna. Es la ciencia que **combina los principios de la estadística y las matemáticas con el poder de las computadoras y la programación** para resolver problemas complejos que involucran una gran cantidad de datos.

Piensa en ella como una herramienta que te permite:
* **Recopilar y organizar** datos masivos.
* **Visualizarlos** de formas que te permitan entenderlos.
* **Analizarlos** usando algoritmos y programas informáticos.
* **Encontrar patrones y soluciones** que serían imposibles de detectar manualmente.

---

### ¿Por qué es tan importante la Estadística Computacional?

Hoy en día, el mundo está lleno de **Big Data** (grandes volúmenes de datos). Desde tu teléfono hasta los sensores de un coche autónomo, todo genera datos constantemente. Estos datos son enormes, complejos y a menudo tienen interacciones "ocultas" que no son evidentes a simple vista.

La estadística computacional es crucial porque nos permite **entender y extraer valor de estos sistemas de Big Data** en un entorno que es inherentemente complejo. Sin ella, nos ahogaríamos en la información.

---

### Los 3 Pilares Fundamentales de la Estadística Computacional

Para dominar la estadística computacional, te basarás en tres áreas técnicas clave:

1.  **Programación:** Aquí es donde pones tus ideas en acción. Aprenderás a escribir código, muy probablemente en el lenguaje **R** (que es muy popular para la estadística), para implementar las fórmulas y algoritmos estadísticos. No se trata de ser un experto programador al principio, sino de entender cómo usar el código para resolver problemas y, muy importante, cómo **reutilizar código** que ya funciona para adaptar soluciones a nuevas necesidades. Piensa en ello como tener un "kit de herramientas" de código que puedes modificar.

2.  **Análisis Numérico:** Muchos problemas complejos, especialmente con Big Data, no tienen una solución matemática "exacta" y simple. O si la tienen, calcularla tomaría demasiado tiempo. El **análisis numérico** se encarga de encontrar **soluciones aproximadas pero muy precisas** utilizando métodos computacionales. Por ejemplo, en lugar de resolver una ecuación enorme de forma manual, una computadora puede usar técnicas numéricas para llegar a una respuesta muy cercana en segundos. Esto es vital en campos como la biología, medicina o ciberseguridad, donde los datos son masivos y las interacciones complejas.

3.  **Estadística Clásica:** Aunque uses computadoras, la base sigue siendo la estadística de toda la vida. Esto incluye:
    * **Estadística Descriptiva:** Cómo resumir y visualizar datos univariados (de una sola variable), como la media, la mediana o la desviación estándar.
    * **Estadística Inferencial:** Cómo sacar conclusiones sobre una población más grande basándote en una muestra de datos.
    * **Combinatoria y Teoría de Probabilidades:** Conceptos básicos que son el fundamento para entender cómo se comportan los datos.
    * **Modelos de Regresión:** Especialmente importantes para el análisis de sistemas con muchas variables. La **regresión lineal** (que busca relaciones directas entre variables) y los **modelos de regresión no paramétricos** (más flexibles y útiles para Big Data) son herramientas poderosas para entender dependencias y hacer predicciones.

---

### ¿Cómo se resuelven los problemas con Estadística Computacional?

Cuando te enfrentas a un problema de análisis de datos con estadística computacional, típicamente sigues estos tres pasos:

1.  **Modelado del Problema:** Primero, entiendes el problema en sí y lo traduces a un "modelo" o una representación que la computadora pueda entender. Esto implica definir qué datos necesitas, qué quieres lograr y qué tipo de análisis es el más adecuado.
2.  **Propuesta de Solución Numérica:** Luego, diseñas la estrategia para resolver ese modelo utilizando métodos numéricos. Piensas en los algoritmos y las técnicas computacionales que te permitirán llegar a una solución aproximada.
3.  **Implementación Numérica:** Finalmente, escribes el código (por ejemplo, en R) para llevar a cabo esa solución. Aquí es donde la computadora hace el trabajo pesado de calcular, procesar y visualizar los datos.

---

### En Resumen: La Fórmula del Éxito

La **estadística computacional** es la poderosa **unión de las matemáticas, la estadística y la programación** para **resolver problemas complejos y de alta dimensionalidad** (muchos datos y mucha complejidad) en el mundo real, especialmente en la era del Big Data. Te permite transformar grandes volúmenes de datos en información útil y actionable.

## 2.3. Ámbitos de aplicación

### ¿Qué es la Estadística Computacional en pocas palabras?

La **estadística computacional** es justo eso: el uso de computadoras, algoritmos y programación para resolver problemas estadísticos que serían imposibles o muy lentos de hacer a mano. No es solo usar una calculadora avanzada, sino que permite abordar retos nuevos y complejos que antes no tenían solución, ¡y esto a su vez genera nuevas preguntas e ideas!

---

### Ámbitos de Aplicación: ¿Dónde la vemos en acción?

Este texto destaca que la estadística computacional es un campo **nuevo, dinámico y en constante crecimiento**, con muchas oportunidades. Se está volviendo esencial en diversas áreas:

#### 1. Biología y Salud: Bioestadística Computacional y Medicina Personalizada

* **¿Qué es?** Es la unión de la estadística computacional con campos como la bioinformática (manejo de datos biológicos), la genómica (estudio de genes) y la biotecnología (uso de organismos vivos para tecnología). También se le conoce como **informática o ingeniería de datos para biomedicina**.
* **¿Por qué es importante aquí?** En estas áreas se generan **enormes volúmenes de datos** (Big Data) y de tipos muy variados (por ejemplo, secuencias genéticas, resultados de pruebas médicas, imágenes). Analizar estos datos requiere herramientas muy eficientes que la estadística computacional provee.
* **Ejemplo práctico:**
    * **Análisis del genoma humano:** Imagina que los científicos quieren predecir si una persona tiene mayor riesgo de desarrollar una enfermedad en el futuro basándose en su ADN. Necesitan comparar el genoma de esa persona con el de miles o millones de otras personas. La bioestadística computacional, junto con técnicas de **machine learning**, permite procesar y analizar esta gigantesca cantidad de información genética para encontrar patrones y hacer predicciones. Esto lleva a la **medicina personalizada**, donde los tratamientos se adaptan al perfil genético único de cada paciente.

---

#### 2. Ciberseguridad

* **¿Qué es?** La ciberseguridad se encarga de proteger los sistemas informáticos y los datos de ataques, daños o accesos no autorizados. Con la inmensa cantidad de datos que se mueven en internet (tráfico de datos), es crucial asegurar esta información.
* **¿Por qué es importante aquí?** Para proteger los datos, es necesario **monitorear, registrar y codificar** las transferencias de información en la red. Estos procesos generan **series de números que cambian con el tiempo** (piensa en el volumen de datos que entra y sale de una red en cada segundo). La estadística computacional es vital para:
    * **Detectar anomalías:** Identificar patrones inusuales en el tráfico de datos que podrían indicar un ciberataque, malware (software malicioso) o intentos de desestabilización (como noticias falsas o ataques de denegación de servicio).
    * **Desarrollar métodos de seguridad:** Crear algoritmos y sistemas que cifren la información, autentiquen usuarios y detecten amenazas de forma proactiva.
* **Ejemplo práctico:**
    * Imagina un banco que necesita proteger las transacciones de sus clientes. Un estadístico computacional podría desarrollar un modelo que analice el volumen, la frecuencia y el tipo de transacciones en tiempo real. Si de repente hay un pico inusual en transacciones pequeñas y rápidas desde una ubicación geográfica extraña, el sistema, usando estadística computacional, podría alertar de un posible fraude o ataque, bloqueando las transacciones sospechosas y protegiendo a los clientes. Esto implica analizar **series temporales de datos** (cómo los datos evolucionan a lo largo del tiempo) para encontrar patrones de comportamiento normal y desviaciones.

---

### Puntos Clave para Recordar y Aplicar

* **Estadística Computacional (EC) = Estadística + Computación.** Permite resolver problemas complejos y afrontar desafíos con **grandes volúmenes de datos (Big Data)** que antes eran imposibles.
* Es un **campo en crecimiento** con muchas oportunidades y un impacto significativo.
* **Áreas principales mencionadas:**
    * **Bioestadística Computacional / Medicina Personalizada:** Crucial para analizar datos biológicos y genéticos masivos, llevando a avances en salud y tratamientos personalizados (ej. predicción de enfermedades por genoma).
    * **Ciberseguridad:** Indispensable para monitorear y proteger el tráfico de datos, detectando anomalías y desarrollando métodos de seguridad basados en el análisis de datos que cambian con el tiempo (series temporales).
* La EC no solo resuelve problemas existentes, sino que también **genera nuevos desafíos y oportunidades de desarrollo**.

---


## 2.4. Técnicas básicas de programación

### Técnicas Básicas de Programación: Escribiendo Código Inteligente

Cuando hablamos de "buenas prácticas de programación", nos referimos a un conjunto de reglas y consejos que te ayudan a escribir código que no solo funciona, sino que es **fácil de entender, mantener y reutilizar**. Esto es crucial, especialmente cuando estás desarrollando soluciones estadísticas, ya que tus modelos y análisis pueden volverse complejos rápidamente.

El texto menciona tres pilares fundamentales que son aplicables a cualquier lenguaje de programación (como R, Python, etc.):

-----

### 1\. Prestar Atención a la Sintaxis: La "Expresividad" del Lenguaje

  * **¿Qué significa?** Se refiere a hacer que tu código sea lo más **claro y legible** posible. Es como escribir un libro: no solo quieres que las oraciones tengan sentido gramatical, sino que también fluyan y sean fáciles de seguir para el lector. En programación, esto se logra usando nombres de variables, funciones y comentarios que reflejen lo que realmente hacen.
  * **¿Por qué es importante?** Un código expresivo puede ser entendido por ti (meses después de haberlo escrito) y por otros colegas, incluso si no son expertos en programación. Esto reduce errores y acelera el desarrollo.
  * **Ejemplo práctico:**
      * **Mal ejemplo:** `x = c(85, 92, 78, 65, 90)`
          * ¿Qué significa `x`? ¿Son edades, pesos, o algo más? No es claro.
      * **Buen ejemplo:** `notas_alumnos = c(85, 92, 78, 65, 90)`
          * Aquí, **`notas_alumnos`** deja muy claro qué tipo de datos contiene la variable.
      * **Consejo adicional:** Para códigos más largos, al inicio del archivo, es una excelente práctica incluir un "encabezado" o "cuadro de texto" con comentarios que describan brevemente:
          * Qué hace el código.
          * Quién lo escribió y la fecha.
          * Una lista de las variables principales y su significado.

-----

### 2\. Seccionar el Programa: Facilitando la Validación

  * **¿Qué significa?** Consiste en dividir tu código en **partes lógicas más pequeñas**. En lugar de tener un bloque gigante de código que hace muchas cosas a la vez, lo separas en secciones que realizan tareas específicas.
  * **¿Por qué es importante?** Facilita la **validación y depuración (debugging)** del código. Si solo una parte falla, sabes exactamente dónde buscar el error. Si todo está junto, encontrar el problema es como buscar una aguja en un pajar.
  * **Ejemplo práctico:**
      * Imagina que estás desarrollando un análisis estadístico que involucra los siguientes pasos:

        1.  **Cargar los datos.**
        2.  **Limpiar y preprocesar los datos** (manejar valores faltantes, transformar variables).
        3.  **Calcular estadísticas descriptivas** (media, mediana, desviación estándar).
        4.  **Realizar un análisis de regresión.**
        5.  **Generar gráficos y reportes.**

      * En lugar de escribir todo en un solo bloque, deberías usar comentarios (o incluso funciones, como veremos en el siguiente punto) para indicar claramente cada sección:

        ```r
        # --- 1. Carga de Datos ---
        datos_ventas <- read.csv("ventas.csv")

        # --- 2. Preprocesamiento de Datos ---
        datos_ventas_limpios <- na.omit(datos_ventas) # Eliminar filas con NA
        datos_ventas_limpios$mes <- factor(datos_ventas_limpios$mes) # Convertir a factor

        # --- 3. Cálculo de Estadísticas Descriptivas ---
        media_ventas <- mean(datos_ventas_limpios$monto)
        mediana_ventas <- median(datos_ventas_limpios$monto)

        # ... y así sucesivamente para las otras secciones
        ```

      * Si el cálculo de la media está mal, sabes que debes revisar la sección "Cálculo de Estadísticas Descriptivas".

-----

### 3\. Facilitar la "Modularidad": Reutilización de Código

  * **¿Qué significa?** Es un paso más allá de seccionar el programa. La modularidad implica diseñar tu código de forma que las partes (los "módulos") sean **independientes y puedan ser usadas en diferentes contextos o programas**. La forma más común de lograr modularidad en programación es a través de **funciones**.
  * **¿Por qué es importante?**
      * **Validación:** Al igual que seccionar, facilita encontrar errores porque cada función se puede probar por separado.
      * **Reutilización:** Una vez que has escrito y probado una función para una tarea específica, puedes usarla una y otra vez en diferentes proyectos sin tener que reescribirla. Esto ahorra tiempo y reduce la probabilidad de introducir nuevos errores.
      * **Mantenimiento:** Si necesitas cambiar cómo se realiza una tarea, solo tienes que modificar la función una vez, y el cambio se reflejará en todos los lugares donde se use esa función.
  * **Ejemplo práctico:**
      * Imagina que frecuentemente necesitas calcular el **coeficiente de variación** (una medida de dispersión relativa) para diferentes conjuntos de datos. En lugar de escribir la fórmula cada vez, puedes crear una función:

        ```r
        # Función para calcular el coeficiente de variación
        calcular_coef_variacion <- function(datos) {
          media <- mean(datos, na.rm = TRUE)
          desviacion_estandar <- sd(datos, na.rm = TRUE)
          if (media == 0) {
            return(NA) # Evitar división por cero
          }
          coef_variacion <- (desviacion_estandar / media) * 100
          return(coef_variacion)
        }

        # Ahora puedes usar esta función en cualquier parte de tu código o en otros proyectos:
        ventas_mes1 <- c(100, 120, 110, 130, 90)
        ventas_mes2 <- c(50, 55, 60, 45, 52)

        cv_mes1 <- calcular_coef_variacion(ventas_mes1)
        cv_mes2 <- calcular_coef_variacion(ventas_mes2)

        print(paste("Coeficiente de Variación Mes 1:", cv_mes1, "%"))
        print(paste("Coeficiente de Variación Mes 2:", cv_mes2, "%"))
        ```

      * Si en el futuro quieres cambiar cómo se calcula el coeficiente de variación (quizás quieras añadir un manejo diferente para valores atípicos), solo tienes que modificar la función `calcular_coef_variacion` y todos los lugares donde la uses se actualizarán automáticamente.

-----

### Puntos Clave para Recordar y Aplicar

  * **Buenas Prácticas = Código más claro, mantenible y eficiente.**
  * **Expresividad:** Usa nombres descriptivos para variables y elementos del código (`notas_alumnos` en lugar de `x`). Incluye comentarios claros, especialmente al inicio del código, para explicar su propósito y las variables clave.
  * **Seccionar el Programa:** Divide tu código en bloques lógicos y etiquétalos (usando comentarios) para facilitar la revisión y encontrar errores rápidamente.
  * **Modularidad:** Crea **funciones** para tareas específicas. Esto permite **reutilizar** código, simplifica la validación y mejora el mantenimiento del software.
  * Estas prácticas son la base para empezar a programar de manera efectiva, especialmente en el desarrollo de software estadístico.

-----

## 2.5. Presentación del software «R»

## Notas 
- Fue creado en 1993 
- Fue creado por dos estadisticos [Ross y Robert]
- Es el estandar en la ciencia de Datos
- Usado por Google y IBM
- Permite los analista convertir grandes volumenes en datos para tomar decisiones 
- Es un Lenguaje de programación para analisis estadistico y grafico
- Su base es lenguaje S creado por John Chambers en los 70s en laboratorios Bell Labs 
- Es código abierto 
- Ofrece herramientas para transformar limpiar y visualizar información 

## Caracteristicas 
- Extensibilidad a traves de paquetes -> Mas de 18k paquetes integrados 
- Capacidad para manejar grandes volumnes de datos -> Integración con Hadoop y spark 
- Fuerte soporte pra gráficos y visualización de datos -> fuerte vinculación con ggplot2, lattice y plotly  
- Ofrece flexibilidad por integrarse con otras herramientas como Python, SQL y Excel 

## Usos 
- Análisis estadisticos -> Permite generar Modelado Lineal y No Líneal 
    - Pruebas Estaditicas de Datos
    - Series temporadas 
- Visualización de datos -> Generación de graficos avanzados 
    - ggplot2, lattice y plotly 
    - Gráficos interactivos y mapas de calor
- Uso en la Ciencia de datos y machine learning 
    - Herramientas especializadas 
        - CC -> Limpiar tranformar y presnetar datos 
        - ML -> Soporte para Entrenamiento guiado y no Guiado 
            - Implementación Arboles de Decisiones, Ramdon Forest y Redes Neuronales 





### Presentación del Software "R": Tu Compañero para el Análisis de Datos

**R** es mucho más que un simple programa; es un **lenguaje de programación y un entorno** diseñado específicamente para la computación estadística y los gráficos. Es una de las herramientas más populares y poderosas en el mundo de la ciencia de datos.

-----

### Características Clave de R

1.  **Es Abierto (Open Source) y Gratuito:**

      * Puedes descargarlo e instalarlo sin costo alguno desde su sitio web oficial: [www.r-project.org](https://www.r-project.org/).
      * Al ser "open source", significa que su **código fuente es público**. Esto permite a usuarios avanzados estudiar cómo funciona internamente y, si lo desean, contribuir a su desarrollo. Esto también fomenta una comunidad activa que constantemente lo mejora.

2.  **Completa Caja de Herramientas Estadísticas:**

      * R viene con implementaciones para **"todas" las herramientas estadísticas** imaginables. Desde las más básicas (como medias y medianas) hasta las más avanzadas (modelos complejos, análisis multivariante).
      * Si una herramienta específica es muy nueva o de nicho, es muy probable que algún usuario la haya desarrollado y la haya puesto a disposición como una **"librería" (o paquete)** de acceso libre.

3.  **Potente Herramienta de Cálculo Numérico y Orientación a Objetos:**

      * R es muy eficiente para realizar cálculos matemáticos intensivos.
      * Su diseño de **programación orientada a objetos** (aunque no se profundiza en el texto, implica que puedes organizar tu código en estructuras que facilitan el trabajo con datos complejos) le permite manejar distintos formatos de datos externos de manera eficaz.

4.  **Capacidad de "Hibridación" con Otros Lenguajes:**

      * R puede interactuar y compartir librerías con otros lenguajes de programación de alto rendimiento como **C, C++ y Fortran**. Esto es útil cuando necesitas realizar operaciones muy específicas o de muy alta velocidad que R por sí solo podría no optimizar tan bien.

5.  **Flexible, Reproducible, Código Abierto e Interfaces de Línea de Comandos:**

      * **Flexible:** Puede resolver prácticamente cualquier problema estadístico y puedes crear tus propias funciones si las que existen no son suficientes.
      * **Reproducible:** Gracias a las buenas prácticas de programación (código claro y modular), puedes reutilizar tu código con diferentes conjuntos de datos, garantizando que tus análisis sean consistentes y puedan ser replicados por otros.
      * **Código Abierto:** Como se mencionó, permite la identificación de errores y la introducción de mejoras por parte de la comunidad.
      * **Línea de Comandos:** Trabajar con comandos te da un control superior, permitiéndote personalizar y optimizar tus análisis más allá de lo que una interfaz gráfica predefinida podría ofrecer.

-----

### Primeros Pasos: Instalación y Estructuras Básicas en R

El objetivo principal de R es leer, manipular, operar y guardar datos. Es muy versátil y puede trabajar con diversos tipos de datos.

#### 1\. Instalación:

  * Simplemente descarga el **ejecutable** para tu sistema operativo desde [www.r-project.org](https://www.r-project.org/) e instálalo como cualquier otro programa.

#### 2\. Estructuras Básicas para Guardar Datos:

  * **Importar desde un archivo externo (`.txt`):**

      * **Paso 1: Preparar el archivo.** Crea un archivo de texto plano (ej. `edad.txt`) con tus datos. Por ejemplo:
        ```
        edad
        10
        20
        30
        10
        40
        80
        ```
      * **Paso 2: Establecer el directorio de trabajo.** En la consola de R, usa el comando `setwd()` para ir a la carpeta donde guardaste el archivo.
        ```r
        setwd("c:/PRACTICAS/") # Asegúrate de que esta ruta exista en tu computadora
        ```
      * **Paso 3: Leer el archivo.** Usa `read.table()` para cargar los datos en una variable (una "tabla" o "dataframe" en R).
        ```r
        TablaEdad <- read.table("edad.txt", dec=".", header=TRUE)
        ```
          * `dec="."`: Indica que el punto es el separador decimal.
          * `header=TRUE`: Indica que la primera fila del archivo contiene los nombres de las columnas.
          * Ahora, `TablaEdad` contendrá tus datos.

  * **Creación de Vectores:** Esta es una forma muy común y directa de guardar datos en R, especialmente para listas de elementos del mismo tipo.

      * Para crear una lista de edades directamente en R:
        ```r
        edad <- c(10, 20, 30, 10, 40, 80)
        ```
          * `c()` significa "concatenar" o "combinar".

#### 3\. Operaciones Básicas:

  * R puede funcionar como una **calculadora**:
    ```r
    4 + 5 # Resultado: 9
    ```
  * Puedes realizar todas las **operaciones algebraicas** conocidas y usar funciones matemáticas integradas (como exponenciales).

#### 4\. Uso de Librerías y Funciones:

  * R tiene **miles de funciones** incorporadas y disponibles a través de librerías.

  * **La rutina de trabajo:**

    1.  **Definir el problema:** ¿Qué necesitas resolver? (Ej. Recodificar una variable).
    2.  **Buscar soluciones existentes:** Investiga si R ya tiene una función o una librería que haga lo que necesitas. La documentación de R y las búsquedas en línea (a menudo usando términos en inglés como "R software recoding") son tus mejores aliados.
    3.  **Cargar la librería (si es necesario):** Si la función pertenece a una librería que no está cargada por defecto, debes cargarla con el comando `library()`.
    4.  **Usar la función:** Aplica la función con la sintaxis correcta.

  * **Ejemplo de Recodificación:**

      * Supongamos que tienes una variable de deportes con "si" y "no".
        ```r
        deportistas <- c("si", "no", "si", "si")
        ```
      * Para recodificar "si" a 1 y "no" a 0, necesitas la **librería `car`** y la función `recode()`:
        ```r
        library(car) # Cargar la librería "car"
        deportistas_recodificado <- recode(deportistas, "'si'=1; else=0")
        print(deportistas_recodificado) # Ver el resultado
        # Resultado esperado: 1, 0, 1, 1
        ```

#### 5\. Representación de Datos: Variables Categóricas y Numéricas

  * R maneja diferentes **clases de variables**, siendo las principales:

      * **Categóricas (o Nominales):** Representan categorías (ej. "aprobados", "suspensos", "rojo", "azul").
      * **Numéricas (o Cuantitativas):** Representan cantidades (ej. edad, altura, ingresos).

  * **Ejemplo con `summary()`:**

      * **Variable Categórica:**
        ```r
        notaUnir_categorica <- c(rep("aprobados", 20), rep("suspensos", 10))
        # rep() es una función útil para "reproducir" un valor varias veces
        summary(notaUnir_categorica)
        # Te mostrará la longitud, clase y modo de la variable. Para categóricas,
        # te indicará cuántas veces aparece cada categoría (20 "aprobados", 10 "suspensos").
        ```
      * **Variable Numérica:**
        ```r
        notaUnir_numerica <- c(rep(8, 20), rep(3, 10))
        summary(notaUnir_numerica)
        # Te mostrará estadísticas descriptivas clave: Mínimo, Primer Cuartil (25%),
        # Mediana (50%), Media, Tercer Cuartil (75%) y Máximo.
        ```
      * Recuerda que `summary()` no da todos los estadísticos; para otros (ej. desviación estándar), necesitarás funciones específicas.

-----

### Perspectivas y Limitaciones de R

  * **Amplio Potencial:** R puede usarse para cualquier problema estadístico y tipo de datos. Desde análisis descriptivos básicos y visualización, hasta operaciones avanzadas con bases de datos, **machine learning** y **modelado matemático avanzado**.
  * **Limitaciones (Hardware):**
      * **Memoria Operativa:** R trabaja principalmente en la RAM. Para cálculos muy extensos, es crucial **guardar el progreso periódicamente** en el disco duro para evitar pérdidas.
      * **Grandes Bases de Datos:** Aunque R es potente, algunas funciones pueden colapsar con bases de datos extremadamente grandes. Para esto, existen **paquetes auxiliares** que ayudan a manejar y dividir estos datos de forma más eficiente.

-----

### Conclusión y Motivación

R te permite ser más que un simple usuario; te da la capacidad de **usar y crear soluciones** en el campo emergente de la estadística computacional. Aunque su curva de aprendizaje puede no ser la más intuitiva al principio (requiere investigar la sintaxis y las funciones), el poder y la flexibilidad que ofrece lo convierten en una herramienta indispensable para cualquier científico de datos. La clave está en **definir tu problema, investigar las soluciones existentes y adaptar los ejemplos** a tus necesidades.


## Guía Rápida: Lo Esencial para Empezar

**R** es tu navaja suiza para el **análisis, visualización y modelado de datos**. Es **gratis, de código abierto** y super flexible.

### 1\. Preparando el Terreno: Instalación y Entorno

  * **Descarga e Instala R:** Ve a [www.r-project.org](https://www.r-project.org/) y descarga la versión para tu sistema operativo. Sigue los pasos de instalación.
  * **¡Recomendación clave\! Instala RStudio:** Aunque puedes usar R directamente, **RStudio** (también gratuito) es un entorno de desarrollo integrado (IDE) que hace la vida muchísimo más fácil. Te ofrece un editor de código, consola, visor de variables, gráficos y ayuda, todo en un mismo lugar. ¡Búscalo y descárgalo\!
  * **Directorio de Trabajo (`setwd()`):** Es tu "base de operaciones". Aquí R buscará y guardará tus archivos por defecto.
    ```r
    # Para saber dónde estás:
    getwd()

    # Para cambiar tu directorio (usa tus propias rutas, con barras /):
    setwd("C:/MisDocumentos/MaestriaR")
    ```

### 2\. Guardando y Manejando Datos: Estructuras Básicas

R trabaja con diferentes formas de guardar información. Aquí las más importantes para empezar:

  * **Vectores (`c()`):** Son listas de elementos del *mismo tipo* (todos números, todas palabras, etc.).
    ```r
    # Vector numérico (edades de personas)
    edades <- c(10, 20, 30, 10, 40, 80)

    # Vector de texto (respuestas "si/no")
    respuestas_deporte <- c("si", "no", "si", "si", "no")
    ```
  * **Data Frames:** Son como las tablas de Excel. Tienen filas y columnas, y cada columna puede ser un tipo diferente de datos (una columna de números, otra de texto). Es la estructura más común para tus **bases de datos**.
    ```r
    # Crear un Data Frame manualmente
    datos_alumnos <- data.frame(
      Nombre = c("Ana", "Juan", "Maria"),
      Edad = c(25, 30, 28),
      Notas = c(9.5, 8.0, 7.2)
    )

    # Ver los datos
    print(datos_alumnos)

    # Acceder a una columna específica (ej. la columna Edad)
    print(datos_alumnos$Edad)
    ```
  * **Importar Datos Externos:** Lo más común es leer datos de archivos (CSV, TXT, Excel).
    ```r
    # Para archivos de texto o CSV (recuerda que el archivo debe estar en tu setwd() o indicar la ruta completa)
    mis_datos <- read.csv("mi_archivo.csv", header = TRUE, sep = ",")
    # 'header=TRUE' si la primera fila tiene nombres de columnas
    # 'sep=","' si las columnas están separadas por comas (usa ';', '\t', etc., según tu archivo)

    # Para Excel necesitas un paquete adicional (ver "Librerías")
    # install.packages("readxl")
    # library(readxl)
    # mis_datos_excel <- read_excel("mi_archivo.xlsx")
    ```

### 3\. ¡Manos a la Obra\! Operaciones y Funciones

R es como una súper calculadora con miles de trucos.

  * **Operaciones Básicas:**
    ```r
    4 + 5       # Suma
    10 - 3      # Resta
    2 * 6       # Multiplicación
    15 / 3      # División
    2^3         # Potencia (2 elevado a 3)
    sqrt(16)    # Raíz cuadrada
    log(10)     # Logaritmo natural
    ```
  * **Funciones para Entender tus Datos (`summary()`):**
    ```r
    # Para un vector numérico (te da min, max, media, mediana, cuartiles)
    notas <- c(8, 6, 9, 7, 5, 10)
    summary(notas)

    # Para un vector de texto (te da conteos de cada categoría)
    sexo <- c("M", "F", "F", "M", "M", "F")
    summary(sexo) # Mostrará el conteo de "F" y "M"

    # Para un Data Frame (te da un resumen por cada columna)
    summary(datos_alumnos)
    ```
  * **Librerías (`library()`):** R es modular. Las funciones avanzadas están organizadas en "paquetes" o "librerías". Debes instalarlos una vez y luego cargarlos cada vez que inicies una sesión de R y los necesites.
    ```r
    # Instalar una librería (solo la primera vez)
    install.packages("dplyr") # 'dplyr' es genial para manipular datos
    install.packages("ggplot2") # 'ggplot2' es increíble para gráficos

    # Cargar una librería (cada vez que inicias R y la vas a usar)
    library(dplyr)
    library(ggplot2)

    # Ejemplo de uso de una función de librería (con recodificación, como en el texto)
    # install.packages("car") # Instalar si no la tienes
    library(car)
    deportistas_recodificado <- recode(respuestas_deporte, "'si'=1; else=0")
    print(deportistas_recodificado)
    ```

### 4\. ¡Siempre Buenas Prácticas\!

  * **Nombres Claros:** Usa nombres que digan qué son (`edad_alumnos` en vez de `x`).
  * **Comentarios (`#`):** Explica tu código.
    ```r
    # Este es un comentario: R ignora todo lo que está después del '#'
    # Cargar los datos de ventas para el análisis
    ventas_mensuales <- read.csv("ventas.csv")
    ```
  * **Seccionar Código:** Organiza tu script con comentarios para separar pasos (carga de datos, limpieza, análisis).
  * **Funciones Personalizadas (Modularidad):** Si haces algo repetidamente, crea tu propia función.
    ```r
    # Función para calcular el porcentaje
    calcular_porcentaje <- function(parte, total) {
      resultado <- (parte / total) * 100
      return(resultado)
    }

    # Usar tu función
    porcentaje_aprobados <- calcular_porcentaje(20, 30)
    print(porcentaje_aprobados)
    ```

-----

## Mapa Mental: Guía R para Análisis de Datos

Imagina el centro de tu mapa mental como **"R para Análisis de Datos"**. De ahí, saldrán las ramas principales:

  * **R Para Análisis de Datos**
      * \--- **1. Fundamentos de R**
          * **¿Qué es?** (Lenguaje y entorno, Open Source, Gratuito)
          * **Instalación** (R, RStudio)
          * **Características Clave** (Estadísticas completas, Hibridación C/C++/Fortran, OO, Flexible, Reproducible, Código Abierto)
      * \--- **2. Estructuras de Datos**
          * **Vectores** (Listas, un solo tipo de dato, `c()`)
              * *Ejemplo:* `edades <- c(10, 20, 30)`
          * **Data Frames** (Tablas, filas/columnas, distintos tipos por columna, `data.frame()`)
              * *Ejemplo:* `datos_alumnos`
          * **Importación** (`read.csv()`, `read.table()`, `read_excel()`)
              * *Concepto:* `setwd()`
      * \--- **3. Operaciones y Funciones**
          * **Básicas** (+, -, \*, /, `sqrt()`, `log()`)
          * **Descriptivas** (`summary()`)
              * *Variables Categóricas*
              * *Variables Numéricas*
          * **Librerías/Paquetes** (`install.packages()`, `library()`)
              * *Ejemplo:* `car` para `recode()`, `dplyr` para manipulación, `ggplot2` para visualización
      * \--- **4. Buenas Prácticas de Programación**
          * **Expresividad** (Nombres claros, Comentarios `#`)
          * **Seccionar Código** (Bloques lógicos)
          * **Modularidad** (Funciones personalizadas)
      * \--- **5. Aplicaciones y Limitaciones**
          * **Usos** (Descriptivos, Visualización, Machine Learning, Modelado Avanzado)
          * **Limitaciones** (RAM para Big Data, Usar paquetes auxiliares, Guardar progreso)

# Para seguir la guia de R tenemos un curso [Curso de R](../Curso_R.md)

# Tema 3. Medidas que resumen la información

##  3.2. Medidas de tendencia central

¡Hola! Me parece excelente tu iniciativa de buscar apoyo para comprender a fondo tu maestría. Con gusto te ayudaré a asimilar la información de la mejor manera posible, utilizando ejemplos claros y fáciles de recordar.

Vamos a desglosar el Tema 3: "Medidas que resumen la información", enfocándonos en las **Medidas de Tendencia Central**.

---

### ¿Qué son las Medidas de Tendencia Central?

Imagina que tienes una gran cantidad de datos, como las calificaciones de todos los alumnos de una clase o los salarios de todos los empleados de una empresa. Las medidas de tendencia central son como el "representante" de esos datos, un valor que se encuentra en el medio y que nos da una idea de dónde se agrupan la mayoría de ellos. Son una forma rápida de resumir un montón de números en uno solo que nos dice "¡Hey, por aquí va la cosa!".

Las tres medidas principales que estudiaremos son:

1.  **La Media (o Promedio)**
2.  **La Mediana**
3.  **La Moda**

---

### 1. La Media (o Promedio): El "Repartidor Equitativo"

Piensa en la media como si fueras a repartir algo de forma completamente equitativa entre un grupo de personas.

**¿Cómo se calcula?**
Simplemente sumas todos los valores y luego los divides por la cantidad total de valores que tienes.

* **Fórmula básica:** $\bar{x} = \frac{\sum x_i}{n}$
    * $\sum x_i$ es la suma de todos los valores individuales.
    * $n$ es el número total de valores.

**Ejemplo simple para recordar:**
Imagina que tienes 4 amigos y cada uno trae dulces:
* Amigo 1: 5 dulces
* Amigo 2: 7 dulces
* Amigo 3: 3 dulces
* Amigo 4: 9 dulces

Para saber cuántos dulces tendría cada uno si los repartieran equitativamente (la media), harías:
$(5 + 7 + 3 + 9) / 4 = 24 / 4 = \textbf{6 dulces por amigo}$

---

**Cuando los datos se repiten (Frecuencia):**
Si tienes muchos datos y algunos valores se repiten, no tienes que sumarlos uno por uno. Puedes multiplicar cada valor por las veces que aparece (su frecuencia) y luego sumar esos resultados, dividiendo por el total de datos.

* **Fórmula con frecuencia:** $\bar{x} = \frac{\sum (x_i \cdot f_i)}{N}$
    * $x_i$ es cada valor único.
    * $f_i$ es la frecuencia (cuántas veces se repite) de cada valor $x_i$.
    * $N$ es el número total de datos.

---

**La Media Ponderada: Cuando no todo pesa igual**

A veces, no todos los datos tienen la misma importancia. Piensa en tus calificaciones: un examen final generalmente "pesa" más que una tarea pequeña. La media ponderada toma esto en cuenta, multiplicando cada valor por un "peso" o importancia.

* **Fórmula:** $\bar{x}_p = \frac{\sum (x_i \cdot w_i)}{\sum w_i}$
    * $x_i$ es cada valor.
    * $w_i$ es el peso o ponderación de cada valor $x_i$.

**Ejemplo para recordar:**
Imagina que tus calificaciones en una materia son:
* Tarea 1: 80 (peso del 10%)
* Examen Parcial: 70 (peso del 30%)
* Examen Final: 90 (peso del 60%)

Tu promedio ponderado sería:
$((80 \times 0.10) + (70 \times 0.30) + (90 \times 0.60)) / (0.10 + 0.30 + 0.60)$
$(8 + 21 + 54) / 1 = \textbf{83}$

Fíjate cómo el 90 (Examen Final) tiene más impacto en tu calificación final porque su peso es mayor.

---

**Media Armónica: ¡Velocidades y proporciones!**

Aunque es menos común, la media armónica se usa en casos muy específicos, como cuando trabajas con velocidades o tasas. La clave es que **ningún valor puede ser cero**.

**Ejemplo 1 (del texto):**
* De Madrid a Barcelona: 100 km/h
* De Barcelona a Madrid: 120 km/h

Si intentaras usar la media aritmética simple $((100 + 120) / 2 = 110)$, no sería correcto para la velocidad media en un viaje de ida y vuelta. La media armónica es la adecuada aquí.

La fórmula es: $\frac{n}{\sum (1/x_i)}$ donde n es el número de valores.
Para el ejemplo: $2 / ((1/100) + (1/120)) = 2 / (0.01 + 0.008333) = 2 / 0.018333 = \textbf{109.09 km/h}$ (aproximadamente).

---

**La "Debilidad" de la Media:**

La media es muy útil, ¡pero tiene un punto débil! Es muy sensible a los **valores atípicos (outliers)**. Un outlier es un dato que es mucho más grande o mucho más pequeño que el resto.

**Ejemplo para recordar:**
Imagina el salario de 5 personas en una pequeña empresa:
* Persona 1: $1,000
* Persona 2: $1,200
* Persona 3: $1,100
* Persona 4: $1,300
* Persona 5 (el dueño): $100,000

Si calculas la media: $(1000 + 1200 + 1100 + 1300 + 100000) / 5 = 104600 / 5 = \textbf{$20,920}$

¿Crees que $20,920 es un salario "representativo" para la mayoría de los empleados? ¡Para nada! Ese $100,000 del dueño distorsiona completamente la media, haciéndola parecer mucho más alta de lo que realmente es para la mayoría. En este caso, la media no nos da una buena idea de lo que "normalmente" ganan.

---

### 2. La Mediana: El "Punto Medio" Robusto

La mediana es el valor que está **justo en el medio** de un conjunto de datos **ordenados**. Piensa en ella como la persona que se para en el centro de una fila después de que todos se han acomodado por altura.

**La gran ventaja de la mediana:** Es **mucho más robusta** (resistente) que la media a los valores atípicos. No le afectan tanto los valores extremos.

**¿Cómo se calcula?**

1.  **Ordena** todos los datos de menor a mayor (¡esto es crucial!).
2.  Busca el valor central.

**Casos:**

* **Si el número de observaciones es impar:** La mediana es el valor que está exactamente en el medio.
    * **Ejemplo para recordar:** Las edades de 5 amigos: 10, 12, **15**, 18, 20.
        * Después de ordenar: 10, 12, **15**, 18, 20
        * La mediana es **15**. (Hay 2 valores antes y 2 después).

* **Si el número de observaciones es par:** No hay un valor central exacto. La mediana es el **promedio de los dos valores centrales**.
    * **Ejemplo para recordar:** Las edades de 6 amigos: 10, 12, 15, 18, 20, 22.
        * Después de ordenar: 10, 12, **15**, **18**, 20, 22
        * Los dos valores centrales son 15 y 18.
        * La mediana es $(15 + 18) / 2 = \textbf{16.5}$.

---

**Retomando el ejemplo de los salarios con la mediana:**
Salarios: $1,000, $1,100, $1,200, $1,300, $100,000

1.  **Ordenar:** $1,000, $1,100, **$1,200**, $1,300, $100,000
2.  La mediana es **$1,200**.

¡Mira la diferencia! $1,200 es mucho más representativo del salario "típico" de la mayoría de los empleados que los $20,920 de la media. Por eso, en presencia de outliers, la mediana es tu mejor amiga.

---

**Mediana para datos agrupados en intervalos (¡Un poco más avanzado!):**

Cuando los datos están agrupados en rangos (ej. 0-10, 11-20), necesitamos una fórmula especial para estimar la mediana, ya que no tenemos los valores exactos. No te preocupes por memorizarla ahora, lo importante es entender cuándo se usa.

La fórmula involucra:
* El **intervalo mediano**: Es el grupo de datos donde "cae" la mediana (donde la frecuencia acumulada alcanza la mitad del total de datos).
* El límite inferior de ese intervalo.
* Su amplitud (qué tan grande es el rango).
* Su frecuencia.
* La frecuencia acumulada de los intervalos anteriores.

**Un punto clave de la Mediana:** A diferencia de la media, ¡la mediana puede usarse con **variables cualitativas ordinales**! Esto es, datos que tienen un orden pero no son números (ej. "nivel de satisfacción: bajo, medio, alto"). No puedes promediar "bajo" y "medio", pero sí puedes encontrar la categoría que está en el medio si las ordenas.

---

### 3. La Moda: El "Más Popular"

La moda es simplemente el valor que **más se repite** en un conjunto de datos. Es como el color de coche más común que ves en la calle.

**¿Cómo se calcula?**
Solo tienes que contar qué valor aparece con mayor frecuencia.

**Ejemplos para recordar:**

* **Unimodal (una moda):**
    * Colores de camisetas vendidas: Rojo, Azul, Rojo, Verde, Amarillo, Rojo, Azul.
    * La moda es **Rojo** (aparece 3 veces).

* **Bimodal (dos modas):**
    * Número de hermanos por familia: 1, 2, 1, 3, 2, 4, 1, 2.
    * La moda es **1 y 2** (ambos aparecen 3 veces).
    * Una distribución bimodal significa que hay dos "picos" o grupos comunes en tus datos.

* **Multimodal (varias modas):**
    * Si hubiera tres o más valores con la misma frecuencia máxima. Es menos común, pero puede pasar.

* **Sin moda:**
    * Si todos los valores aparecen solo una vez o con la misma frecuencia.
    * Números: 1, 2, 3, 4, 5. No hay moda.

---

### En Resumen: Elige la Medida Correcta

* **Media:** Tu primera opción, el promedio. Útil cuando los datos son simétricos y no hay muchos outliers. ¡Cuidado con los extremos!
* **Mediana:** La "media" de los datos ordenados. ¡Tu mejor amiga cuando hay outliers! Funciona muy bien para datos con valores extremos o distribuciones asimétricas. También sirve para datos cualitativos ordinales.
* **Moda:** El valor que más se repite. Útil para identificar el elemento más común y es la única medida de tendencia central que puedes usar con **datos cualitativos nominales** (ej. colores, tipos de frutas, que no tienen un orden).

## 3.3. Medidas de tendencia central robustas

### ¿Por qué necesitamos Medidas de Tendencia Central Robustas?

Como vimos en la sesión anterior, la **media aritmética** es como una balanza: si pones un peso muy grande en un extremo (un **outlier**), la balanza se inclina y ya no te da una idea del "peso" promedio de la mayoría de los objetos. Es muy sensible a esos valores extremos.

La **mediana**, por otro lado, es naturalmente robusta a los outliers. Simplemente encuentra el valor del medio y no le importa cuán grandes o pequeños sean los extremos. Sin embargo, su interpretación a veces no es tan intuitiva como la de la media, y queremos una medida que sea "parecida" a la media pero que no se vea afectada por esos valores atípicos.

Aquí es donde entran en juego las **medidas de tendencia central robustas**, que son variaciones de la media diseñadas para ignorar o mitigar el impacto de los outliers. Nos centraremos en dos: la **media recortada** y la **media winsorizada**.

---

### 1. Media Recortada (Trimmed Mean): "Cortando los Extremos"

Imagina que tienes una fila de personas ordenadas por altura, y quieres calcular la altura promedio de la "gente normal" sin que los gigantes o los enanos extremos afecten el resultado. Lo que harías es "recortar" a los más altos y a los más bajos de los extremos y luego promediar a los que quedan en el medio.

Eso es exactamente lo que hace la media recortada: **elimina un cierto porcentaje de datos de cada extremo** del conjunto (una vez ordenados) y luego calcula la media con los datos restantes.

* **¿Cómo funciona?**
    1.  **Ordena** todos los datos de menor a mayor.
    2.  Decide un **porcentaje de recorte** (por ejemplo, 10%).
    3.  Calcula cuántos datos corresponden a ese porcentaje en cada extremo.
    4.  **Elimina** esa cantidad de datos de la parte inferior y de la parte superior del conjunto ordenado.
    5.  Calcula la **media aritmética** de los datos restantes.

* **Notación:** Se suele decir "media recortada al Y%", donde Y es el porcentaje a eliminar de *cada* lado.
    * Si Y = 0%, la media recortada es igual a la media aritmética normal (no se recorta nada).
    * Si Y = 25%, se le llama **"centrimedia"**.

**Ejemplo para recordar (el mismo del texto, pero explicado en pasos):**
Tienes los siguientes datos y te piden calcular una media recortada al 10%:
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] (¡Ya están ordenados!)

1.  **Número total de datos (n):** 10
2.  **Porcentaje de recorte:** 10%
3.  **Cantidad de datos a recortar por cada lado:** 10% de 10 datos = $0.10 \times 10 = \textbf{1 dato}$ por cada lado.
4.  **Eliminar:** Quitas el 1 (el más pequeño) y el 10 (el más grande).
    Los datos que quedan son: [2, 3, 4, 5, 6, 7, 8, 9]
5.  **Calcular la media de los datos restantes:**
    $(2 + 3 + 4 + 5 + 6 + 7 + 8 + 9) / 8 = 44 / 8 = \textbf{5.5}$

En este caso particular del ejemplo del texto, el resultado da 6, pero es porque el ejemplo del texto es el siguiente:
[1, 2, 3, 4, 5, 7, 8, 9, 10, 11]
Si aplicamos el recorte al 10% de estos datos:
1.  **Ordenar:** [1, 2, 3, 4, 5, 7, 8, 9, 10, 11]
2.  **Recortar 1 dato de cada lado:**
    Quitamos el 1 y el 11.
    Los datos restantes son: [2, 3, 4, 5, 7, 8, 9, 10]
3.  **Media:** $(2+3+4+5+7+8+9+10) / 8 = 48 / 8 = \textbf{6}$

¡Ahí está el 6 del ejemplo! Es importante el conjunto de datos específico que se usa.

* **Nivel de recorte:** A veces, en lugar de porcentajes, se habla de "nivel". Si decimos "media recortada de nivel 1", significa que se elimina 1 dato de cada extremo. "Nivel 2" eliminaría 2 datos de cada extremo, y así sucesivamente.

---

### 2. Media Winsorizada (Winsorized Mean): "Reemplazando los Extremos"

La media winsorizada es como la media recortada, pero en lugar de simplemente eliminar los valores extremos, los **reemplaza** por los valores más cercanos que *no* fueron eliminados. Piensa que los outliers son "demasiado grandes" o "demasiado pequeños", así que los "normalizamos" al valor más extremo que consideramos aceptable.

* **¿Cómo funciona?**
    1.  **Ordena** todos los datos de menor a mayor.
    2.  Decide un **porcentaje o nivel de "winsorización"**.
    3.  Calcula cuántos datos extremos deben ser afectados en cada lado.
    4.  **Reemplaza** los valores más pequeños (inferiores) por el valor más pequeño *no winsorizado*.
    5.  **Reemplaza** los valores más grandes (superiores) por el valor más grande *no winsorizado*.
    6.  Calcula la **media aritmética** de *todos* los datos, incluyendo los reemplazados.

**Ejemplo para recordar (el mismo del texto):**
Datos originales: [1, 2, 3, 4, 5, 7, 8, 9, 10, 11]
Nos piden calcular la media winsorizada de **nivel 2**. Esto significa que los 2 valores más pequeños y los 2 valores más grandes serán modificados.

1.  **Ordenar:** [1, 2, 3, 4, 5, 7, 8, 9, 10, 11]
2.  **Identificar los valores a winsorizar:**
    * Los 2 más pequeños: 1 y 2.
    * Los 2 más grandes: 10 y 11.
3.  **Identificar los valores de reemplazo:**
    * El valor más pequeño *no winsorizado* es el 3.
    * El valor más grande *no winsorizado* es el 9.
4.  **Reemplazar:**
    * Reemplazamos 1 y 2 por 3.
    * Reemplazamos 10 y 11 por 9.
    Los datos winsorizados son ahora: [3, 3, 3, 4, 5, 7, 8, 9, 9, 9]
5.  **Calcular la media de los datos winsorizados:**
    $(3+3+3+4+5+7+8+9+9+9) / 10 = 60 / 10 = \textbf{6.0}$

El ejemplo del texto dice 5.9, lo que sugiere que podría haber un pequeño error de redondeo en el texto o una diferencia en la forma en que aplicaron el nivel 2. **Si seguimos la lógica de "nivel 2" como 2 valores por cada extremo, el resultado es 6.0.**

Vamos a revisar el ejemplo del texto tal cual:
"La media winsorizada de nivel 2 del ejemplo visto consistiría en realizar una media aritmética sobre el siguiente conjunto de datos:
[2, 2, 3, 4, 5, 7, 8, 9, 9, 9] El resultado en este caso nos da un valor de 5,9."
Si sumamos los valores que ellos proponen: $(2+2+3+4+5+7+8+9+9+9) / 10 = 58 / 10 = \textbf{5.8}$
Esto indica que en su ejemplo, ellos reemplazaron el 1 por 2, y el 10 y 11 por 9. Si el "nivel 2" significa "reemplazar los 2 valores más pequeños y los 2 valores más grandes", entonces lo correcto sería reemplazar 1 y 2 por el 3, y 10 y 11 por el 9, como hicimos en mi explicación. La ligera discrepancia es importante notarla, pero el **concepto fundamental** de reemplazar es el mismo. Lo que sí es claro es que el 5.9 que menciona el texto no se obtiene con sus propios números [2, 2, 3, 4, 5, 7, 8, 9, 9, 9] ya que la suma es 58 y dividido por 10 da 5.8.

---

### ¿Cuándo usar cada una?

* **Media Recortada:** Es útil cuando quieres eliminar los outliers por completo, considerándolos ruido. Si tienes un subconjunto de datos muy ruidoso en los extremos y quieres enfocarte solo en el "centro" más típico.
* **Media Winsorizada:** Es preferible cuando no quieres "perder" datos, sino que quieres que los outliers sigan influyendo de alguna manera (limitada) en la media, al arrastrarlos hacia el centro de la distribución. Esto puede ser útil si crees que los outliers, aunque extremos, aún contienen cierta información relevante, pero su magnitud real distorsiona demasiado.

---

### Conclusión: La Importancia de la Robustez

La idea principal detrás de estas medias es precisamente **evitar que los valores atípicos (outliers) distorsionen la medida de tendencia central**. Al hacerlo, obtenemos un valor que es mucho más representativo de la "mayoría" de los datos, un valor de tendencia central que refleja los patrones más típicos y no los casos excepcionales.

Estas herramientas son muy valiosas en el análisis de datos reales, donde los errores de medición, los eventos inusuales o los datos anómalos son una constante.


## 3.4. Medidas de dispersión

### ¿Qué son las Medidas de Dispersión?

Si las medidas de tendencia central (media, mediana, moda) nos dicen dónde está el "centro" de nuestros datos, las **medidas de dispersión** nos dicen qué tan "esparcidos" o "agrupados" están esos datos alrededor de ese centro. Son como una linterna que nos ayuda a ver si los datos están muy juntos, formando un grupo apretado, o si están muy separados, abarcando un amplio rango de valores.

Imagina que dos equipos de baloncesto tienen la misma altura promedio (la media), pero en el Equipo A, todos miden casi lo mismo, mientras que en el Equipo B, hay jugadores muy bajitos y otros muy altos. Las medidas de dispersión nos ayudarían a ver esa diferencia.

Vamos a explorar las más importantes:

1.  **Rango**
2.  **Varianza**
3.  **Desviación Típica (o Estándar)**
4.  **Coeficiente de Variación**

---

### 1. Rango: La "Distancia Total"

El **rango** es la medida de dispersión más sencilla y directa. Simplemente nos dice la "distancia" entre el valor más pequeño y el valor más grande en nuestro conjunto de datos.

* **¿Cómo se calcula?**
    $\text{Rango} = \text{Valor Máximo} - \text{Valor Mínimo}$

**Ejemplo para recordar:**
Tienes las calificaciones de un examen en tu clase: 5, 7, 8, 9, 10.
* Valor Máximo = 10
* Valor Mínimo = 5
* Rango = $10 - 5 = \textbf{5}$

**Limitación:** El rango es fácil de calcular, pero tiene una gran debilidad: solo usa dos valores (el extremo superior e inferior) y **ignora por completo cómo se distribuyen los datos intermedios**. Por ejemplo, dos conjuntos de datos podrían tener el mismo rango pero ser muy diferentes por dentro.

---

### 2. Varianza: El "Promedio de las Desviaciones Cuadradas"

La **varianza** es una medida de dispersión mucho más completa que el rango porque toma en cuenta **todos los valores** en el conjunto de datos. La idea es ver cuánto se "desvía" cada dato individual de la media y luego promediar esas desviaciones.

* **El concepto clave:** Mide la **dispersión promedio** de los datos respecto a su **media**.
* **¿Por qué se eleva al cuadrado?** Cuando calculamos la diferencia de cada dato con la media, algunas diferencias serán positivas (el dato es mayor que la media) y otras serán negativas (el dato es menor que la media). Si las sumáramos directamente, se anularían entre sí, dando un resultado de cero. Al elevarlas al cuadrado, todas se vuelven positivas, y así podemos sumarlas para obtener una medida real de dispersión.

* **Fórmula (Varianza Poblacional):** $\sigma^2 = \frac{\sum (x_i - \mu)^2}{N}$
    * $\sigma^2$ (sigma al cuadrado) es la varianza poblacional.
    * $x_i$ es cada valor individual de los datos.
    * $\mu$ (mu) es la media de la población.
    * $N$ es el número total de datos en la población.

**Ejemplo simple para entender la idea:**
Imagina 3 personas con las siguientes edades: 10, 12, 14.
1.  **Calcula la media ($\mu$):** $(10 + 12 + 14) / 3 = 12$
2.  **Calcula la desviación de cada dato respecto a la media y elévala al cuadrado:**
    * $(10 - 12)^2 = (-2)^2 = 4$
    * $(12 - 12)^2 = (0)^2 = 0$
    * $(14 - 12)^2 = (2)^2 = 4$
3.  **Suma las desviaciones al cuadrado:** $4 + 0 + 4 = 8$
4.  **Divide por el número de datos (N):** $8 / 3 = \textbf{2.67}$ (Esta sería la varianza)

**Puntualización importante (el denominador n vs. n-1):**
Cuando trabajamos con una **muestra** de datos (que es lo más común), la fórmula de la varianza muestral divide por $(n-1)$ en lugar de $n$. Esto se hace para que el estimador de la varianza sea **"insesgado"**, es decir, que sea una estimación más precisa de la varianza real de la población. No te preocupes por los detalles ahora, solo recuerda que si ves $(n-1)$ es porque se está trabajando con una muestra.

---

### 3. Desviación Típica o Desviación Estándar ($\sigma$): La "Dispersión Promedio en Unidades Originales"

El problema con la varianza es que, al elevar las desviaciones al cuadrado, la unidad de medida de la varianza también está al cuadrado. Si tus datos son edades en años, la varianza estará en "años cuadrados", ¡lo cual es difícil de interpretar!

Para solucionar esto, simplemente tomamos la **raíz cuadrada de la varianza**. Esto nos devuelve a las unidades originales de los datos y es mucho más fácil de interpretar.

* **¿Cómo se calcula?** Es la raíz cuadrada positiva de la varianza.
    $\sigma = \sqrt{\sigma^2}$

* **Fórmula (Desviación Típica Poblacional):** $\sigma = \sqrt{\frac{\sum (x_i - \mu)^2}{N}}$

**Ejemplo simple (continuando con las edades):**
* Varianza ($\sigma^2$) = 2.67
* Desviación Típica ($\sigma$) = $\sqrt{2.67} = \textbf{1.63}$

Esto significa que, en promedio, las edades se desvían alrededor de 1.63 años de la media (12 años).

**Ejemplo 3 (del texto, con Gasol):**
El texto menciona una fórmula alternativa para calcular la varianza/desviación típica, que es computacionalmente más eficiente, especialmente con tablas de frecuencia:
$\sigma = \sqrt{\frac{\sum x_i^2 \cdot f_i}{N} - \mu^2}$
Esta fórmula es matemáticamente equivalente a la anterior y se usa cuando ya tienes la media y quieres una forma más directa de calcular la varianza, o cuando trabajas con datos agrupados o con frecuencias. Lo importante es que, al final, la desviación típica nos dará un número que representa la dispersión en las mismas unidades que los puntos de Gasol (puntos anotados).

---

### 4. Coeficiente de Variación (CV): Comparando la "Dispersión Relativa"

La desviación típica nos dice la dispersión en las unidades de los datos. Pero, ¿qué pasa si queremos comparar la dispersión de dos conjuntos de datos que están en **escalas o unidades de medida completamente diferentes**? Por ejemplo, comparar la dispersión de los pesos (en kg) y las estaturas (en metros) de un grupo de personas.

Aquí es donde entra el **coeficiente de variación**. Nos da una medida de la dispersión **relativa** a la media. Al dividir la desviación típica por la media, las unidades se "cancelan", dejándonos con un valor sin unidades que podemos usar para comparar.

* **¿Cómo se calcula?**
    $\text{CV} = \frac{\sigma}{\mu}$

**Ejemplo 4 (del texto):**
* **Variable 1: Estatura**
    * Media ($\mu_{\text{estatura}}$) = 1.68 m
    * Desviación Típica ($\sigma_{\text{estatura}}$) = 0.07 m
    * $\text{CV}_{\text{estatura}} = 0.07 / 1.68 = \textbf{0.0417}$ (o 4.17%)

* **Variable 2: Peso**
    * Media ($\mu_{\text{peso}}$) = 57 Kg
    * Desviación Típica ($\sigma_{\text{peso}}$) = 5 Kg
    * $\text{CV}_{\text{peso}} = 5 / 57 = \textbf{0.0877}$ (o 8.77%)

**Conclusión:**
Al comparar los CV, vemos que $\text{CV}_{\text{peso}}$ (0.0877) es casi el doble de $\text{CV}_{\text{estatura}}$ (0.0417). Esto significa que, **relativamente a su propia media**, el peso de las mujeres en esa población es casi dos veces más variable (o disperso) que su estatura. Aunque 5 kg es un número mayor que 0.07 m, el CV nos dice que esa variación es más significativa en el contexto del peso que en el de la estatura.

---

### Resumen General de Medidas de Dispersión:

* **Rango:** Rápido, pero ignora la mayoría de los datos y es muy sensible a los outliers.
* **Varianza ($\sigma^2$):** Toma en cuenta todos los datos, pero sus unidades están al cuadrado, lo que dificulta la interpretación directa. Es el paso intermedio para la desviación típica.
* **Desviación Típica ($\sigma$):** La más usada y fácil de interpretar. Mide la dispersión promedio en las mismas unidades que los datos originales. ¡Tu mejor amiga para entender la "escala" de la dispersión!
* **Coeficiente de Variación (CV):** Ideal para comparar la dispersión entre conjuntos de datos que tienen diferentes unidades o escalas. Nos da una idea de la dispersión "relativa".

Comprender la dispersión es tan importante como entender la tendencia central. Te permite ver si tu "centro" es representativo de un grupo homogéneo o de uno muy diverso.


## 3.5. Medidas de dispersión robustas
## 3.6. Medidas de posición y forma
## 3.7 Gráficos de caja
## 3.8 Datos atípicos y análisis exploratorio de datos

Materia 02 - Analisis Interpretación de datos - Tema 1

