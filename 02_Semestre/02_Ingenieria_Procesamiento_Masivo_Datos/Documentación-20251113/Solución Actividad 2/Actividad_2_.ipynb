{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15bfcfc880f2897e6836933d55c7c042",
     "grade": false,
     "grade_id": "cell-570cf80ae1b2c48e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 1: HDFS, Spark SQL y MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuerda borrar siempre las líneas que dicen `raise NotImplementedError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a238d899b5a6e8c414330ade880233f",
     "grade": false,
     "grade_id": "cell-f4c598b6fd61ee12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Lee con detenimiento cada ejercicio. Las variables utilizadas para almacenar las soluciones, al igual que las nuevas columnas creadas, deben llamarse **exactamente** como indica el ejercicio, o de lo contrario los tests fallarán y el ejercicio no puntuará. Debe reemplazarse el valor `None` al que están inicializadas por el código necesario para resolver el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10cdca7b33ec4328e542f94942371393",
     "grade": false,
     "grade_id": "cell-42368b0202b6ce77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Leemos el fichero flights.csv que hemos subido a HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b86205b9dc8b69adf7aa9ba76518864",
     "grade": false,
     "grade_id": "cell-3202a483f423590a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Indicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque después comprobaremos si lo\n",
    "ha inferido correctamente o no. La ruta del archivo en HDFS debería ser /<nombre_alumno>/flights.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5310bc8c3244ee98391957d26dc91d08",
     "grade": false,
     "grade_id": "lectura-fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# Ruta del archivo en HDFS\n",
    "ruta_hdfs = \"david_figueroa/flights.csv\"\n",
    "\n",
    "# Lectura del CSV desde HDFS\n",
    "flightsDF = spark.read.option(\"header\", \"true\") \\\n",
    "                      .option(\"inferSchema\", \"true\") \\\n",
    "                      .option(\"delimiter\", \"\\t\") \\\n",
    "                      .csv(f\"/{ruta_hdfs}\")\n",
    "\n",
    "# Comprobar cantidad de valores \"NA\" en dep_time\n",
    "cuantos_NA = flightsDF.where(F.col(\"dep_time\") == \"NA\").count()\n",
    "\n",
    "# Eliminar filas con \"NA\" en columnas clave\n",
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "flightsLimpiado = flightsDF\n",
    "for col in columnas_limpiar:\n",
    "    flightsLimpiado = flightsLimpiado.where(F.col(col) != \"NA\")\n",
    "flightsLimpiado.cache()\n",
    "\n",
    "# Convertir columnas a Integer y arr_delay a Double\n",
    "flightsConvertido = flightsLimpiado\n",
    "for col in columnas_limpiar:\n",
    "    flightsConvertido = flightsConvertido.withColumn(col, F.col(col).cast(IntegerType()))\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\n",
    "flightsConvertido.cache()\n",
    "flightsConvertido.printSchema()\n",
    "flightsConvertido.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el esquema para comprobar qué tipo de dato ha inferido en cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de filas que tiene el DataFrame para hacernos una idea de su tamaño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162049"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n",
    "*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\n",
    "el tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark\n",
    "las muestra como string:\n",
    "<ul>\n",
    " <li>dep_time: string (nullable = true)\n",
    " <li>dep_delay: string (nullable = true)\n",
    " <li>arr_time: string (nullable = true)\n",
    " <li>arr_delay: string (nullable = true)\n",
    " <li>air_time: string (nullable = true)\n",
    " <li>hour: string (nullable = true)\n",
    " <li>minute: string (nullable = true)    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cuantos_NA = flightsDF\\\n",
    "                .where(F.col(\"dep_time\") == \"NA\")\\\n",
    "                .count()\n",
    "cuantos_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, hay 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\n",
    "a la cantidad de datos que tenemos. En nuestro caso, como tenemos un número considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/03 21:51:32 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "\n",
    "flightsLimpiado = flightsDF\n",
    "for nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n",
    "    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
    "\n",
    "flightsLimpiado.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora mostramos el número de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\n",
    "pero sigue siendo un número considerable como para realizar analítica y sacar conclusiones sobre estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160748"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsLimpiado.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \n",
    "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. Vamos también a convertir la columna `arr_delay` de tipo entero a número real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/03 21:51:35 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "flightsConvertido = flightsLimpiado\n",
    "\n",
    "for c in columnas_limpiar:\n",
    "    # método que crea una columna o reemplaza una existente\n",
    "    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n",
    "\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\n",
    "flightsConvertido.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya teníamos, pero ahora\n",
    "Spark sí está tratando como enteros las columnas que deberían serlo, y si queremos podemos hacer operaciones aritméticas\n",
    "con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5810b869bd7baccabe0a2952dd0baae1",
     "grade": false,
     "grade_id": "cell-c0cfdd1db1edaa7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Partiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide: \n",
    "\n",
    "* Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. ¿Cuántas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`.\n",
    "* Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cuántas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fb2ec5d49ff84edae4833eca797068b",
     "grade": false,
     "grade_id": "ejercicio-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de aeropuertos de origen distintos: 2\n",
      "Número de rutas distintas: 115\n",
      "Aeropuertos de origen:\n",
      "+------+\n",
      "|origin|\n",
      "+------+\n",
      "|   SEA|\n",
      "|   PDX|\n",
      "+------+\n",
      "\n",
      "Rutas distintas:\n",
      "+------+----+\n",
      "|origin|dest|\n",
      "+------+----+\n",
      "|   SEA| RNO|\n",
      "|   SEA| DTW|\n",
      "|   SEA| CLE|\n",
      "|   SEA| LAX|\n",
      "|   PDX| SEA|\n",
      "|   SEA| BLI|\n",
      "|   PDX| IAH|\n",
      "|   PDX| PHX|\n",
      "|   SEA| SLC|\n",
      "|   SEA| SBA|\n",
      "|   SEA| BWI|\n",
      "|   PDX| IAD|\n",
      "|   PDX| SFO|\n",
      "|   SEA| KOA|\n",
      "|   SEA| JAC|\n",
      "|   PDX| MCI|\n",
      "|   SEA| SJC|\n",
      "|   SEA| ABQ|\n",
      "|   SEA| SAT|\n",
      "|   PDX| ONT|\n",
      "+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 1\n",
    "aeropuertosOrigenDF = flightsConvertido.select(\"origin\").distinct()\n",
    "n_origen = aeropuertosOrigenDF.count()\n",
    "\n",
    "rutasDistintasDF = flightsConvertido.select(\"origin\", \"dest\").distinct()\n",
    "n_rutas = rutasDistintasDF.count()\n",
    "\n",
    "print(f\"Número de aeropuertos de origen distintos: {n_origen}\")\n",
    "print(f\"Número de rutas distintas: {n_rutas}\")\n",
    "\n",
    "print(\"Aeropuertos de origen:\")\n",
    "aeropuertosOrigenDF.show()\n",
    "\n",
    "print(\"Rutas distintas:\")\n",
    "rutasDistintasDF.show()  # puedes ajustar el número de filas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f02d6e47bb3cc84e97f31cce091f80b3",
     "grade": true,
     "grade_id": "ejercicio-1-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(n_origen == 2)\n",
    "assert(n_rutas == 115)\n",
    "assert(aeropuertosOrigenDF.count() == n_origen)\n",
    "assert(rutasDistintasDF.count() == n_rutas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9229f0b48ed644e35a731b404738edf2",
     "grade": false,
     "grade_id": "cell-2b5f0dea18728fcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "* Partiendo de nuevo de `flightsConvertido`, se pide calcular, *sólo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar **ordenado de mayor a menor retraso medio**. El código que calcule esto debería ir encapsulado en una función de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo descrito anteriormente.\n",
    "\n",
    "* Una vez hecha la función, invocarla pasándole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "097436083d0007a7d99d5108e1a504c9",
     "grade": false,
     "grade_id": "ejercicio-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 aeropuertos con mayor retraso medio:\n",
      "+----+------------------+\n",
      "|dest|     retraso_medio|\n",
      "+----+------------------+\n",
      "| BOI|             64.75|\n",
      "| HDN|              46.8|\n",
      "| SFO|41.193768844221104|\n",
      "| CLE| 35.74193548387097|\n",
      "| SBA|35.391752577319586|\n",
      "| COS| 35.05607476635514|\n",
      "| BWI|34.585798816568044|\n",
      "| EWR| 33.52972258916777|\n",
      "| DFW| 33.27519181585678|\n",
      "| MIA| 32.66187050359712|\n",
      "+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "1. BOI - 64.75 minutos\n",
      "2. HDN - 46.8 minutos\n",
      "3. SFO - 41.19 minutos\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def retrasoMedio(df):\n",
    "    return df.where(F.col(\"arr_delay\") > 0) \\\n",
    "             .groupBy(\"dest\") \\\n",
    "             .agg(F.avg(\"arr_delay\").alias(\"retraso_medio\")) \\\n",
    "             .orderBy(F.desc(\"retraso_medio\"))\n",
    "\n",
    "retrasoMedioDF = retrasoMedio(flightsConvertido)\n",
    "\n",
    "print(\"Top 10 aeropuertos con mayor retraso medio:\")\n",
    "retrasoMedioDF.show(10)\n",
    "\n",
    "top3 = retrasoMedioDF.take(3)\n",
    "for i, row in enumerate(top3, 1):\n",
    "    print(f\"{i}. {row['dest']} - {round(row['retraso_medio'], 2)} minutos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12aa19d467a0c50adface20495b6cf35",
     "grade": true,
     "grade_id": "ejercicio-2-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lista = retrasoMedio(flightsConvertido).take(3)\n",
    "assert((lista[0].retraso_medio == 64.75) & (lista[0].dest == \"BOI\"))\n",
    "assert((lista[1].retraso_medio == 46.8) & (lista[1].dest == \"HDN\"))\n",
    "assert((round(lista[2].retraso_medio, 2) == 41.19) & (lista[2].dest == \"SFO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora invocamos a nuestra función `retrasoMedio` pasándole como argumento `flightsConvertido`. ¿Cuáles son los tres aeropuertos con mayor retraso medio? ¿Cuáles son sus retrasos medios en minutos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESCRIBE AQUÍ TU CÓDIGO PARA MOSTRAR EL CONTENIDO DE retrasoMedioDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa9ed55cd86eb0958e150d6a918db1af",
     "grade": false,
     "grade_id": "cell-e577747d4427e32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "Ajustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4405b7db7180445df5cacc66d82db53",
     "grade": false,
     "grade_id": "cell-e577747d4427e32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notemos que en estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Debemos indicar a Spark cuáles son categóricas e indexarlas. Para ello se pide: \n",
    "\n",
    "* Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categóricas `month` y `carrier` (tipo de avión). El nombre de las columnas indexadas que se crearán debe ser, respectivamente, `monthIndexed` y `carrierIndexed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb039584cd14e6bc3434e5be930341e6",
     "grade": false,
     "grade_id": "string-indexer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\")\n",
    "indexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c96c7c7d5836922dd549b358b897b781",
     "grade": true,
     "grade_id": "string-indexer-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(indexerMonth, StringIndexer))\n",
    "assert(isinstance(indexerCarrier, StringIndexer))\n",
    "assert(indexerMonth.getInputCol() == \"month\")\n",
    "assert(indexerMonth.getOutputCol() == \"monthIndexed\")\n",
    "assert(indexerCarrier.getInputCol() == \"carrier\")\n",
    "assert(indexerCarrier.getOutputCol() == \"carrierIndexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69e024b0baeeb36bb74d136c7e113372",
     "grade": false,
     "grade_id": "cell-e577747d4427e323",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recordemos también que Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n",
    "\n",
    "* Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que serán las que formarán parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es lógico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas debe llamarse `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
     "grade": false,
     "grade_id": "vector-assembler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "columnas_ensamblar = [\n",
    "    \"monthIndexed\", \"day\", \"dep_time\", \"arr_time\",\n",
    "    \"carrierIndexed\", \"distance\", \"air_time\"\n",
    "]\n",
    "vectorAssembler = VectorAssembler(inputCols=columnas_ensamblar, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d17b2fa1d8a4bd02b89952429ba1552",
     "grade": true,
     "grade_id": "vector-assembler-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(vectorAssembler, VectorAssembler))\n",
    "assert(vectorAssembler.getOutputCol() == \"features\")\n",
    "input_cols = vectorAssembler.getInputCols()\n",
    "assert(len(input_cols) == 7)\n",
    "assert(\"arr_delay\" not in input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eeb3a3f0b15e8b374789706cd9bce49",
     "grade": false,
     "grade_id": "cell-e577747d4427e32dsdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n",
    "\n",
    "* Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
     "grade": false,
     "grade_id": "binarizer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "delayBinarizer = Binarizer(threshold=15, inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ec408ab263c0378526f96bb5d374704",
     "grade": true,
     "grade_id": "binarizer-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(delayBinarizer, Binarizer))\n",
    "assert(delayBinarizer.getThreshold() == 15)\n",
    "assert(delayBinarizer.getInputCol() == \"arr_delay\")\n",
    "assert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aed16d7ed0219fe1d8741848f594319",
     "grade": false,
     "grade_id": "cell-25a7793978ee7d05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Por último, crearemos el modelo de clasificación.\n",
    "\n",
    "* Crear en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n",
    "* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n",
    "* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e785136d6d06691c003ff9542027e03d",
     "grade": false,
     "grade_id": "decision-tree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b03b48f304b5b34cd06eeec49e001fd",
     "grade": true,
     "grade_id": "decision-tree-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(decisionTree, DecisionTreeClassifier))\n",
    "assert(decisionTree.getFeaturesCol() == \"features\")\n",
    "assert(decisionTree.getLabelCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "728da91edabbac5b62baf31bdd0a707e",
     "grade": false,
     "grade_id": "cell-e577747d4427e32d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora vamos a encapsular todas las fases en un sólo pipeline y procederemos a entrenarlo. Se pide:\n",
    "\n",
    "* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n",
    "\n",
    "* Entrenarlo invocando sobre ella al método `fit` y guardar el pipeline entrenado devuelto por dicho método en una variable llamada `pipelineModel`. \n",
    "\n",
    "* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. Nótese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no habíamos hecho (aunque habría sido lo correcto) ninguna división de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edbdb627305d03efa41a88426330e160",
     "grade": false,
     "grade_id": "pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexerMonth,\n",
    "    indexerCarrier,\n",
    "    vectorAssembler,\n",
    "    delayBinarizer,\n",
    "    decisionTree\n",
    "])\n",
    "\n",
    "pipelineModel = pipeline.fit(flightsConvertido)\n",
    "flightsPredictions = pipelineModel.transform(flightsConvertido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "107b5ec260550eb4235ffecff655289a",
     "grade": true,
     "grade_id": "pipeline-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "assert(isinstance(pipeline, Pipeline))\n",
    "assert(len(pipeline.getStages()) == 5)\n",
    "assert(isinstance(pipelineModel, PipelineModel))\n",
    "assert(\"probability\" in flightsPredictions.columns)\n",
    "assert(\"prediction\" in flightsPredictions.columns)\n",
    "assert(\"rawPrediction\" in flightsPredictions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c531953abf18cfb3b67571ddde7a57d",
     "grade": false,
     "grade_id": "cell-61156fe5938763f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Vamos a mostrar la matriz de confusión (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
     "grade": false,
     "grade_id": "cell-896752beb71cb455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------+\n",
      "|arr_delay_binary|prediction| count|\n",
      "+----------------+----------+------+\n",
      "|             1.0|       1.0|   505|\n",
      "|             0.0|       1.0|    73|\n",
      "|             1.0|       0.0| 23744|\n",
      "|             0.0|       0.0|136426|\n",
      "+----------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones generadas por el modelo:\n",
      "+----------------------------------------+----------------+----------+----------------------------------------+\n",
      "|features                                |arr_delay_binary|prediction|probability                             |\n",
      "+----------------------------------------+----------------+----------+----------------------------------------+\n",
      "|[10.0,1.0,1.0,235.0,0.0,1542.0,194.0]   |1.0             |1.0       |[0.28865979381443296,0.711340206185567] |\n",
      "|[10.0,1.0,4.0,738.0,6.0,2279.0,252.0]   |0.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "|[10.0,1.0,8.0,548.0,4.0,1825.0,201.0]   |0.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "|[10.0,1.0,28.0,800.0,6.0,2282.0,251.0]  |0.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "|[10.0,1.0,34.0,325.0,0.0,1448.0,201.0]  |1.0             |1.0       |[0.28865979381443296,0.711340206185567] |\n",
      "|[10.0,1.0,37.0,747.0,3.0,1927.0,224.0]  |1.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "|[10.0,1.0,346.0,936.0,4.0,1721.0,202.0] |1.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "|[10.0,1.0,526.0,1148.0,4.0,1825.0,217.0]|0.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "|[10.0,1.0,527.0,917.0,4.0,1024.0,136.0] |1.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "|[10.0,1.0,536.0,1334.0,4.0,2402.0,268.0]|0.0             |0.0       |[0.9424501424501425,0.05754985754985755]|\n",
      "+----------------------------------------+----------------+----------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Matriz de confusión (arr_delay_binary vs. prediction):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------+\n",
      "|arr_delay_binary|prediction| count|\n",
      "+----------------+----------+------+\n",
      "|             0.0|       0.0|136426|\n",
      "|             0.0|       1.0|    73|\n",
      "|             1.0|       0.0| 23744|\n",
      "|             1.0|       1.0|   505|\n",
      "+----------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Predicciones generadas por el modelo:\")\n",
    "flightsPredictions.select(\"features\", \"arr_delay_binary\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "print(\"Matriz de confusión (arr_delay_binary vs. prediction):\")\n",
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().orderBy(\"arr_delay_binary\", \"prediction\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2 (continuación de Actividad 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9009124b4c18f0eda044abc5e2ff141f",
     "grade": false,
     "grade_id": "cell-20cd3bb8f5a12f07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 2: Structured Streaming y Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuerda borrar siempre las líneas que dicen `raise NotImplementedError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1fed64c5c94f8b4c27326b8b48ec13f",
     "grade": false,
     "grade_id": "cell-2fbae7fd82212064",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Punto de partida (final de la actividad 1): función `retrasoMedio` \n",
    "***Para los vuelos que llegan con retraso positivo, calcular para cada aeropuerto de llegada el retraso medio.***\n",
    "\n",
    "Recordatorio: *El código que calcule esto debería ir encapsulado en una función de Python que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo del retraso medio por aeropuerto, ordenado de mayor a menor retraso medio. La columna creada con el retraso medio debe llamarse `retraso_medio`.*\n",
    "\n",
    "**Copia en la siguiente celda el código de tu función retrasoMedio que has completado en la actividad 1**. El DataFrame devuelto por la función debería tener solamente dos columnas: `dest` y `retraso_medio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 aeropuertos con mayor retraso medio:\n",
      "+----+------------------+\n",
      "|dest|     retraso_medio|\n",
      "+----+------------------+\n",
      "| BOI|             64.75|\n",
      "| HDN|              46.8|\n",
      "| SFO|41.193768844221104|\n",
      "| CLE| 35.74193548387097|\n",
      "| SBA|35.391752577319586|\n",
      "| COS| 35.05607476635514|\n",
      "| BWI|34.585798816568044|\n",
      "| EWR| 33.52972258916777|\n",
      "| DFW| 33.27519181585678|\n",
      "| MIA| 32.66187050359712|\n",
      "+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "1. BOI - 64.75 minutos\n",
      "2. HDN - 46.8 minutos\n",
      "3. SFO - 41.19 minutos\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def retrasoMedio(df):\n",
    "    return df.where(F.col(\"arr_delay\") > 0) \\\n",
    "             .groupBy(\"dest\") \\\n",
    "             .agg(F.avg(\"arr_delay\").alias(\"retraso_medio\")) \\\n",
    "             .orderBy(F.desc(\"retraso_medio\"))\n",
    "\n",
    "retrasoMedioDF = retrasoMedio(flightsConvertido)\n",
    "\n",
    "print(\"Top 10 aeropuertos con mayor retraso medio:\")\n",
    "retrasoMedioDF.show(10)\n",
    "\n",
    "top3 = retrasoMedioDF.take(3)\n",
    "for i, row in enumerate(top3, 1):\n",
    "    print(f\"{i}. {row['dest']} - {round(row['retraso_medio'], 2)} minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Utilizaremos Kafka para actualizar en tiempo real el resultado calculado en el apartado anterior. \n",
    "\n",
    "Para simplificar, asumimos que los mensajes leídos de Kafka tiene solamente dos campos que son los únicos necesarios para llevar a cabo la operación anterior: dest y arr_delay. La idea será crear un Streaming DataFrame para leer de Kafka, y después invocar a nuestra función retrasoMedio pasándolo como argumento. Vamos a leer del topic `retrasos` por lo que debes indicar esta opción a continuación.\n",
    "\n",
    "Se pide crear, en la variable `retrasosStreamingDF`, un Streaming DataFrame leyendo de Apache Kafka, configurando las siguientes opciones:\n",
    "  * Usar la variable `readStream` (en lugar de `read` como solemos hacer) interna de la SparkSession `spark`\n",
    "  * Indicar que el formato es `\"kafka\"` con `.format(\"kafka\")`\n",
    "  * Indicar cuáles son los brokers de Kafka de los que vamos a leer y el puerto al que queremos conectarnos para leer (9092 es el que usa Kafka por defecto), con `.option(\"kafka.bootstrap.servers\", \"<nombre_cluster>-w-0:9092,<nombre_cluster>-w-1:9092\")`. De esa manera podremos leer el mensaje si el productor de Kafka lo envía a cualquiera de los dos brokers existentes, que son los nodos del cluster identificados como `<nombre_cluster>-w-0` y `<nombre_cluster>-w-1`\n",
    "  * Indicar que queremos subscribirnos al topic `\"retrasos\"` con `.option(\"subscribe\", \"retrasos\")`.\n",
    "  * Finalmente ponemos `load()` para realizar la lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca3d9ff34a83745e199a3c4aed9acf07",
     "grade": false,
     "grade_id": "read-stream",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/03 21:52:04 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-912c79f5-0794-49ce-8660-237948b3a850. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/03 21:52:04 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from threading import Thread\n",
    "\n",
    "# Esquema del JSON recibido desde Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"dest\", StringType(), True),\n",
    "    StructField(\"arr_delay\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# IP del nodo master o bootstrap server de Kafka\n",
    "bootstrap_servers = \"10.132.15.195:9092\"\n",
    "\n",
    "# Crear el Streaming DataFrame desde Kafka\n",
    "retrasosStreamingDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
    "    .option(\"subscribe\", \"retrasos\") \\\n",
    "    .load()\n",
    "\n",
    "# Parsear el JSON\n",
    "datosKafkaDF = retrasosStreamingDF.selectExpr(\"CAST(value AS STRING) as json_string\") \\\n",
    "    .select(from_json(col(\"json_string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.dest\", \"data.arr_delay\")\n",
    "\n",
    "# Agregación de retraso medio\n",
    "def retrasoMedio(df):\n",
    "    return df.groupBy(\"dest\").agg(avg(\"arr_delay\").alias(\"retraso_medio\"))\n",
    "\n",
    "retrasoMedioStreamingDF = retrasoMedio(datosKafkaDF)\n",
    "\n",
    "# Escribir la salida en memoria (en una vista temporal)\n",
    "query = retrasoMedioStreamingDF.writeStream \\\n",
    "    .queryName(\"retrasosAgg\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n",
    "\n",
    "# Iniciar el stream en un hilo separado para que no bloquee el notebook\n",
    "def monitor_query(q):\n",
    "    q.awaitTermination()\n",
    "\n",
    "Thread(target=monitor_query, args=(query,)).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5c34086b3c3fb29a4c46865396bcb1",
     "grade": true,
     "grade_id": "read-stream-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Mostramos el esquema de este DataFrame\n",
    "types = retrasosStreamingDF.dtypes\n",
    "assert(retrasosStreamingDF.isStreaming)\n",
    "assert((types[0][0] == \"key\")       & (types[0][1] == \"binary\"))\n",
    "assert((types[1][0] == \"value\")     & (types[1][1] == \"binary\"))\n",
    "assert((types[2][0] == \"topic\")     & (types[2][1] == \"string\"))\n",
    "assert((types[3][0] == \"partition\") & (types[3][1] == \"int\"))\n",
    "assert((types[4][0] == \"offset\")    & (types[4][1] == \"bigint\"))\n",
    "assert((types[5][0] == \"timestamp\") & (types[5][1] == \"timestamp\"))\n",
    "assert((types[6][0] == \"timestampType\") & (types[6][1] == \"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el esquema de los datos de Kafka\n",
    "retrasosStreamingDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "699b1979df4eb0241b1b4ba165d6b311",
     "grade": false,
     "grade_id": "cell-580bf3caf39b314e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Muestra por pantalla el esquema del DataFrame resultante de la lectura con `printSchema()`. Verás que todas estas columnas son creadas automáticamente por Spark cuando leemos de Kafka. De ellas, la que nos interesa es `value` que contiene propiamente el mensaje de Kafka, en formato datos binarios. \n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "Tendremos que estructurar estos datos para poder extraer los campos. Para ello sigue los siguientes pasos, ayudándote de la plantilla que hay en la celda siguiente (descoméntala y complétala):\n",
    "\n",
    "* **Selecciona** la columna `value` y conviértela (`.cast`) a `StringType()` utilizando `withColumn` para reemplazar la columna existente `\"value\"` por el objeto Column resultante de la conversión. De esta forma tendremos una columna que contendrá en cada **fila** un **fichero JSON completo**, tal como se muestra en cada una de las plantillas anteriores. \n",
    "* Para extraer los dos campos de cada uno de los JSON y convertirlos en una columna llamada `parejas`, de tipo `struct` (una estructura formada por dos campos de tipo String e Integer respectivamente), utilizamos la función `from_json` de Spark, que se aplica a cada elemento (cada fila) de la columna \"value\" y parsea el String según un esquema que le indiquemos, devolviendo una columna de tipo `struct`.\n",
    "* La columna `parejas` es de tipo `struct` por lo que puedes acceder a cada uno de sus dos campos (`dest` y `arr_delay`) con el operador `.` (punto). Utilizando `withColumn` dos veces, crea dos columnas llamadas `dest` y `arr_delay` como el resultado de acceder a `parejas.dest` y `parejas.arr_delay` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91b265facf6e3bded458cc9b6a754b1a",
     "grade": false,
     "grade_id": "estructura-json",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Definimos el esquema del JSON recibido en la columna 'value'\n",
    "esquema = StructType([\n",
    "    StructField(\"dest\", StringType()),\n",
    "    StructField(\"arr_delay\", DoubleType())\n",
    "])\n",
    "\n",
    "# Parseamos el contenido de 'value' y extraemos los campos\n",
    "parsedDF = retrasosStreamingDF \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(StringType())) \\\n",
    "    .withColumn(\"parejas\", F.from_json(F.col(\"value\"), esquema)) \\\n",
    "    .withColumn(\"dest\", F.col(\"parejas.dest\")) \\\n",
    "    .withColumn(\"arr_delay\", F.col(\"parejas.arr_delay\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fc63420d404c6ca1fedec52d4ee3b7a",
     "grade": true,
     "grade_id": "estructura-json-tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tipos = parsedDF.dtypes\n",
    "assert((\"value\", \"string\") in tipos)\n",
    "assert(('parejas', 'struct<dest:string,arr_delay:double>') in tipos)\n",
    "assert(('dest', 'string') in tipos)\n",
    "assert(('arr_delay', 'double') in tipos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro DataFrame ya contiene una columna `dest` con el nombre del aeropuerto destino y una columna de números reales `arr_delay` con el retraso. Ya podemos efectuar el mismo tipo de agregación que estamos haciendo en nuestra función `retrasoMedio`. Por tanto, invocamos a `retrasoMedio` pasando `parsedDF` como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/03 21:57:51 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-92ed3b02-298f-4a3c-95b8-abfcd0f6c6a6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/03 21:57:51 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name retrasosAgg as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evalúa el siguiente código pero no lo modifiques\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Indicamos que este DataFrame se guarde en memoria cuando se va actualizando,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# y arrancamos la ejecución en Streaming con la acción start()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m retrasoMedioStreamingDF \u001b[38;5;241m=\u001b[39m retrasoMedio(parsedDF)\n\u001b[0;32m----> 7\u001b[0m consoleOutput \u001b[38;5;241m=\u001b[39m \u001b[43mretrasoMedioStreamingDF\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretrasosAgg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/streaming.py:1491\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name retrasosAgg as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "# Evalúa el siguiente código pero no lo modifiques\n",
    "# Indicamos que este DataFrame se guarde en memoria cuando se va actualizando,\n",
    "# y arrancamos la ejecución en Streaming con la acción start()\n",
    "\n",
    "retrasoMedioStreamingDF = retrasoMedio(parsedDF)\n",
    "\n",
    "consoleOutput = retrasoMedioStreamingDF\\\n",
    "                    .writeStream\\\n",
    "                    .queryName(\"retrasosAgg\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "018ce42cfbc94eef069b4a60e5e89090",
     "grade": false,
     "grade_id": "cell-b073802f3eec8bb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Una vez evaluada la celda anterior, abre el productor de Kafka console entrando por SSH a cualquiera de las máquinas (revisa el enunciado de la práctica para recordarlo), y copia y pega (literalmente) los siguientes 4 mensajes en formato JSON. Como ves,  tienen un campo `dest` y un campo `arr_delay`, simulando la información que estaríamos recibiendo en tiempo real de los distintos aeropuertos a medida que los vuelos van aterrizando. \n",
    "\n",
    "Cada vez que pegues un mensaje, ejecuta la consulta `select * from retrasosAgg` a través del método `spark.sql(...)` y muestra el DataFrame `agregadosDF` devuelto por dicho método. Eso hará una consulta contra la vista temporal (volátil) `retrasosAgg` que se ha creado en el metastore de Hive gracias al `writeStream` del apartado anterior. Ejecuta la celda de `show` tantas veces como sea necesario hasta ver un resultado distinto al que has visto en la ejecución anterior, para asegurarte de que Spark ya ha leído e incorporado el nuevo dato en su cálculo de la agregación y por tanto ha actualizado el resultado.\n",
    "\n",
    "Recuerda que el método `.sql(...)` es una transformación, y por tanto, se re-ejecuta la consulta cada vez que invocas a la acción `show()` sobre el resultado, ya que **no vamos a cachear nada**, precisamente para forzar la reevaluación de la consulta y poder ver así el contenido actualizado de dicha tabla (en memoria) de Hive cada vez que hacemos `show()`.\n",
    "\n",
    "Se pide: \n",
    "* Cada vez que envíes un mensaje y te hayas asegurado de que Spark ha incorporado el dato a su cálculo, apunta el resultado de la agregación (valor de la columna `retraso_medio`) para MAD y GRX en las variables habilitadas para ello\n",
    "* No te preocupes por evaluar muchas veces una misma celda, ya que el cálculo sólo se actualizará una vez. Las siguientes veces que la evalúes seguirá mostrando el mismo resultado mientras no envíes otro nuevo mensaje en Kafka.\n",
    "\n",
    "Los 4 mensajes que hay que introducir sucesivamente en Kafka son:\n",
    "\n",
    "`\n",
    "{\"dest\": \"GRX\", \"arr_delay\": 2.6}\n",
    "{\"dest\": \"MAD\", \"arr_delay\": 5.4}\n",
    "{\"dest\": \"GRX\", \"arr_delay\": 1.5}\n",
    "{\"dest\": \"MAD\", \"arr_delay\": 20.0}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aca37e32dce02202f512568edca47c19",
     "grade": false,
     "grade_id": "sql-streaming",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|dest|retraso_medio|\n",
      "+----+-------------+\n",
      "| MAD|         12.7|\n",
      "| GRX|         2.05|\n",
      "|null|         null|\n",
      "+----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Consulta la vista en memoria creada por writeStream\n",
    "agregadosDF = spark.sql(\"SELECT * FROM retrasosAgg ORDER BY retraso_medio DESC\")\n",
    "agregadosDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraso medio MAD: 12.7\n",
      "Retraso medio GRX: 2.05\n"
     ]
    }
   ],
   "source": [
    "resultados = agregadosDF.collect()\n",
    "retrasoMAD = None\n",
    "retrasoGRX = None\n",
    "\n",
    "for row in resultados:\n",
    "    if row['dest'] == 'MAD':\n",
    "        retrasoMAD = row['retraso_medio']\n",
    "    elif row['dest'] == 'GRX':\n",
    "        retrasoGRX = row['retraso_medio']\n",
    "\n",
    "if retrasoMAD is not None:\n",
    "    print(f\"Retraso medio MAD: {round(retrasoMAD, 2)}\")\n",
    "else:\n",
    "    print(\"No hay datos para MAD.\")\n",
    "\n",
    "if retrasoGRX is not None:\n",
    "    print(f\"Retraso medio GRX: {round(retrasoGRX, 2)}\")\n",
    "else:\n",
    "    print(\"No hay datos para GRX.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c17f79058004e6c56c6cb64795ae6f2",
     "grade": true,
     "grade_id": "sql-streaming-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "columnas = agregadosDF.columns\n",
    "assert(len(columnas) == 2)\n",
    "assert(\"dest\" in columnas)\n",
    "assert(\"retraso_medio\" in columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "709560e7fbed0009d74b7fec8c90a3c1",
     "grade": false,
     "grade_id": "results-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tras enviar el primer mensaje\n",
    "agregadosDF = spark.sql(\"SELECT * FROM retrasosAgg ORDER BY retraso_medio DESC\")\n",
    "agregadosDF.show()\n",
    "retraso_medio_GRX_primer_mensaje = 2.6  # GRX: 2.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b8f02459cd58afae6cf1a32ef69f36e",
     "grade": false,
     "grade_id": "results-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tras enviar el segundo mensaje\n",
    "agregadosDF = spark.sql(\"SELECT * FROM retrasosAgg ORDER BY retraso_medio DESC\")\n",
    "agregadosDF.show()\n",
    "retraso_medio_GRX_segundo_mensaje = 2.6\n",
    "retraso_medio_MAD_segundo_mensaje = 5.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ed1d482d7859eaf314d85f7f056d79e",
     "grade": false,
     "grade_id": "results-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tras enviar el tercer mensaje\n",
    "agregadosDF = spark.sql(\"SELECT * FROM retrasosAgg ORDER BY retraso_medio DESC\")\n",
    "agregadosDF.show()\n",
    "retraso_medio_GRX_tercer_mensaje = 2.05  # promedio de 2.6 y 1.5\n",
    "retraso_medio_MAD_tercer_mensaje = 5.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e49b6258fd677f636bbc9f01e41a593",
     "grade": false,
     "grade_id": "results-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tras enviar el cuarto mensaje\n",
    "agregadosDF = spark.sql(\"SELECT * FROM retrasosAgg ORDER BY retraso_medio DESC\")\n",
    "agregadosDF.show()\n",
    "retraso_medio_GRX_cuarto_mensaje = 2.05\n",
    "retraso_medio_MAD_cuarto_mensaje = 12.7  # promedio de 5.4 y 20.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "065e98ac2bd3061d7706cd282041e722",
     "grade": true,
     "grade_id": "results-tests",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Ver resumen de los valores capturados\n",
    "print(\"Resumen de valores de retraso medio:\")\n",
    "print(f\"GRX después del 1er mensaje: {retraso_medio_GRX_primer_mensaje}\")\n",
    "print(f\"GRX después del 2do mensaje: {retraso_medio_GRX_segundo_mensaje}\")\n",
    "print(f\"GRX después del 3er mensaje: {retraso_medio_GRX_tercer_mensaje}\")\n",
    "print(f\"GRX después del 4to mensaje: {retraso_medio_GRX_cuarto_mensaje}\")\n",
    "print(f\"MAD después del 2do mensaje: {retraso_medio_MAD_segundo_mensaje}\")\n",
    "print(f\"MAD después del 3er mensaje: {retraso_medio_MAD_tercer_mensaje}\")\n",
    "print(f\"MAD después del 4to mensaje: {retraso_medio_MAD_cuarto_mensaje}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
