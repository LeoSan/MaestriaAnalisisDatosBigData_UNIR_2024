
# Tema 1. La ciencia del dato y los datos masivos

## 1.1. Generalidades
- La ciencia de datos y los datos masivos son disciplinas que se encuentran en constante crecimiento y evolución en el ámbito empresarial. 

- Su importancia radica en su capacidad para extraer conocimiento y generar valor a partir de grandes volúmenes de datos, 

- Permitiendo así tomar decisiones estratégicas fundamentadas en información precisa y relevante.

## 1.2. La Cadena de Valor

> La cadena de valor de los datos es el proceso de transformar datos crudos en acciones valiosas para una empresa. Comienza con la recolección de datos, continúa con su procesamiento y análisis, y culmina con la toma de decisiones informadas que generan valor. Cada etapa es crucial para aprovechar al máximo el potencial de los datos.

La cadena de valor de los datos consta de las siguientes etapas:

1. Recopilación: Se recolectan datos de diversas fuentes.
2. Procesamiento: Los datos se limpian, organizan y preparan para el análisis.
3. Análisis: Se aplican técnicas estadísticas y de aprendizaje automático para extraer insights.[perspectivas]
4. Información: Se generan conocimientos útiles a partir de los datos analizados.
5. Decisión: Se toman decisiones estratégicas basadas en la información obtenida.
6. Valor: Se crea valor para la empresa al implementar las decisiones.

## 1.3. ¿Qué son los datos masivos?

> Los datos masivos (big data) son conjuntos de información extremadamente grandes y complejos que se generan a una velocidad vertiginosa. Esta información proviene de diversas fuentes, como redes sociales, transacciones en línea y sensores.

## Características clave de los datos masivos:**

- Tienen Volumen: Grandes cantidades de datos que superan la capacidad de herramientas tradicionales.

- Tienen Velocidad: Datos generados a una velocidad muy alta y que necesitan ser procesados rápidamente.

- Tienen Variedad: Diferentes tipos de datos (texto, imágenes, videos, etc.) en diversos formatos.

## ¿Por qué son importantes los datos masivos?

- Contiene Información valiosa: Al analizar estos datos, se pueden obtener insights (perspectivas) clave para tomar mejores decisiones.

- Contienen Ventajas competitivas: Las empresas pueden identificar tendencias, predecir comportamientos y mejorar sus estrategias.

- Contienen Aprendizaje automático: Gracias a tecnologías como el aprendizaje automático, se pueden descubrir patrones complejos en los datos.

> Un dato puede definirse como un hecho concreto y discreto acerca de un evento. [ Seguiremos la proporcionada por Davenport y Prusak (1998).] La característica de ser discreto implica que, semánticamente, es la unidad mínima  que puede comunicarse o almacenarse. Por sí solos, los datos no brindan detalles significantes del entorno del que fueron obtenidos.

> La información puede definirse como un mensaje formado por la composición de varios datos

## 1.4. ¿Qué es la Ciencia de Datos?

- La ciencia de datos es una disciplina que busca extraer conocimiento a partir de grandes conjuntos de datos. 
- A diferencia de otras ciencias que se basan en teorías preexistentes, la ciencia de datos utiliza herramientas computacionales y estadísticas para descubrir patrones y tendencias ocultas en los datos.

**¿Qué hace única a la ciencia de datos?**

- Agnosticismo: Puede aplicarse a cualquier campo, desde química hasta marketing.
- Fundamental: Busca información básica en los datos, no solo confirmar teorías.
- Multidisciplinaria: Combina herramientas de matemáticas, estadística, informática, inteligencia artificial y más.
- Transformadora: Está cambiando la forma en que entendemos y abordamos la ciencia.

**Beneficios para las organizaciones:**

- Descubrimiento de patrones ocultos: Identificar tendencias y oportunidades que no son evidentes a simple vista.
- Toma de decisiones basada en datos: Utilizar la información para tomar decisiones más informadas y estratégicas.
- Optimización de procesos: Mejorar la eficiencia y reducir costos.
- Generación de estrategias competitivas: Obtener una ventaja competitiva en el mercado.

## 1.5. La toma de decisiones basadas en datos

La toma de decisiones basada en datos es un proceso que utiliza análisis estadístico profundo y herramientas avanzadas para transformar grandes cantidades de información en conocimiento accionable. Al emplear datos objetivos y rigurosos, las empresas pueden:

- Tomar decisiones más informadas: Identificando tendencias, patrones y riesgos.
- Optimizar procesos: Aumentando la eficiencia y reduciendo costos.
- Mejorar la toma de decisiones estratégicas: Alineado con los objetivos organizacionales.
- Minimizar la incertidumbre: Basando las decisiones en evidencia sólida.
- Incrementar la competitividad: Identificando nuevas oportunidades y mejorando la toma de decisiones.

## 1.6. Problemas empresariales y soluciones de ciencia de datos

- La ciencia de datos: un aliado estratégico para optimizar la cadena de suministro

- La ciencia de datos ofrece una herramienta poderosa para abordar de manera efectiva los desafíos que enfrentan las empresas en la actualidad. Al analizar grandes volúmenes de datos, esta disciplina permite:

- Optimizar la cadena de suministro: Identificando cuellos de botella, mejorando la planificación de la demanda y reduciendo costos.
- Detectar fraudes: Implementando sistemas de alerta temprana basados en patrones anómalos en los datos.
- Personalizar la experiencia del cliente: Analizando el comportamiento de los consumidores para ofrecer productos y servicios más relevantes.
- Segmentar el mercado: Identificando grupos de clientes con características similares para adaptar las estrategias de marketing.
- Predecir la demanda: Anticipando las necesidades futuras de los clientes para ajustar la producción y evitar desabastecimientos.

**¿Cómo funciona?**

A través de técnicas como el aprendizaje automático, la minería de datos y la inteligencia artificial, la ciencia de datos transforma los datos en información valiosa y accionable. Esto permite a las empresas tomar decisiones más informadas y basadas en evidencias, reduciendo así la incertidumbre y el riesgo.


## Cuestionario 

1. ¿Qué es la cadena de valor en la ciencia de datos?
    A. Un software específico para análisis de datos.
    B. Un proceso que transforma los datos en valor agregado para las empresas. -> Correcto 
    C. Una metodología de visualización de datos.
    D. Un tipo de algoritmo de aprendizaje automático.

2. ¿Qué caracteriza a los datos masivos?
    A. Pequeños volúmenes de información.
    B. Procesamiento manual de datos.
    C. Grandes volúmenes de datos que requieren herramientas especiales. -> Correcto 
    D. Datos siempre estructurados.

3. ¿Cuál es una aplicación de la ciencia de datos en las empresas?
    A. Reducir la cantidad de datos a analizar.
    B. Ignorar las tendencias del mercado.
    C. Tomar decisiones basadas en conjeturas. -> correcto 
    D. Optimizar procesos y mejorar la eficiencia operativa.

4. ¿Qué se requiere en la fase de recopilación de datos?
    A. Fuentes confiables y métodos eficientes. --> Correcto 
    B. Limitar la cantidad de datos recogidos.
    C. Usar exclusivamente datos estructurados.
    D. Evitar el uso de tecnología moderna.

5. ¿Qué permite el aprendizaje automático en el contexto de datos masivos?
    A. Reducir la cantidad de datos necesarios para análisis.
    B. Automatizar tareas complejas de análisis de datos. -> Datos
    C. Evitar el uso de cualquier tipo de datos.
    D. Utilizar exclusivamente datos antiguos.

6. ¿Cuál es el primer paso en la cadena de valor de la ciencia de datos?
    A. Visualización de datos. 
    B. Análisis de datos.
    C. Recopilación de datos. -> Correcto 
    D. Toma de decisiones.

7. ¿Qué tecnologías son cruciales para manejar datos masivos?
    A. Aprendizaje automático e inteligencia artificial. --> Correcto 
    B. Calculadoras básicas.
    C. Herramientas de escritura manual.
    D. Procesamiento de texto simple.

8. ¿Qué beneficios ofrece la ciencia de datos a las empresas?
    A. Limita las fuentes de datos utilizadas.
    B. Ignora las tendencias y patrones en los datos.
    C. Ayuda a tomar decisiones basadas en datos y resolver problemas --> Correcto 
    complejos.
    D. Fomenta decisiones basadas únicamente en la intuición.

9. ¿Qué implica la acción basada en decisiones dentro de la cadena de valor de los datos?
    A. No tomar ninguna medida basada en los análisis.
    B. Implementar decisiones que crean valor para la organización. --> Correcto 
    C. Desconocer los resultados del análisis.
    D. Revertir todas las decisiones previas.

10. ¿Qué se necesita para maximizar el potencial de los datos en beneficio de la empresa?
    A. Herramientas y técnicas específicas en cada etapa de la cadena de valor. --> Correcto 
    B. Evitar la adopción de nuevas tecnologías.
    C. Reducir la cantidad de datos analizados.
    D. Concentrarse en datos irrelevantes.


## Tema 2. Ciclo de vida de los datos masivos

## 2.1. Recolección
Es el proceso que nos permite iniciar con la redaudacción de datos aquí podremos indentifcar la fuente de información, tambien podemos implementar tareas de limpieza y tratamiento de datos como validaciones y comprobaciones, para luego ser almacenados bajo su estructura previamente analizada bien sea BD SQL o NOsql 

## Es importante conocer como se encuentran los datos para iniciar su recolección

**Datos no estructurados**
Los datos no estructurados son la forma «más cruda» de los datos sin una estructura
identificable y pueden ser cualquier tipo de dato como textos, imágenes, sonidos o
vídeos. Una forma de determinar si los datos son no estructurados es si no podemos
almacenarlos en filas y columnas en una base de datos relacional

## Algunos ejemplos de datos no estructurados serían:
- Textos: archivos de texto plano (.txt)
- Imágenes y animaciones: archivos JPEG, PNG, GIF, etc.
- Sonidos: archivos MP3, OGG, WAV, etc.
- Vídeos: archivos MPEG / MP4, AVI, MKV, OGG, etc

**Datos estructurados**
- Siguen un esquema (schema) que requiere cierto esfuerzo elaborar. 
- El caso más habitual de este tipo de esquema en los datos estructurados es el de tablas o esquema tabular. 
- En los datos tabulares podemos estructurar los datos en filas y columnas, como si de una tabla se
tratara.
- La cabecera (header) o primera fila nos indica el nombre (y tipo) de cada uno
de los atributos. 
- Una fila o un conjunto de atributos es conocida como un registro o una instancia de la especificación dada por el esquema de datos.

## Algunos ejemplos de datos estructurados serían:
- Los archivos CSV (comma-separated values) son un ejemplo claro de datos
estructurados, siendo ampliamente utilizados como datasets en ciencia de datos.
- Ficheros CSV.
- Hojas de cálculo (Excel o similares).
- Bases de datos relacionales (SQL u OLAP).


**Datos semiestructurados**
En los datos semiestructurados el esquema no está separado de los datos, sino que
estos son autodescriptivos. Esto no significa que el esquema como tal por separado
no exista, sino que, en realidad, este es opcional los datos semiestructurados son estructuras más flexibles que los datos
estructurados y no tienen por qué ser tabulares

## Algunos ejemplos de datos estructurados serían:

- Archivos XML (extended markup language)
- Archivos HTML (hypertext markup language)
- Archivos JSON (Javascript object notation)
- Archivos YAML (YAML ain’t markup language u, originalmente, yet another markup language), 
- Tripletas RDF (resource description framework)


## Mecanismos de muestreo

En este apartado vamos a describir, en primer lugar, los principales mecanismos de
muestreo a la hora de recoger los datos desde diferentes fuentes,

**Muestro de señales y el teorema de Shannon-Nyquist**
Un ejemplo evidente es la recogida de datos desde redes de sensores o
dispositivos IoT, ampliamente utilizados en la medición de datos acerca del
entorno. 

Según el teorema de Shannon-Nyquist, si una función x(t) no contiene frecuencias
superiores a B hertzios, podemos caracterizar completamente esta si tomamos
muestras cada 1/(2B) segundos o, lo que es lo mismo, con una frecuencia igual a 2B
(Vaidyanathan, 2001). 2B es llamada también la tasa de Nyquist, mientras que la
máxima B permitida por un equipo de muestro (un conversor analógico-digital) es la
frecuencia de Nyquist.


**Muestreo de poblaciones: métodos probabilísticos, métodos no probabilísticos y sesgo**

El muestreo puede basarse en la probabilidad, un enfoque que utiliza números
aleatorios que corresponden a puntos del conjunto de datos para garantizar que no
haya correlación entre los puntos elegidos para la muestra. Entre los métodos
probabilísticos podemos destacar:

- Muestreo aleatorio simple (simple random sampling): se utiliza un proceso
informático para seleccionar aleatoriamente a los sujetos de toda la población.
Muestreo estratificado (stratified sampling): se crean subconjuntos de los conjuntos
de datos o de la población en función de un factor común y se recogen muestras al
azar de cada subgrupo (por ejemplo, estratificar la población por sexo, edad,
provincia, ingresos, etc.). Así nos aseguramos de que no tenemos, por ejemplo, un
70 % de muestras de hombres, cuando estos no representan tal porcentaje en la
población total. En caso contrario, introduciríamos un sesgo en el muestreo.

- Muestreo por conglomerados (cluster sampling): el conjunto de datos más amplio
se divide en subconjuntos (clusters o clústeres) en función de un factor definido, y
luego se analiza un muestreo aleatorio de los clústeres. Por ejemplo, crear
subconjuntos por región o por provincia o crear subconjuntos por tipos de
consumidores que se comportan de forma similar. Es importante que el
comportamiento dentro de cada clúster sea relativamente homogéneo, mientras que
el comportamiento entre individuos de diferentes clústeres sea relativamente
heterogéneo.

- Muestreo multietapa (multistep sampling): se trata de una forma más complicada
del muestreo por conglomerados. Este método también implica la división de la
población más grande en una serie de clústeres. A continuación, los clústeres de la
segunda etapa se dividen en función de un factor secundario y esos clústeres se
muestrean y analizan. Este escalonamiento podría continuar a medida que se
identifican, agrupan y analizan múltiples subconjuntos.

- Muestreo sistemático (systematic sampling): en este método, se crea una muestra
estableciendo un intervalo en el que se extraen los datos de la población mayor. Por
ejemplo, seleccionando cada 10 filas en una hoja de cálculo o un fichero CSV de
2000 elementos para crear un tamaño de muestra de 200 filas para analizar.


**El muestreo también puede basarse en la no probabilidad, un enfoque en el que
se determina y extrae una muestra de datos basada en el juicio del científico de Datos**

- Muestreo de conveniencia (convenience sampling): los datos se recogen de un
grupo fácilmente accesible y disponible.

- Muestreo consecutivo (consecutive sampling): los datos se recogen de cada sujeto
que cumple los criterios hasta alcanzar el tamaño de muestra predeterminado.

- Muestreo intencional o de juicio (purposive o intentional sampling): el investigador
selecciona los datos a muestrear basándose en criterios predefinidos.

- Muestreo por cuotas (quota sampling): el investigador garantiza una representación
equitativa dentro de la muestra para todos los subgrupos del conjunto de datos o
población.


**¿Qué es ETL?**

ETL es un proceso de integración de datos que combina, limpia y organiza datos de múltiples fuentes en un único conjunto de datos coherente para su almacenamiento en un almacén de datos, un lago de datos u otro sistema de destino.

**Las tres fases del proceso ETL son:**

**Extracción (Extract):**

- En esta fase, se extraen los datos relevantes de los sistemas de origen.
- Los sistemas de origen pueden ser bases de datos, archivos planos, aplicaciones SaaS, etc.
- Los datos se extraen utilizando diferentes técnicas, como consultas SQL, API, o scripts personalizados.

**Transformación (Transform):**

- En esta fase, los datos extraídos se limpian, validan y transforman para que sean adecuados para el análisis.
- Las transformaciones comunes incluyen:
    - Limpieza de datos: Eliminar valores nulos, duplicados o inconsistentes.
    - Validación de datos: Asegurarse de que los datos cumplen con los estándares de calidad.
    - Normalización de datos: Convertir los datos a un formato estándar.
    - Agregación de datos: Combinar múltiples fuentes de datos en una sola.
    - Enriquecimiento de datos: Agregar contexto o información adicional a los datos.

**Carga (Load):**

- En esta fase, los datos transformados se cargan en el sistema de destino.
- El sistema de destino puede ser un almacén de datos, un lago de datos, una base de datos analítica, etc.
- Los datos se cargan utilizando diferentes técnicas, como carga por lotes, carga incremental o carga directa.

## Técnicas avanzadas en la recolección de datos masivos
- OCR (Reconocimiento óptico de caracteres). Convierte documentos impresos o imágenes en texto editable y buscable, facilitando el  acceso y la manipulación de información.

- ICR (Reconocimiento Inteligente de Caracteres). Capaz de reconocer y aprender de texto manuscrito, mejorando continuamente su precisión y eficiencia en el procesamiento de datos.

- RFID (Identificación por Radiofrecuencia). Utiliza etiquetas electrónicas para el seguimiento y gestión remota de inventarios y activos mediante señales de radio.

- NFC (Near-field communication-Comunicaciones de Campo Cercano). Permite la comunicación inalámbrica entre dispositivos a corta distancia. Ampliamente usado en pagos móviles y sistemas de seguridad.

- Aplicaciones de Estas Tecnologías. Esenciales en logística, retail, seguridad y gestión de datos, mejorando significativamente la eficiencia operativa y la precisión de la información.

## Desafíos y métodos de captura en la recolección de datos masivos

- Métodos de captura.
- Optimización mediante tecnología.
- Automatización de la captura. 

## 2.2. Análisis
Es el proceso que nos permite de ciertamanera iniciar con el estudio de los datos, preparar todo para realizar calculos, comparaciones y encontrar patrones que nos apoyen a generar insights. Podemos usar algunos modelos de regresión, clasificación, segmentación y recomendación

Hay tres tipos de análisis 

- Análisis descriptivo: describe lo que ha pasado con estadísticas, gráficos, tablas e informes.

- Análisis predictivo: realiza predicciones que van a ser útiles en el futuro siendo de especial importancia la precisión de la predicción.

- Análisis prescriptivo: ayuda a entender qué debe hacerse para obtener los resultados esperados en el futuro.



## Introducción Al Análisis de Datos 
- Definición de análisis de datos: Proceso de inspeccionar, limpiar, transformar y modelar datos con el objetivo de descubrir información útil, informar conclusiones y apoyar la toma de decisiones.
- Importancia en la era digital: El análisis de datos permite a las organizaciones y empresas aprovechar el potencial de los datos masivos para optimizar operaciones, prever tendencias y mejorar la experiencia del cliente.
- Desafíos actuales: Manejar el volumen creciente de datos, asegurar la calidad y la privacidad de los datos y extraer conocimientos significativos en tiempo real.

## Preparación de datos en análisis de datos
- Evaluación de la calidad: Incluye el análisis de formatos, completitud, integridad y disponibilidad de los datos. Fundamental para garantizar la fiabilidad de los análisis posteriores.
- Tareas de preprocesamiento: Realización de transformaciones y filtrados, análisis de datos faltantes, eliminación de datos ruidosos o atípicos.
- También aparecen tareas como la discretización o categorización, encodings y la reducción de los datos.


## El papel del machine learning en el análisis de datos
- Fundamentos del Machine Learning:  Machine learning utiliza algoritmos para analizar datos, aprender de ellos y hacer predicciones o tomar decisiones.
> Se considera una disciplina de la inteligencia artificial. 
- Se especializa en el reconocimiento de patrones; sobre muestras de datos, aprende y permite extraer inferencias de nuevos conjuntos de datos para los que no ha sido entrenado previamente.
- Impacto. Permite automatizar el análisis complejo de grandes volúmenes de datos y descubrir patrones y tendencias que no son evidentes para los métodos tradicionales de análisis.

## Técnicas de machine learning datos

- Técnicas comunes. Incluye métodos de análisis supervisado:
    - Regresión 
    - Clasificación
    - No supervisado. 
- Clustering, reducción de dimensiones y sistemas de recomendación, aplicables a diferentes tipos de datos y necesidades. También encontramos técnicas de aprendizaje semi-supervizado aprendizaje por refuerzo.
- Procesamiento del Lenguaje Natural. El ML es crucial en el procesamiento del lenguaje natural, ayudando a interpretar y generar lenguaje humano de manera efectiva.

## Visualización en el análisis de grandes volúmenes
- Propósito de la visualización: Transformar grandes volúmenes de datos en representaciones gráficas que facilitan la comprensión y la toma de decisiones rápidas.
- Herramientas esenciales: Herramientas de BI como Tableau, Qlik Sense, Looker, Domo, Power BI y Google Data Studio son plataformas líderes que permiten a los usuarios crear visualizaciones dinámicas y dashboards interactivos.
- Impacto en el análisis de datos: Mejora significativamente la accesibilidad y la interpretación de datos complejos, permitiendo a los analistas y stakeholders obtener insights valiosos de manera eficiente.

## 2.3. Visualización

Es el proceso mediante el cual la información analizada se presenta de manera gráfica. Esta fase es esencial porque transforma grandes cantidades de datos complejos en representaciones visuales más accesibles y comprensibles, como gráficos, diagramas y mapas de calor

## 2.4. Interpretación

Finalmente, la interpretación de los datos es el último paso, donde los resultados visualizados se examinan para tomar decisiones informadas


## Visualización efectiva de datos 

- Fundamentos de la Visualización: Consiste en transforma un conjunto complejo de datos en representaciones graficas intuitivas. 
- Estrategias para Visualización efectiva: Incluyen el uso de colores adecuados, diseño limpio y la seleccion de tipos de graficos que mejor represente los datos. 
- Herramienta Clave: Uso de herramienta Tableu, Power BI, Goggle Data Studio. 



# Técnicas para la interpretación de datos
- Análisis de sensibilidad: Evalúa cómo diferentes inputs afectan los resultados de un modelo, ayudando a identificar variables críticas y a entender la robustez de los modelos.
- Técnicas de visualización avanzadas: Incluye el uso de mapas de calor, gráficos de red y visualizaciones tridimensionales para una representación más rica y detallada de los datos.
- Discusión crítica y revisión por pares: Fomenta la evaluación y validación de los resultados de análisis mediante la discusión con expertos y colegas, asegurando la precisión y objetividad.

## Cuestionario 

1. ¿Cómo clasificarías un fichero JSON en función de su organización?
    A. Datos estructurados.
    B. Datos semiestructurados.        -> Correcto 
    C. Datos completamente estructurados.
    D. Ninguna de ellas.

2. ¿Cómo clasificarías una base de datos relacional en función de su organización?
    A. Datos estructurados.    --> Correcto 
    B. Datos semiestructurados.
    C. Datos completamente estructurados.
    D. Ninguna de ellas.

3. ¿Cuáles de los siguientes serían ejemplos de datos no estructurados?
    A. Imágenes, vídeos y sonidos. --> Correctos 
    B. Bases de datos SQL y OLAP.
    C. Archivos JSON y XML.
    D. Archivos CSV.

4. Si estoy creando un instrumento para capturar secuencias de voz humana para
su posterior procesamiento, ¿a qué frecuencia mínima debería muestrear el audio
para no tener aliasing?
    A. 10 Hz.
    B. 4 KHz.
    C. 8000 Hz.
    D. 16 KHz. --> Correcto 

5. ¿En qué método de muestreo probabilístico se crean subconjuntos de los
conjuntos de datos o de la población en función de un factor común y se recogen
muestras al azar de cada subgrupo?
    A. Muestro multietapa.
    B. Muestro sistemático.
    C. Muestreo por conglomerados.
    D. Muestro estratificado. ---> Correcto 


- Muestreo estratificado: En este método, la población se divide en subgrupos homogéneos (estratos) basados en una característica relevante para la investigación (edad, género, nivel de ingresos, etc.). Luego, se selecciona una muestra aleatoria de cada estrato, asegurando así que cada subgrupo esté representado en la muestra final.

- Muestreo multietapa: Implica seleccionar muestras en varias etapas, como seleccionar primero estados, luego ciudades, y finalmente hogares. No se enfoca en crear subgrupos homogéneos.

- Muestreo sistemático: Se selecciona un punto de partida aleatorio y luego se seleccionan elementos a intervalos regulares de la lista de población. No implica la creación de subgrupos.

- Muestreo por conglomerados: Se divide la población en grupos heterogéneos (conglomerados) y se selecciona una muestra aleatoria de conglomerados. Luego, se incluyen todos los elementos de los conglomerados seleccionados. A diferencia del estratificado, los conglomerados son heterogéneos.


6. En el método de muestreo no probabilístico intencional:
    A. Los datos se recogen de un grupo fácilmente accesible y disponible.
    B. Los datos se recogen de cada sujeto que cumple los criterios hasta
    alcanzar el tamaño de muestra predeterminado.
    C. El investigador selecciona los datos a muestrear basándose en criterios predefinidos. -->Correcto 
    D. El investigador garantiza una representación equitativa dentro de la
    muestra para todos los subgrupos del conjunto de datos o población.


Opción A: Corresponde más al muestreo por conveniencia, otro tipo de muestreo no probabilístico, en el cual se seleccionan los participantes más accesibles.
Opción B: Esta opción describe más un tipo de muestreo por cuotas, en el que se establecen cuotas para cada subgrupo y se seleccionan participantes hasta cumplirlas.
Opción C: Esta es la definición precisa del muestreo intencional. El investigador utiliza su juicio para seleccionar a los participantes que considera más adecuados para el estudio, basándose en criterios específicos relacionados con los objetivos de la investigación.
Opción D: La representación equitativa es una característica de los métodos de muestreo probabilístico, no de los no probabilísticos.


7. Los datos recogidos por el propio científico de datos mediante encuestas se
considerarían:
    A. Fuentes de datos primarias.  --> Correcto 
    B. Fuente de datos secundarias.
    C. Fuentes de datos terciarias.
    D. Ninguna de las demás respuestas es correcta.

- Fuentes de datos primarias: Son aquellos datos que son recolectados directamente por el investigador para un propósito específico. En este caso, el científico de datos está recolectando los datos a través de encuestas, lo que significa que él es la fuente original de esa información.
- Fuentes de datos secundarias: Son datos que ya han sido recopilados por alguien más y que el investigador utiliza para su análisis. Ejemplos incluyen datos de censos, informes de empresas o investigaciones previas.
- Fuentes de datos terciarias: Se refieren a materiales que indexan o resumen fuentes primarias y secundarias. Un ejemplo sería un directorio de revistas científicas.


8. El método de captura de datos de forma automatizada mediante el procesamiento de páginas HTML de un sitio web se conoce como:
    A. Web semántica.
    B. Web service.
    C. Web scraping. --> Correcto 
    D. Ninguna de las demás respuestas es correcta.

9. ¿En qué categoría de captura o fuente de datos encaja la lectura de información
del pulso cardíaco por una pulsera de actividad?
    A. Captura manual.
    B. Sensores.  
    C. Captura automatizada.
    D. B y C son correctas. --> Correcto 

10. La infografía y la visualización de datos tienen como objetivo principal:
    A. Presentar la información de una manera muy atractiva. --> Correcto 
    B. Informar y ampliar el conocimiento.
    C. Mostrar una información diferente al lector.
    D. Buscar y organizar datos.


## Tema 3. Arquitecturas Típicas en Proyectos de Datos Masivos

Fuentes Heterogenias de Datos 
- Tipos de Datos: 
    - Estructruturados: bases de datos SQL hojas de calculo 
    - Semiestructurados: documentos XML, json 
    - No estructurados: texto, imagenes, audio, video
    - Datos tiempo real:sensores datos GPS


- Desafios de Integración: La diversidad de formatos y tipos de datos, junto con la necesidad de mantener la calidad y consistencia  de los datos integrados. 
- Tecnologías y estrategias: uso de sistemas de gestion de datos flexibles, herramientas ETL, plataforma de big Data como Hadoop y analisis en tiempo real 
- Ejemplo de usos: Los bancos utilizan datos estructurados para el analisis de credito, Las redes sociales analizan datos no estructurados para insights sobre el comportamiento de usuarios.  


## Proceso ETL
- Extracción: recopilación de datos de diversas fuentes heterogéneas como bases de datos relacionales, NoSQL, archivos y los API-
- Transformación: limpieza, integración y transformación de datos. Aplicación de reglas y operaciones para convertir datos al formato adecuado.
- Carga: almacenamiento de datos transformados en sistemas de almacenamiento como data warehouses o data lakes.
- Herramientas: ejemplos de herramientas ETL incluyen Apache Nifi, Talend, Informatica, AWS Glue.


## Metodos de Extracción 


- Extracción en línea (online): los datos se recuperan directamente del sistema fuente. Puede acceder a tablas de origen o sistemas centrales que almacenan datos predefinidos. No difiere físicamente del sistema fuente.
- Extracción fuera de línea (offline): los datos se organizan fuera del sistema fuente original. Pueden ser registros de rehacer, archivos o creados por rutinas de extracción. Importancia de la recurrencia en extracciones incrementales o completas.
- Ejemplos de uso:
    - Online: recuperación directa de bases de datos operacionales. 
    - Offline: uso de archivos de registro o archivos de backup para extracción de datos.

- Consideraciones: los volúmenes de datos pueden ser grandes. La elección del método depende de la necesidad de frescura («edad») de datos y la infraestructura disponible.


## Transformación y tratamiento de datos
- Limpieza de datos: eliminación de errores, duplicados y datos inconsistentes para asegurar la integridad y calidad.
- Transformación: aplicación de reglas y operaciones para convertir datos a un formato adecuado para análisis.
- Integración: unificación de datos de diferentes fuentes para crear una vista coherente y completa.
- Validación: verificación de que los datos transformados cumplen con los estándares y requisitos definidos.


## Proceso de carga
- Importancia de la carga: incluye tanto la carga de tablas de dimensiones como de tablas de hechos. Asegurar que la carga se realice correctamente y con la menor cantidad de recursos posibles.
- Mejores prácticas: deshabilitar restricciones e índices antes de la carga y habilitarlos después. Mantener la integridad referencial durante el proceso de carga.

- Tipos de carga:
    - Carga inicial: poblar todas las tablas del data warehouse.
    - Carga incremental: aplicar cambios periódicos según sea necesario. 
    - Refresco completo: borrar y recargar una o más tablas con datos frescos.

- Destino principal: la base de datos del proceso de carga. Garantizar que el destino sea eficiente y mantenga la coherencia e integridad de los datos.


## Proceso ETL en la era actual
- Extracción moderna: uso de los API, servicios web y conectores avanzados para recopilar datos en tiempo real de diversas fuentes, incluyendo loT y redes sociales.
- Transformación avanzada: aplicación de machine learning para limpieza y enriquecimiento de datos. Uso de herramientas como Apache Spark para procesamiento distribuido.
- Carga eficiente: implementación en la nube con servicios como AWS Glue y Azure Data Factory, que permiten escalabilidad y flexibilidad en la gestión de grandes volúmenes de datos.
- Automatización: automatización de flujos de trabajo ETL con orquestadores como Apache Airflow, mejorando la eficiencia y reduciendo errores humanos.

## Resumen 
Los ETL ayudan a las empresas a integrar datos de multiples fuentes de datos, transformarlos de manera efectiva para luego almacenarlos en data warehouse para mejorar la toma de decisiones. 

Un Data Warehouse está optimizado para consultas rápidas y complejas, lo que mejora su rendimiento para análisis específicos.

La etapa de transformación implica limpiar, integrar y convertir los datos para que sean adecuados para el análisis.

La automatización de flujos de trabajo ETL mejora la eficiencia y reduce errores humanos, utilizando herramientas avanzadas.



## Videoclase 2. Almacenamiento de Datos

## Bases de datos Relacionales 
- Organización: datos almacenados en tablas con filas y columnas. Ideales para almacenar datos estructurados.
- Ventaja: integridad. Aseguran la consistencia y precisión mediante claves primarias y relaciones facilidad de uso, soporte amplio.
- SQL: utilizan el lenguaje de consulta estructurado (SQL) para gestionar y manipular datos. Permiten realizar consultas complejas.
- Normalización: estructuración para reducir redundancia y mejorar integridad de los datos.
- Desventajas: No son adecuadas para datos no estructurados o semiestructurados.


## Data Lake
- Almacenamiento: capacidad de almacenar grandes volúmenes de datos en diversos formatos.
- Flexibilidad: permiten almacenar datos sin procesar y definir el esquema durante el acceso. Almacena datos en su formato nativo, sin procesar. Puede contener datos estructurados, semiestructurados y no estructurados (schema-on-read)
- Costo: generalmente más económicos debido a su arquitectura escalable.
- Accesibilidad: datos accesibles para múltiples aplicaciones y análisis.
- Desventajas: riesgo de convertirse en un Data Swamp (Data Lake mal gestionado, donde los datos se almacenan sin un orden o metadatos adecuados).
- Un Data Lake puede convertirse en un Data Swamp si los datos se almacenan sin orden ni metadatos adecuados.

## Data Warehouses
- Almacenamiento: datos organizados en estructuras optimizadas para consultas y análisis (Schema-on-write), donde los datos deben cumplir con un esquema definido antes de ser almacenados.
- ETL: procesos de extracción, transformación y carga (ETL) para asegurar la calidad y consistencia de los datos.
- Ventajas: diseño optimizado para realizar consultas rápidas y complejas.. Alta calidad de datos, optimización para consultas analíticas.
- Rendimiento: alta velocidad y eficiencia en la ejecución de consultas. Desventajas: mayor rigidez comparado con Data Lakes, mayor costo de implementación.

## Data Mart
- Un Data Mart es una versión más pequeña y especializada de un Data Warehouse, enfocada en un área específica de la organización. Contiene una parte pequeña y específica de los datos que la empresa almacena en un sistema de almacenamiento más grande.
- Ventajas: menor tiempo de implementación, específico para necesidades de departamentos.
- Desventajas: puede llevar a la creación de silos de datos.

## Data Mesh (una alternativa de almacenamiento)
- Descentralización del almacenamiento de datos: permite a los equipos independientes gestionar y almacenar sus propios datos. Data Lake tiene un enfoque centralizado en un único repositorio que almacena todos los datos, mientras que el Data Mesh distribuye los datos entre varios dominios.
- Escalabilidad y flexibilidad: mejora la escalabilidad y flexibilidad al distribuir la gestión de datos.
- Responsabilidad de los datos: cada equipo es responsable de la calidad y consistencia de sus datos.
- Interoperabilidad: facilita la interoperabilidad entre diferentes sistemas y fuentes de datos.

## Tecnologías de almacenamiento en la nube
- Escalabilidad: capacidad de escalar recursos de almacenamiento según la demanda.
- Costo: modelos de pago por uso, permitiendo costos más bajos y predecibles.
- Accesibilidad: acceso a datos desde cualquier lugar y en cualquier momento.
Elasticidad: ajuste dinámico de recursos según las necesidades del negocio.
- Herramientas:
o AWS S3: almacenamiento de objetos escalable y de alta durabilidad. o Microsoft Azure Blob Storage: almacenamiento de objetos optimizado para datos no estructurados.
o Google Cloud Storage: solución unificada de almacenamiento de objetos para desarrolladores y empresas.


## Impacto en el análisis de datos
- Data Lakes:
    - Exploración de datos: facilitando el análisis exploratorio y la investigación.
    - Machine Learning: soporte para grandes volúmenes de datos no estructurados, ideal para entrenar modelos de ML.
    - Flexibilidad: permite adaptarse rápidamente a nuevas fuentes y tipos de datos.

- Data Warehouses:
    - Análisis de BI: optimización para consultas rápidas y generación de informes.
    - Integridad de datos: garantiza la calidad y consistencia de los datos.
    - Rendimiento: diseñado para manejar grandes volúmenes de consultas concurrentes.

## Videoclase 3. Visualización Avanzada de Datos

## Propósito de la visualización de datos
- Claridad:
    - Facilita la comprensión de grandes volúmenes de datos. Simplifica la complejidad de los datos crudos.
    - Ejemplos: diagramas de dispersión, gráficos de líneas. Comunicación efectiva:
    - Permite compartir insights de manera clara y concisa. Mejora la colaboración entre equipos.
    - Ejemplos: Dashboards interactivos.
    
- Detección de patrones:
    - Ayuda a identificar tendencias y patrones ocultos en los datos.
    - Facilita la identificación de anomalías.
    - Ejemplos: mapas de calor, gráficos de series temporales.

- Toma de decisiones:
    - Apoya la toma de decisiones basada en datos concretos y visuales.
    - Reduce la incertidumbre en la toma de decisiones.
    - Ejemplos: informes de Business Intelligence (BI).

## Herramientas de la visualización de datos
- Datawrapper:
    - Programa de código abierto para crear visualizaciones fácilmente. Ideal para periodistas y comunicadores.

- Timeline JS:
    - Construcción de líneas de tiempo interactivas.
    - Utilizado en presentaciones históricas y narrativas.

- RAWGraphs:
    - Visualizaciones con D3.js sin necesidad de programación. Flexible y altamente personalizable.

- CartoDB:
    - Mapas interactivos en la web.
    - Análisis geoespacial avanzado.

- Visualización e inteligencia empresarial
    - AWS, Microsoft Azure, Google Cloud Platform:
    - Integración de fuentes de datos y procesamiento.
    - Infraestructura escalable y segura.

- Tableau:

    - Conexión con diversas fuentes de datos y creación de cuadros de mando.
    - Interfaz intuitiva y potente capacidad de análisis.
    - Capacidad de manejar grandes volúmenes de datos.

- Google Data Studio:
    - Dashboards gratuitos y conectores de terceros.
    - Fácil de usar y compartir.
    - Interactividad y personalización.

- Power BI:
    - Visualizaciones de inteligencia empresarial con reconocimiento por voz.
    - Integración con Microsoft Office y otras herramientas.
    - Análisis en tiempo real y predicciones.



## Otras herramientas de visualización
- Looker:Solución de inteligencia empresarial de Google. Análisis de datos en tiempo real y exploración.
- Qlik:Visualización y análisis de datos IoT. Potente motor de asociación de datos.
- Adverity:Plataforma para ingestión, procesamiento y presentación de datos. Conexión a múltiples fuentes de datos y automatización de ETL.
- Funnel:Conexión de datos de múltiples fuentes con destinos diversos. Simplificación del marketing digital y análisis de rendimiento.

## Lenguajes de Programación para Visualización
    - Python:
        - Librerías: Matplotlib, Seaborn, Plotly, Bokeh.
        - Usado en machine learning, data science, inteligencia artificial.
        - Amplia comunidad y numerosas librerías.
        - Integración con proyectos de machine learning.

    - R:
        - Librerías: ggplot2, Shiny, Plotly.
        - Popular en análisis estadístico y académico.
        - Potente en análisis estadístico.
        - Interactividad y extensibilidad.


## Python y visualization
Matplotlib:
Capacidad de personalización avanzada.
Seaborn:
Mejora la estética y la capacidad gráfica. Se integra con R y Python.
Plotly:
Gráficos interactivos en web.
Compatible con Dash para dashboards interactivos.
Bokeh:
Visualización interactiva de datos a gran escala. Utilizado en análisis de grandes conjuntos de datos.

## R y visualization
- ggplot2:
    - Gráficos en una única figura.
    - Extensibilidad y personalización.
- Shiny:
    - Aplicaciones web interactivas para visualización de datos.
    - Facilitación de la creación de dashboards.
- Plotly en R:
    - Gráficos interactivos y altamente personalizables.
    - Integración con aplicaciones Shiny.
- Paquetes adicionales:
    - rgl para gráficos 3D.
    - Leaflet para mapas interactivos.

## Desafíos en la visualización de datos
- Desafíos:
    - Gestión de grandes volúmenes de datos.
    - Garantizar la precisión y calidad de los datos.
    - Elegir herramientas según necesidades específicas.
    - Considerar factores como usabilidad, costo y capacidades.
    - Formación continua en nuevas herramientas y tecnologías. Importancia de la alfabetización de datos.
    - Integrar visualización de datos en la estrategia empresarial.
    - Fomentar una cultura de datos y toma de decisiones basada en datos.

- Algunas soluciones:
    - Uso de herramientas avanzadas de visualización.
    - Implementación de buenas prácticas de gobernanza de datos.

- Tecnologías de apoyo:
    - Big Data, Machine Learning, inteligencia artificial. Integración con plataformas de análisis de datos.

- Beneficios:
    - Mejor toma de decisiones.
    - Eficiencia operativa y reducción de costos.

## Cuestionario 

1. ¿Qué nombre recibe un conjunto de datos persistente utilizado por un sistema de software?
    A. Archivo.
    B. Base de datos.
    C. Registro.
    D. Las respuestas A y B son correctas. -> Correcto  

2. ¿Qué tipo de datos puede almacenar un data warehouse?
    A. Datos estructurados.   --> Correcto 
    B. Datos no procesados.
    C. Las respuestas A y B son correctas.
    D. Ficheros planos.

3. Al proceso de utilizar métodos de minería de datos para extraer lo que se
considera conocimiento según la especificación de medidas y umbrales, utilizando
una base de datos junto con procesos de transformación de los datos se lo conoce
como:
    A. CRISP-DM.
    B. ETL.
    C. KDD.
    D. Machine learning.  --> Correcto 


4. Entre las ventajas de la preparación de los datos nos encontramos con las
siguientes (marca todas las correctas):
    A. Preparar los datos para el análisis de forma rentable y eficiente.
    B. Garantizar que los datos utilizados para el BI tengan niveles de calidad
    suficientes.
    C. Crear duplicados de los datos para que puedan utilizarse en múltiples
    aplicaciones de forma segura.
    D. Todas son correctas.


5. La limpieza de datos corrige problemas como:
    A. Datos duplicados.
    B. Datos redundantes. --> Correcto 
    C. Datos no estructurados.
    D. Datos incoherentes.

6. ¿Cuáles de los siguientes repositorios de datos almacenan datos no
estructurados, semiestructurados y estructurados?
    A. Data warehouses.
    B. A y C son correctas.
    C. Data lakes.
    D. Data marts.

7. ¿Cuáles de los siguientes repositorios de datos siguen una estructura de
procesamiento schema on write? Marca todas las correctas:
    A. Data swamps.
    B. Data warehouses.
    C. Data lakes.
    D. Data marts.

8. ¿Cuál de las siguientes afirmaciones describe mejor el propósito del proceso
ETL?
    A. Procesar eventos en tiempo real para análisis instantáneo.
    B. Integrar datos de múltiples fuentes en un formato homogéneo.
    C. Enviar y recibir mensajes entre diferentes aplicaciones.
    D. Visualizar datos para reportes y paneles de control.

9. ¿Cuál es una fase crítica del proceso ETL donde se aplican reglas para corregir o
eliminar datos incorrectos o incompletos?
    A. Extracción.
    B. Transformación.
    C. Carga.
    D. Almacenamiento.

10. ¿Qué aspecto del proceso ETL se enfoca principalmente en mejorar el
rendimiento de las consultas y la escalabilidad del sistema de almacenamiento de
datos?
    A. Optimización de la extracción.
    B. Paralelización de la transformación.
    C. Incremento en la frecuencia de carga.
    D. Diseño del esquema de datos





## Tema 4:  El Perfil del Científico de Datos

En el mundo actual, dominado por datos y tecnología, el papel del científico de datos
se ha vuelto indispensable en las organizaciones que buscan capitalizar la vasta
cantidad de información disponible. Este perfil profesional combina habilidades en
ciencias de la computación, matemáticas y estadística, comunicación y
conocimientos de negocios para extraer patrones significativos, predecir tendencias
futuras y proporcionar recomendaciones basadas en datos que impulsan las
decisiones estratégicas.

La importancia de los científicos de datos radica en su capacidad para no solo
manejar grandes volúmenes de datos sino también en transformar estos datos en
métricas accionables que pueden traducirse en ventajas competitivas para las
empresas

Los científicos de datos actúan como puentes entre los datos técnicos y las
decisiones de negocio, empleando su experiencia técnica para solucionar problemas
complejos y comunicando sus hallazgos de manera efectiva a los stakeholders de la
empresa. En una era donde los datos se generan a una velocidad y volumen sin
precedentes, estos profesionales son clave para navegar por el ruido informativo y
descubrir la información valiosa que subyace.

## Ciencias de la computación
En el campo de la ciencia de datos, las ciencias de la computación desempeñan un
papel esencial debido a que proporcionan la base técnica que permite el análisis y
manejo eficaz de grandes volúmenes de datos. La ciencia de la computación ofrece
las herramientas y técnicas necesarias para crear sistemas capaces de procesar,
almacenar y analizar datos a una escala sin precedentes.



## Importancia de la Ciencia de la Computación en la Ciencia de Datos

## Desarrollo de Algoritmos y Modelos de Aprendizaje Automático: 

- Los científicos de datos utilizan su conocimiento en ciencias de la computación para desarrollar algoritmos complejos. 
- Se desarrollan modelos que son fundamentales para transformar grandes
conjuntos de datos en insights accionables que pueden influir en decisiones críticas
de negocio.

## Gestión de Grandes Volúmenes de Datos: 

- Se necesitan habilidades en bases de datos, estructuras de datos y algoritmos, los científicos de datos están equipados para manejar y optimizar bases de datos y sistemas de almacenamiento de datos, asegurando que los datos se almacenen de manera eficiente y sean accesibles para el análisis.

## Optimización del Rendimiento de las Consultas: 

- El conocimiento en ciencias de la computación también permite a los científicos de datos optimizar las consultas a bases de datos para mejorar el rendimiento y la velocidad del análisis de datos, lo cual es crucial en entornos empresariales donde el tiempo de respuesta es crítico.

## Desarrollo de Software y Herramientas de Análisis: 

- La programación es una habilidad central en ciencias de la computación que los científicos de datos utilizan para escribir scripts y desarrollar software que automatiza la recopilación, el procesamiento y el análisis de datos.



## Videoclase 1. Competencias Fundamentales del Científico de Datos

## Notas: 
- La opción correcta es porque el científico de datos utiliza su conocimiento para convertir datos en información valiosa, apoyando así la toma de decisiones estratégicas en la empresa.   

- La opción correcta es porque las habilidades cuantitativas incluyen el uso de técnicas matemáticas, estadística e informática para analizar datos, identificar patrones y validar hipótesis. 

- La opción correcta es D porque el análisis de series de tiempo permite identificar patrones temporales y realizar predicciones, fundamental en sectores como finanzas y economía. 

- La opción correcta es porque la explicabilidad ayuda a entender las decisiones de los modelos de machine learning, asegurando su transparencia y confiabilidad. 

- La opción correcta es porque el científico de datos necesita habilidades en programación y manejo de datos a gran escala.

## Videoclase 2. La Comunicación en Ciencia de Datos

## Notas
- La opción correcta es porque la comunicación efectiva permite que los hallazgos sean entendidos por todos los interesados, facilitando la toma de decisiones. 

- La opción correcta es porque la comunicación visual utiliza representaciones gráficas para facilitar la comprensión de datos complejos

- La opción correcta es porque la comunicación escrita debe ser clara, detallada y estructurada para documentar y compartir hallazgos de manera efectiva

- La opción correcta es porque la integridad implica representar los datos de manera precisa, asegurando que no se distorsione la información

- La opción correcta es porque cada sector requiere un enfoque de comunicación distinto para garantizar que los datos sean relevantes y comprendidos por las partes interesadas

## Videoclase 3. Aplicaciones Prácticas en Negocios

## Notas
- La opción correcta es porque la ciencia de datos permite personalizar el contenido en función de las preferencias de los usuarios, mejorando la retención.
- La opción correcta es porque Amazon utiliza algoritmos para prever la demanda y optimizar la logística, mejorando su eficiencia operativa
- La opción correcta es porque Starbucks utiliza ciencia de datos para seleccionar ubicaciones estratégicas que maximicen la visibilidad y el tráfico.
- La opción correcta es porque Goldman Sachs utiliza modelos predictivos para manejar riesgos financieros y tomar decisiones informadas. 
- La opción correcta es porque Uber optimiza rutas y tiempos de espera usando algoritmos avanzados, mejorando la experiencia del usuario.



## Habilidades 

- Predicción de Demanda: utiliza modelos estadísticos y de series temporales para
prever la demanda futura de productos o servicios, permitiendo a las empresas
ajustar la producción, el inventario y la planificación de la logística.

- Sistemas de Recomendación: emplea técnicas de álgebra lineal y algoritmos de
aprendizaje automático para recomendar productos, películas o música a los
usuarios basándose en sus intereses y comportamientos pasados.

- Detección de Fraude: aplica algoritmos de clasificación y patrones estadísticos para
identificar transacciones o comportamientos anómalos que puedan indicar fraude en
sectores como banca y seguros.

- Optimización de Rutas: usa algoritmos de optimización para determinar la ruta más
eficiente en términos de costos y tiempo para la entrega de mercancías o la
planificación de rutas de transporte público.

- Análisis de Sentimiento: implementa modelos matemáticos para analizar y clasificar
opiniones de los usuarios en datos textuales, como reseñas o comentarios en redes
sociales, determinando si son positivas, negativas o neutrales.

- Segmentación de Mercado: utiliza técnicas de clustering y análisis de componentes
principales (PCA) para identificar segmentos de clientes basados en características
similares, lo que ayuda a las empresas a dirigir sus estrategias de marketing de
manera más efectiva.

- Evaluación de Riesgos: emplea modelos de regresión y simulaciones Monte Carlo
para evaluar y cuantificar los riesgos financieros, como el crédito o el mercado,
ayudando a las instituciones financieras en la toma de decisiones.

- Modelado de Propagación de Enfermedades: aplica modelos matemáticos de
epidemiología para predecir la propagación de enfermedades y evaluar la efectividad
de las intervenciones de salud pública.

- Análisis de Redes Sociales: usa teoría de grafos y algoritmos para analizar redes
sociales, identificando patrones de conexión, influenciadores clave y comunidades
dentro de las redes.

- Valoración de Activos: emplea modelos financieros y estadísticos para determinar el
valor justo de diversos activos, incluyendo acciones, bonos y derivados

## Comunicación

- La comunicación es una habilidad crítica en la ciencia de datos, vital para el éxito de cualquier proyecto de análisis de datos. 

- El científico de datos no solo necesita ser competente en técnicas estadísticas y de programación, sino también en la habilidad de comunicar hallazgos complejos de manera clara y persuasiva a un público diverso. 

- Comunicación de Resultados

    - Esto implica presentar los datos de manera que resalten las conclusiones clave sin perderse en detalles técnicos innecesarios.

    - Herramientas como visualizaciones de datos, dashboards interactivos y presentaciones claras son fundamentales. 
    
    - Por ejemplo, un científico de datos puede usar una visualización de gráfico de calor para demostrar áreas de alta actividad en un estudio de mercado, facilitando la comprensión rápida de datos complejos.


- Presentación de Avances de los Proyectos de Datos

    - Durante la gestión de proyectos de datos, comunicar los avances de manera efectiva asegura que todas las partes interesadas estén informadas sobre el progreso, los desafíos y los cambios en los objetivos del proyecto. 
    
    - Esto es crucial para mantener alineados a todos los miembros del equipo y para gestionar las expectativas de los
    stakeholders. 
    
    - Ejemplo de esto sería la actualización periódica a través de reuniones
    regulares donde se presentan métricas de progreso y se discuten las necesidades de
    ajustes en la estrategia o recursos del proyecto.

- Impacto de los Proyectos en la Sociedad

    - Los científicos de datos también tienen la responsabilidad de comunicar cómo los proyectos de datos impactan en la sociedad. 

    - Un científico de datos eficaz debe ser capaz de narrar una historia con los datos. 

    - Por ejemplo, en proyectos que involucran datos de salud pública, es vital comunicar cómo se manejan y protegen los datos para evitar preocupaciones sobre privacidad.


## Aspectos relevantes de la comunicación según el ámbito


- Sector Sanitario
    - En el ámbito sanitario, la comunicación debe manejar con cuidado la privacidad y la sensibilidad de la información personal de salud. 

- Sector Gubernamental
    - La comunicación en el sector gubernamental debe ser transparente y diseñada para fomentar la confianza pública.

- Ética en Ciencia de Datos
    - La comunicación en temas de ética implica discutir cómo se manejan los datos y las implicaciones de los proyectos de ciencia de datos. 
    - Esto incluye temas como el sesgo en los algoritmos y la equidad en el análisis de datos.

- Sostenibilidad
    - En el campo de la sostenibilidad, la comunicación se centra en cómo los proyectos de datos pueden ayudar a resolver problemas ambientales o mejorar la eficiencia de recursos.

## Notas: 

- Análisis de sentimiento:
    - Concepto: Es una técnica que utiliza la inteligencia artificial para determinar la opinión o emoción expresada en un texto. Puede ser positiva, negativa o neutral. Imagina que tienes miles de reseñas de un producto en Amazon, el análisis de sentimiento te permitiría identificar rápidamente si la mayoría de los clientes están satisfechos o no.
    
    - Ejemplo: Una empresa de aerolíneas utiliza el análisis de sentimiento para monitorear las opiniones de sus clientes en redes sociales. De esta manera, pueden identificar rápidamente problemas en sus servicios y tomar medidas correctivas.

- Algoritmos predictivos:

    - Concepto: Son modelos matemáticos que utilizan datos históricos para predecir futuros resultados. Son como adivinos, pero basados en datos. Por ejemplo, pueden predecir la demanda de un producto en un mes determinado, o la probabilidad de que un cliente abandone una empresa.

    - Ejemplo: Un banco utiliza algoritmos predictivos para identificar a los clientes con mayor probabilidad de solicitar un préstamo. De esta manera, pueden ofrecerles productos financieros personalizados.

- Minería de texto:

    - Concepto: Es el proceso de extraer información útil y significativa de grandes volúmenes de texto no estructurado. Imagina que tienes una enorme base de datos de correos electrónicos, la minería de texto te permitiría encontrar patrones, temas y tendencias en esos correos.
    - Ejemplo: Una empresa de investigación de mercado utiliza la minería de texto para analizar las transcripciones de entrevistas con clientes. De esta manera, pueden identificar las principales preocupaciones de sus clientes y mejorar sus productos y servicios.

- Análisis de redes sociales:

    - Concepto: Es el estudio de las interacciones sociales y el contenido generado en las redes sociales. Permite entender cómo se comporta una comunidad en línea, identificar influyentes y analizar la opinión pública sobre un tema determinado.
    - Ejemplo: Una marca de ropa utiliza el análisis de redes sociales para monitorear las conversaciones sobre sus productos en Twitter e Instagram. De esta manera, pueden identificar a los usuarios que influyen en la opinión de otros y colaborar con ellos para promocionar sus productos.


- Clustering:
    - Concepto: Es una técnica de aprendizaje no supervisado que busca agrupar un conjunto de datos en subconjuntos (clusters) de manera que los elementos dentro de cada grupo sean más similares entre sí que con los elementos de otros grupos.
    - Objetivo: Identificar patrones ocultos en los datos y crear categorías naturales.
    - Ejemplo: Segmentar a los clientes de una empresa en grupos con características similares (por ejemplo, edad, ingresos, hábitos de compra) para diseñar estrategias de marketing más efectivas.


- Regresión lineal:
    - Concepto: Es una técnica estadística que busca modelar la relación lineal entre una variable dependiente (a la que queremos predecir) y una o más variables independientes.
    - Objetivo: Predecir valores futuros de la variable dependiente basándose en los valores de las variables independientes.
    - Ejemplo: Predecir el precio de una vivienda en función de su tamaño, ubicación, número de habitaciones, etc.

- Aprendizaje automático:
    - Concepto: Es un subcampo de la inteligencia artificial que se enfoca en desarrollar algoritmos y modelos estadísticos que permiten a las computadoras aprender de los datos y realizar tareas sin ser programadas explícitamente para cada tarea
    - Objetivo: Crear sistemas inteligentes capaces de tomar decisiones, hacer predicciones y aprender de nuevas experiencias.
    - Ejemplo: Reconocimiento de voz, detección de fraudes, sistemas de recomendación.

- Análisis factorial:
    - Concepto: Es una técnica estadística que busca reducir la dimensionalidad de un conjunto de datos, identificando un número menor de variables latentes (factores) que explican la mayor parte de la variabilidad en los datos originales.
    - Objetivo: Simplificar la interpretación de los datos y descubrir las estructuras subyacentes que relacionan a las variables observadas.
    - Ejemplo: Identificar los factores que influyen en la satisfacción del cliente en una encuesta, reduciendo un gran número de preguntas a un conjunto más pequeño de factores subyacentes.


- Clustering: sirve para agrupar datos.
- Regresión lineal: se utiliza para predecir valores.
- Aprendizaje automático es un campo más amplio que abarca diversas técnicas para crear sistemas inteligentes.
- Análisis factorial: reduce la complejidad de los datos y revela estructuras latentes.


- la paralelización de procesos: En el ámbito de la ciencia de la computación, especialmente cuando se trabaja con grandes volúmenes de datos, la paralelización de procesos es una técnica esencial.
    - Aumento de velocidad: Al dividir un problema en múltiples subproblemas y asignarlos a diferentes procesadores o núcleos de un procesador, se pueden resolver tareas complejas en mucho menos tiempo.
    - Escalabilidad: La paralelización permite aprovechar el poder de cálculo de sistemas multi-core y clusters, lo que es fundamental para manejar conjuntos de datos cada vez más grandes.
    - Eficiencia: Al aprovechar al máximo los recursos computacionales disponibles, se optimiza el uso de hardware y se reducen los costos.

- Programación funcional: Aunque es un paradigma de programación poderoso, no es específico para el procesamiento de grandes volúmenes de datos.
- Algoritmos de ordenamiento: Son importantes para organizar datos, pero no resuelven el problema de procesar grandes volúmenes de datos en sí mismos.
- Uso de variables estáticas: Las variables estáticas tienen un alcance específico y no están directamente relacionadas con la capacidad de procesar grandes conjuntos de datos.


- El análisis de cluster: En el contexto del marketing digital, esto significa que podemos agrupar a los usuarios en segmentos basados en sus comportamientos, intereses, demografía y otras variables relevantes.
- Cálculo integral: Se utiliza para calcular áreas bajo curvas y volúmenes, no para agrupar datos.
- Álgebra lineal: Se utiliza para resolver sistemas de ecuaciones lineales y realizar transformaciones lineales, pero no es la herramienta principal para la segmentación.
- Probabilidad y estadística: Si bien son fundamentales para el análisis de datos, no se refieren específicamente a la técnica de agrupar datos en segmentos.


- Simulaciones Monte Carlo

    - Concepto: Es una técnica estadística que utiliza números aleatorios para simular un proceso o sistema muchas veces. Cada simulación produce un resultado diferente, lo que permite obtener una distribución de posibles resultados y evaluar la probabilidad de diferentes eventos.
    - Ejemplo: Se utiliza para estimar el valor futuro de una inversión en la bolsa, considerando la volatilidad del mercado y otros factores aleatorios.
    - Aplicaciones:
        - Finanzas: Valoración de opciones, gestión de riesgos, simulación de carteras de inversión.
        - Ingeniería: Análisis de confiabilidad de sistemas, optimización de procesos.
        - Ciencias naturales: Modelado de fenómenos físicos y biológicos.
        - Gestión de proyectos: Estimación de costos y plazos.

- Análisis de ubicación

    - Concepto: Es un conjunto de técnicas que ayudan a determinar la mejor ubicación para una nueva instalación (fábrica, tienda, almacén, etc.) considerando factores como la demanda, los costos de transporte, la accesibilidad y la competencia.
    - Ejemplo: Una empresa de logística busca encontrar el lugar óptimo para construir un nuevo centro de distribución para minimizar los costos de transporte a sus clientes.
    - Aplicaciones:
        - Logística: Diseño de redes de distribución, selección de ubicaciones para almacenes.
        - Marketing: Selección de ubicaciones para tiendas, análisis de zonas de influencia.
        - Planificación urbana: Ubicación de servicios públicos, análisis de impacto de nuevas construcciones.

- Modelos de regresión

    - Concepto: Son herramientas estadísticas que permiten modelar la relación entre una variable dependiente (lo que queremos predecir) y una o más variables independientes (factores que influyen en la variable dependiente).
    - Ejemplo: Predecir el precio de una vivienda en función de su tamaño, ubicación, número de habitaciones, etc.
    - Aplicaciones:
        - Economía: Análisis de series de tiempo, predicción de ventas.
        - Marketing: Análisis de la relación entre las campañas publicitarias y las ventas.
        - Ciencias sociales: Estudio de la relación entre variables socioeconómicas.

- Análisis de la competencia

    - Concepto: Es el proceso de recopilar y analizar información sobre los competidores de una empresa para identificar sus fortalezas, debilidades, estrategias y oportunidades.
    - Ejemplo: Una nueva empresa de tecnología quiere entrar al mercado y analiza a sus principales competidores para identificar un nicho de mercado donde pueda diferenciarse.
    - Aplicaciones:
        - Desarrollo de estrategias de negocio: Identificación de oportunidades de crecimiento, desarrollo de nuevos productos o servicios.
        - Fijación de precios: Análisis de los precios de la competencia.
        - Marketing: Diseño de campañas publicitarias diferenciadas.

## Cuestionario 

1. ¿Qué técnica utilizan las empresas como Amazon para optimizar su cadena de suministro?
    A. Análisis de sentimiento.
    B. Algoritmos predictivos.
    C. Minería de texto.
    D. Análisis de redes sociales.

2. ¿Qué método utiliza Netflix para personalizar las recomendaciones a sus usuarios?
    A. Clustering.
    B. Regresión lineal.
    C. Aprendizaje automático.
    D. Análisis factorial.

3. ¿Qué herramienta matemática es crucial en la evaluación de riesgos financieros en empresas como Goldman Sachs?
    A. Cálculo diferencial.
    B. Teoría de grafos.
    C. Modelos predictivos.
    D. Geometría analítica.

4. En el contexto de la ciencia de la computación, ¿qué técnica es fundamental para el procesamiento de grandes volúmenes de datos?
    A. Programación funcional.
    B. Algoritmos de ordenamiento.
    C. Paralelización de procesos.
    D. Uso de variables estáticas.

5. ¿Qué técnica matemática es ampliamente utilizada para segmentar audiencias en marketing digital?
    A. Cálculo integral.
    B. Análisis de cluster.
    C. Álgebra lineal.
    D. Probabilidad y estadística.

6. ¿Cómo contribuye la teoría de grafos en la ciencia de datos?
    A. Optimización de algoritmos de búsqueda.
    B. Mejora de interfaces gráficas.
    C. Análisis de redes sociales.
    D. Desarrollo de juegos.

7. ¿Qué metodología utiliza Starbucks para determinar las ubicaciones óptimas para sus nuevas tiendas?
    A. Simulaciones Monte Carlo.
    B. Análisis de ubicación.
    C. Modelos de regresión.
    D. Análisis de la competencia.
8. ¿Cuál es un ejemplo de aplicación de regresión lineal en ciencia de datos?
    A. Predecir el precio futuro de las acciones.
    B. Codificar datos para algoritmos de cifrado.
    C. Crear gráficos interactivos.
    D. Diseñar bases de datos.

9. ¿Qué representa la comunicación efectiva de los resultados en proyectos de ciencia de datos en negocios?
    A. Publicar papers académicos.
    B. Convencer a los stakeholders del valor de los hallazgos.
    C. Implementar directamente los cambios en la producción.
    D. Ninguna de las anteriores.
10. ¿Qué rol juegan las visualizaciones de datos en la comunicación científica de datos?
    A. Solo para presentaciones académicas.
    B. Para simplificar el código.
    C. Para hacer los hallazgos comprensibles y accesibles.
    D. Para aumentar la carga computacional.


## Tema 5. Áreas de aplicación de la Ciencia de Datos