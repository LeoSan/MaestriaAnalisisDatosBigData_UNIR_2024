
# Tema 1. La ciencia del dato y los datos masivos

## 1.1. Generalidades
- La ciencia de datos y los datos masivos son disciplinas que se encuentran en constante crecimiento y evolución en el ámbito empresarial. 

- Su importancia radica en su capacidad para extraer conocimiento y generar valor a partir de grandes volúmenes de datos, 

- Permitiendo así tomar decisiones estratégicas fundamentadas en información precisa y relevante.

## 1.2. La Cadena de Valor

> La cadena de valor de los datos es el proceso de transformar datos crudos en acciones valiosas para una empresa. Comienza con la recolección de datos, continúa con su procesamiento y análisis, y culmina con la toma de decisiones informadas que generan valor. Cada etapa es crucial para aprovechar al máximo el potencial de los datos.

La cadena de valor de los datos consta de las siguientes etapas:

1. Recopilación: Se recolectan datos de diversas fuentes.
2. Procesamiento: Los datos se limpian, organizan y preparan para el análisis.
3. Análisis: Se aplican técnicas estadísticas y de aprendizaje automático para extraer insights.[perspectivas]
4. Información: Se generan conocimientos útiles a partir de los datos analizados.
5. Decisión: Se toman decisiones estratégicas basadas en la información obtenida.
6. Valor: Se crea valor para la empresa al implementar las decisiones.

## 1.3. ¿Qué son los datos masivos?

> Los datos masivos (big data) son conjuntos de información extremadamente grandes y complejos que se generan a una velocidad vertiginosa. Esta información proviene de diversas fuentes, como redes sociales, transacciones en línea y sensores.

## Características clave de los datos masivos:**

- Tienen Volumen: Grandes cantidades de datos que superan la capacidad de herramientas tradicionales.

- Tienen Velocidad: Datos generados a una velocidad muy alta y que necesitan ser procesados rápidamente.

- Tienen Variedad: Diferentes tipos de datos (texto, imágenes, videos, etc.) en diversos formatos.

## ¿Por qué son importantes los datos masivos?

- Contiene Información valiosa: Al analizar estos datos, se pueden obtener insights (perspectivas) clave para tomar mejores decisiones.

- Contienen Ventajas competitivas: Las empresas pueden identificar tendencias, predecir comportamientos y mejorar sus estrategias.

- Contienen Aprendizaje automático: Gracias a tecnologías como el aprendizaje automático, se pueden descubrir patrones complejos en los datos.

> Un dato puede definirse como un hecho concreto y discreto acerca de un evento. [ Seguiremos la proporcionada por Davenport y Prusak (1998).] La característica de ser discreto implica que, semánticamente, es la unidad mínima  que puede comunicarse o almacenarse. Por sí solos, los datos no brindan detalles significantes del entorno del que fueron obtenidos.

> La información puede definirse como un mensaje formado por la composición de varios datos

## 1.4. ¿Qué es la Ciencia de Datos?

- La ciencia de datos es una disciplina que busca extraer conocimiento a partir de grandes conjuntos de datos. 
- A diferencia de otras ciencias que se basan en teorías preexistentes, la ciencia de datos utiliza herramientas computacionales y estadísticas para descubrir patrones y tendencias ocultas en los datos.

**¿Qué hace única a la ciencia de datos?**

- Agnosticismo: Puede aplicarse a cualquier campo, desde química hasta marketing.
- Fundamental: Busca información básica en los datos, no solo confirmar teorías.
- Multidisciplinaria: Combina herramientas de matemáticas, estadística, informática, inteligencia artificial y más.
- Transformadora: Está cambiando la forma en que entendemos y abordamos la ciencia.

**Beneficios para las organizaciones:**

- Descubrimiento de patrones ocultos: Identificar tendencias y oportunidades que no son evidentes a simple vista.
- Toma de decisiones basada en datos: Utilizar la información para tomar decisiones más informadas y estratégicas.
- Optimización de procesos: Mejorar la eficiencia y reducir costos.
- Generación de estrategias competitivas: Obtener una ventaja competitiva en el mercado.

## 1.5. La toma de decisiones basadas en datos

La toma de decisiones basada en datos es un proceso que utiliza análisis estadístico profundo y herramientas avanzadas para transformar grandes cantidades de información en conocimiento accionable. Al emplear datos objetivos y rigurosos, las empresas pueden:

- Tomar decisiones más informadas: Identificando tendencias, patrones y riesgos.
- Optimizar procesos: Aumentando la eficiencia y reduciendo costos.
- Mejorar la toma de decisiones estratégicas: Alineado con los objetivos organizacionales.
- Minimizar la incertidumbre: Basando las decisiones en evidencia sólida.
- Incrementar la competitividad: Identificando nuevas oportunidades y mejorando la toma de decisiones.

## 1.6. Problemas empresariales y soluciones de ciencia de datos

- La ciencia de datos: un aliado estratégico para optimizar la cadena de suministro

- La ciencia de datos ofrece una herramienta poderosa para abordar de manera efectiva los desafíos que enfrentan las empresas en la actualidad. Al analizar grandes volúmenes de datos, esta disciplina permite:

- Optimizar la cadena de suministro: Identificando cuellos de botella, mejorando la planificación de la demanda y reduciendo costos.
- Detectar fraudes: Implementando sistemas de alerta temprana basados en patrones anómalos en los datos.
- Personalizar la experiencia del cliente: Analizando el comportamiento de los consumidores para ofrecer productos y servicios más relevantes.
- Segmentar el mercado: Identificando grupos de clientes con características similares para adaptar las estrategias de marketing.
- Predecir la demanda: Anticipando las necesidades futuras de los clientes para ajustar la producción y evitar desabastecimientos.

**¿Cómo funciona?**

A través de técnicas como el aprendizaje automático, la minería de datos y la inteligencia artificial, la ciencia de datos transforma los datos en información valiosa y accionable. Esto permite a las empresas tomar decisiones más informadas y basadas en evidencias, reduciendo así la incertidumbre y el riesgo.



## Tema 2. Ciclo de vida de los datos masivos

## 2.1. Recolección
Es el proceso que nos permite iniciar con la redaudacción de datos aquí podremos indentifcar la fuente de información, tambien podemos implementar tareas de limpieza y tratamiento de datos como validaciones y comprobaciones, para luego ser almacenados bajo su estructura previamente analizada bien sea BD SQL o NOsql 

## Es importante conocer como se encuentran los datos para iniciar su recolección

**Datos no estructurados**
Los datos no estructurados son la forma «más cruda» de los datos sin una estructura
identificable y pueden ser cualquier tipo de dato como textos, imágenes, sonidos o
vídeos. Una forma de determinar si los datos son no estructurados es si no podemos
almacenarlos en filas y columnas en una base de datos relacional

## Algunos ejemplos de datos no estructurados serían:
- Textos: archivos de texto plano (.txt)
- Imágenes y animaciones: archivos JPEG, PNG, GIF, etc.
- Sonidos: archivos MP3, OGG, WAV, etc.
- Vídeos: archivos MPEG / MP4, AVI, MKV, OGG, etc

**Datos estructurados**
- Siguen un esquema (schema) que requiere cierto esfuerzo elaborar. 
- El caso más habitual de este tipo de esquema en los datos estructurados es el de tablas o esquema tabular. 
- En los datos tabulares podemos estructurar los datos en filas y columnas, como si de una tabla se
tratara.
- La cabecera (header) o primera fila nos indica el nombre (y tipo) de cada uno
de los atributos. 
- Una fila o un conjunto de atributos es conocida como un registro o una instancia de la especificación dada por el esquema de datos.

## Algunos ejemplos de datos estructurados serían:
- Los archivos CSV (comma-separated values) son un ejemplo claro de datos
estructurados, siendo ampliamente utilizados como datasets en ciencia de datos.
- Ficheros CSV.
- Hojas de cálculo (Excel o similares).
- Bases de datos relacionales (SQL u OLAP).


**Datos semiestructurados**
En los datos semiestructurados el esquema no está separado de los datos, sino que
estos son autodescriptivos. Esto no significa que el esquema como tal por separado
no exista, sino que, en realidad, este es opcional los datos semiestructurados son estructuras más flexibles que los datos
estructurados y no tienen por qué ser tabulares

## Algunos ejemplos de datos estructurados serían:

- Archivos XML (extended markup language)
- Archivos HTML (hypertext markup language)
- Archivos JSON (Javascript object notation)
- Archivos YAML (YAML ain’t markup language u, originalmente, yet another markup language), 
- Tripletas RDF (resource description framework)


## Mecanismos de muestreo

En este apartado vamos a describir, en primer lugar, los principales mecanismos de
muestreo a la hora de recoger los datos desde diferentes fuentes,

**Muestro de señales y el teorema de Shannon-Nyquist**
Un ejemplo evidente es la recogida de datos desde redes de sensores o
dispositivos IoT, ampliamente utilizados en la medición de datos acerca del
entorno. 

Según el teorema de Shannon-Nyquist, si una función x(t) no contiene frecuencias
superiores a B hertzios, podemos caracterizar completamente esta si tomamos
muestras cada 1/(2B) segundos o, lo que es lo mismo, con una frecuencia igual a 2B
(Vaidyanathan, 2001). 2B es llamada también la tasa de Nyquist, mientras que la
máxima B permitida por un equipo de muestro (un conversor analógico-digital) es la
frecuencia de Nyquist.


**Muestreo de poblaciones: métodos probabilísticos, métodos no probabilísticos y sesgo**

El muestreo puede basarse en la probabilidad, un enfoque que utiliza números
aleatorios que corresponden a puntos del conjunto de datos para garantizar que no
haya correlación entre los puntos elegidos para la muestra. Entre los métodos
probabilísticos podemos destacar:

- Muestreo aleatorio simple (simple random sampling): se utiliza un proceso
informático para seleccionar aleatoriamente a los sujetos de toda la población.
Muestreo estratificado (stratified sampling): se crean subconjuntos de los conjuntos
de datos o de la población en función de un factor común y se recogen muestras al
azar de cada subgrupo (por ejemplo, estratificar la población por sexo, edad,
provincia, ingresos, etc.). Así nos aseguramos de que no tenemos, por ejemplo, un
70 % de muestras de hombres, cuando estos no representan tal porcentaje en la
población total. En caso contrario, introduciríamos un sesgo en el muestreo.

- Muestreo por conglomerados (cluster sampling): el conjunto de datos más amplio
se divide en subconjuntos (clusters o clústeres) en función de un factor definido, y
luego se analiza un muestreo aleatorio de los clústeres. Por ejemplo, crear
subconjuntos por región o por provincia o crear subconjuntos por tipos de
consumidores que se comportan de forma similar. Es importante que el
comportamiento dentro de cada clúster sea relativamente homogéneo, mientras que
el comportamiento entre individuos de diferentes clústeres sea relativamente
heterogéneo.

- Muestreo multietapa (multistep sampling): se trata de una forma más complicada
del muestreo por conglomerados. Este método también implica la división de la
población más grande en una serie de clústeres. A continuación, los clústeres de la
segunda etapa se dividen en función de un factor secundario y esos clústeres se
muestrean y analizan. Este escalonamiento podría continuar a medida que se
identifican, agrupan y analizan múltiples subconjuntos.

- Muestreo sistemático (systematic sampling): en este método, se crea una muestra
estableciendo un intervalo en el que se extraen los datos de la población mayor. Por
ejemplo, seleccionando cada 10 filas en una hoja de cálculo o un fichero CSV de
2000 elementos para crear un tamaño de muestra de 200 filas para analizar.


**El muestreo también puede basarse en la no probabilidad, un enfoque en el que
se determina y extrae una muestra de datos basada en el juicio del científico de Datos**

- Muestreo de conveniencia (convenience sampling): los datos se recogen de un
grupo fácilmente accesible y disponible.

- Muestreo consecutivo (consecutive sampling): los datos se recogen de cada sujeto
que cumple los criterios hasta alcanzar el tamaño de muestra predeterminado.

- Muestreo intencional o de juicio (purposive o intentional sampling): el investigador
selecciona los datos a muestrear basándose en criterios predefinidos.

- Muestreo por cuotas (quota sampling): el investigador garantiza una representación
equitativa dentro de la muestra para todos los subgrupos del conjunto de datos o
población.


**¿Qué es ETL?**

ETL es un proceso de integración de datos que combina, limpia y organiza datos de múltiples fuentes en un único conjunto de datos coherente para su almacenamiento en un almacén de datos, un lago de datos u otro sistema de destino.

**Las tres fases del proceso ETL son:**

**Extracción (Extract):**

- En esta fase, se extraen los datos relevantes de los sistemas de origen.
- Los sistemas de origen pueden ser bases de datos, archivos planos, aplicaciones SaaS, etc.
- Los datos se extraen utilizando diferentes técnicas, como consultas SQL, API, o scripts personalizados.

**Transformación (Transform):**

- En esta fase, los datos extraídos se limpian, validan y transforman para que sean adecuados para el análisis.
- Las transformaciones comunes incluyen:
    - Limpieza de datos: Eliminar valores nulos, duplicados o inconsistentes.
    - Validación de datos: Asegurarse de que los datos cumplen con los estándares de calidad.
    - Normalización de datos: Convertir los datos a un formato estándar.
    - Agregación de datos: Combinar múltiples fuentes de datos en una sola.
    - Enriquecimiento de datos: Agregar contexto o información adicional a los datos.

**Carga (Load):**

- En esta fase, los datos transformados se cargan en el sistema de destino.
- El sistema de destino puede ser un almacén de datos, un lago de datos, una base de datos analítica, etc.
- Los datos se cargan utilizando diferentes técnicas, como carga por lotes, carga incremental o carga directa.

## Técnicas avanzadas en la recolección de datos masivos
- OCR (Reconocimiento óptico de caracteres). Convierte documentos impresos o imágenes en texto editable y buscable, facilitando el  acceso y la manipulación de información.

- ICR (Reconocimiento Inteligente de Caracteres). Capaz de reconocer y aprender de texto manuscrito, mejorando continuamente su precisión y eficiencia en el procesamiento de datos.

- RFID (Identificación por Radiofrecuencia). Utiliza etiquetas electrónicas para el seguimiento y gestión remota de inventarios y activos mediante señales de radio.

- NFC (Near-field communication-Comunicaciones de Campo Cercano). Permite la comunicación inalámbrica entre dispositivos a corta distancia. Ampliamente usado en pagos móviles y sistemas de seguridad.

- Aplicaciones de Estas Tecnologías. Esenciales en logística, retail, seguridad y gestión de datos, mejorando significativamente la eficiencia operativa y la precisión de la información.

## Desafíos y métodos de captura en la recolección de datos masivos

- Métodos de captura.
- Optimización mediante tecnología.
- Automatización de la captura. 

## 2.2. Análisis
Es el proceso que nos permite de ciertamanera iniciar con el estudio de los datos, preparar todo para realizar calculos, comparaciones y encontrar patrones que nos apoyen a generar insights. Podemos usar algunos modelos de regresión, clasificación, segmentación y recomendación

Hay tres tipos de análisis 

- Análisis descriptivo: describe lo que ha pasado con estadísticas, gráficos, tablas e informes.

- Análisis predictivo: realiza predicciones que van a ser útiles en el futuro siendo de especial importancia la precisión de la predicción.

- Análisis prescriptivo: ayuda a entender qué debe hacerse para obtener los resultados esperados en el futuro.



## Introducción Al Análisis de Datos 
- Definición de análisis de datos: Proceso de inspeccionar, limpiar, transformar y modelar datos con el objetivo de descubrir información útil, informar conclusiones y apoyar la toma de decisiones.
- Importancia en la era digital: El análisis de datos permite a las organizaciones y empresas aprovechar el potencial de los datos masivos para optimizar operaciones, prever tendencias y mejorar la experiencia del cliente.
- Desafíos actuales: Manejar el volumen creciente de datos, asegurar la calidad y la privacidad de los datos y extraer conocimientos significativos en tiempo real.

## Preparación de datos en análisis de datos
- Evaluación de la calidad: Incluye el análisis de formatos, completitud, integridad y disponibilidad de los datos. Fundamental para garantizar la fiabilidad de los análisis posteriores.
- Tareas de preprocesamiento: Realización de transformaciones y filtrados, análisis de datos faltantes, eliminación de datos ruidosos o atípicos.
- También aparecen tareas como la discretización o categorización, encodings y la reducción de los datos.


## El papel del machine learning en el análisis de datos
- Fundamentos del Machine Learning:  Machine learning utiliza algoritmos para analizar datos, aprender de ellos y hacer predicciones o tomar decisiones.
> Se considera una disciplina de la inteligencia artificial. 
- Se especializa en el reconocimiento de patrones; sobre muestras de datos, aprende y permite extraer inferencias de nuevos conjuntos de datos para los que no ha sido entrenado previamente.
- Impacto. Permite automatizar el análisis complejo de grandes volúmenes de datos y descubrir patrones y tendencias que no son evidentes para los métodos tradicionales de análisis.

## Técnicas de machine learning datos

- Técnicas comunes. Incluye métodos de análisis supervisado:
    - Regresión 
    - Clasificación
    - No supervisado. 
- Clustering, reducción de dimensiones y sistemas de recomendación, aplicables a diferentes tipos de datos y necesidades. También encontramos técnicas de aprendizaje semi-supervizado aprendizaje por refuerzo.
- Procesamiento del Lenguaje Natural. El ML es crucial en el procesamiento del lenguaje natural, ayudando a interpretar y generar lenguaje humano de manera efectiva.

## Visualización en el análisis de grandes volúmenes
- Propósito de la visualización: Transformar grandes volúmenes de datos en representaciones gráficas que facilitan la comprensión y la toma de decisiones rápidas.
- Herramientas esenciales: Herramientas de BI como Tableau, Qlik Sense, Looker, Domo, Power BI y Google Data Studio son plataformas líderes que permiten a los usuarios crear visualizaciones dinámicas y dashboards interactivos.
- Impacto en el análisis de datos: Mejora significativamente la accesibilidad y la interpretación de datos complejos, permitiendo a los analistas y stakeholders obtener insights valiosos de manera eficiente.

## 2.3. Visualización

Es el proceso mediante el cual la información analizada se presenta de manera gráfica. Esta fase es esencial porque transforma grandes cantidades de datos complejos en representaciones visuales más accesibles y comprensibles, como gráficos, diagramas y mapas de calor

## 2.4. Interpretación

Finalmente, la interpretación de los datos es el último paso, donde los resultados visualizados se examinan para tomar decisiones informadas

